{"cells":[{"cell_type":"markdown","source":["# Applying Schemas to JSON Data\n\nApache Spark&trade; and Azure Databricks&reg; provide a number of ways to project structure onto semi-structured data allowing for quick and easy access."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Schemas\n\nSchemas are at the heart of data structures in Spark.\n**A schema describes the structure of your data by naming columns and declaring the type of data in that column.** \nRigorously enforcing schemas leads to significant performance optimizations and reliability of code.\n\nWhy is open source Spark so fast, and why is [Databricks Runtime even faster?](https://databricks.com/blog/2017/07/12/benchmarking-big-data-sql-platforms-in-the-cloud.html) While there are many reasons for these performance improvements, two key reasons are:<br><br>\n* First and foremost, Spark runs first in memory rather than reading and writing to disk. \n* Second, using DataFrames allows Spark to optimize the execution of your queries because it knows what your data looks like.\n\nTwo pillars of computer science education are data structures, the organization and storage of data and algorithms, and the computational procedures on that data.  A rigorous understanding of computer science involves both of these domains. When you apply the most relavant data structures, the algorithms that carry out the computation become significantly more eloquent.\n\nIn the roadmap for ETL, this is the **Apply Schema** step:\n\n<img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/ETL-Process-2.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"],"metadata":{}},{"cell_type":"markdown","source":["### Schemas with Semi-Structured JSON Data\n\n**Tabular data**, such as that found in CSV files or relational databases, has a formal structure where each observation, or row, of the data has a value (even if it's a NULL value) for each feature, or column, in the data set.  \n\n**Semi-structured data** does not need to conform to a formal data model. Instead, a given feature may appear zero, once, or many times for a given observation.  \n\nSemi-structured data storage works well with hierarchical data and with schemas that may evolve over time.  One of the most common forms of semi-structured data is JSON data, which consists of attribute-value pairs."],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell to mount the data:"],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Print the first few lines of a JSON file holding ZIP Code data."],"metadata":{}},{"cell_type":"code","source":["%fs head /mnt/training/zips.json"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Schema Inference\n\nImport data as a DataFrame and view its schema with the `printSchema()` DataFrame method."],"metadata":{}},{"cell_type":"code","source":["zipsDF = spark.read.json(\"/mnt/training/zips.json\")\nzipsDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Store the schema as an object by calling `.schema` on a DataFrame. Schemas consist of a `StructType`, which is a collection of `StructField`s.  Each `StructField` gives a name and a type for a given field in the data."],"metadata":{}},{"cell_type":"code","source":["zipsSchema = zipsDF.schema\nprint(type(zipsSchema))\n\n[field for field in zipsSchema]"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### User-Defined Schemas\n\nSpark infers schemas from the data, as detailed in the example above.  Challenges with inferred schemas include:  \n<br>\n* Schema inference means Spark scans all of your data, creating an extra job, which can affect performance\n* Consider providing alternative data types (for example, change a `Long` to a `Integer`)\n* Consider throwing out certain fields in the data, to read only the data of interest\n\nTo define schemas, build a `StructType` composed of `StructField`s."],"metadata":{}},{"cell_type":"markdown","source":["Import the necessary types from the `types` module. Build a `StructType`, which takes a list of `StructField`s.  Each `StructField` takes three arguments: the name of the field, the type of data in it, and a `Boolean` for whether this field can be `Null`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\nzipsSchema2 = StructType([\n  StructField(\"city\", StringType(), True), \n  StructField(\"pop\", IntegerType(), True) \n])"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["-sandbox\nApply the schema using the `.schema` method. This `read` returns only  the columns specified in the schema and changes the column `pop` from `LongType` (which was inferred above) to `IntegerType`.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> A `LongType` is an 8-byte integer ranging up to 9,223,372,036,854,775,807 while `IntegerType` is a 4-byte integer ranging up to 2,147,483,647.  Since no American city has over two billion people, `IntegerType` is sufficient."],"metadata":{}},{"cell_type":"code","source":["zipsDF2 = (spark.read\n  .schema(zipsSchema2)\n  .json(\"/mnt/training/zips.json\")\n)\n\ndisplay(zipsDF2)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["-sandbox\n### Primitive and Nonprimitive Types\n\nThe Spark [`types` package](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types) provides the building blocks for constructing schemas.\n\nA primitive type contains the data itself.  The most common primitive types include:\n\n| Numeric | General | Time |\n|-----|-----|\n| `FloatType` | `StringType` | `TimestampType` | \n| `IntegerType` | `BooleanType` | `DateType` | \n| `DoubleType` | `NullType` | |\n| `LongType` | | |\n| `ShortType` |  | |\n\nNon-primitive types are sometimes called reference variables or composite types.  Technically, non-primitive types contain references to memory locations and not the data itself.  Nonprimitive types are the composite of a number of primitive types such as an Array of the primitive type `Integer`.\n\nThe two most common composite types are `ArrayType` and `MapType`. These types allow for a given field to contain an arbitrary number of elements in either an Array/List or Map/Dictionary form.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See the [Spark documentation](http://spark.apache.org/docs/latest/sql-programming-guide.html#data-types) for a complete picture of types in Spark."],"metadata":{}},{"cell_type":"markdown","source":["The ZIP Code dataset contains an array with the latitude and longitude of the cities.  Use an `ArrayType`, which takes the primitive type of its elements as an argument."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType, FloatType\n\nzipsSchema3 = StructType([\n  StructField(\"city\", StringType(), True), \n  StructField(\"loc\", \n    ArrayType(FloatType(), True), True),\n  StructField(\"pop\", IntegerType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Apply the schema using the `.schema()` method and observe the results.  Expand the array values in the column `loc` to explore further."],"metadata":{}},{"cell_type":"code","source":["zipsDF3 = (spark.read\n  .schema(zipsSchema3)\n  .json(\"/mnt/training/zips.json\")\n)\ndisplay(zipsDF3)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["## Exercise 1: Exploring JSON Data\n\n<a href=\"https://archive.ics.uci.edu/ml/datasets/UbiqLog+(smartphone+lifelogging)\">Smartphone data from UCI Machine Learning Repository</a> is available under `/mnt/training/UbiqLog4UCI`. This is log data from the open source project [Ubiqlog](https://github.com/Rezar/Ubiqlog).\n\nImport this data and define your own schema."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Import the Data\n\nImport data from `/mnt/training/14_F/log*`. (This is the log files from a given user.)"],"metadata":{}},{"cell_type":"markdown","source":["Look at the head of one file from the data set.  Use `/mnt/training/UbiqLog4UCI/14_F/log_1-6-2014.txt`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nprint(dbutils.fs.head('/mnt/training/UbiqLog4UCI/14_F/log_1-6-2014.txt', 200)) # this evaluates to the thing as %fs head /mnt/training/UbiqLog4UCI/14_F/log_1-6-2014.txt"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Read the data and save it to `smartphoneDF`. Read the logs using a `*` in your path like `/mnt/training/UbiqLog4UCI/14_F/log*`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nsmartphoneDF = spark.read.json(\"/mnt/training/UbiqLog4UCI/14_F/log*\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nfrom pyspark.sql.functions import desc\n\ncols = set(smartphoneDF.columns)\nexpected = {'Application', 'Bluetooth', 'Call', 'Location', 'SMS', 'WiFi', '_corrupt_record'}\nsample = smartphoneDF.orderBy(desc(\"Application\")).first()[0][0]\n\ndbTest(\"ET1-P-05-01-01\", 25372, smartphoneDF.count())\ndbTest(\"ET1-P-05-01-02\", expected <= cols, True)\ndbTest(\"ET1-P-05-01-03\", expected >= cols, True)\ndbTest(\"ET1-P-05-01-04\", '12-9-2013 21:30:02', sample)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["### Step 2: Explore the Inferred Schema\n\nPrint the schema to get a sense for the data."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nsmartphoneDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["The schema shows:  \n\n* Six categories of tracked data \n* Nested data structures\n* A field showing corrupt records"],"metadata":{}},{"cell_type":"markdown","source":["## Exercise 2: Creating a User Defined Schema"],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Set Up Your workflow\n\nOften the hardest part of a coding challenge is setting up a workflow to get continuous feedback on what you develop.\n\nStart with the import statements you need, including functions from two main packages:\n\n| Package | Function |\n|---------|---------|\n| `pyspark.sql.types` | `StructType`, `StructField`, `StringType` |\n| `pyspark.sql.functions` | `col` |"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\nfrom pyspark.sql.types import StructType, StructField, StringType\nfrom pyspark.sql.functions import col"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["The **SMS** field needs to be parsed. Create a placeholder schema called `schema` that's a `StructType` with one `StructField` named **SMS** of type `StringType`. This imports the entire attribute (even though it contains nested entities) as a String.  \n\nThis is a way to get a sense for what's in the data and make a progressively more complex schema."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\nfrom pyspark.sql.types import StructType, StructField, StringType\nfrom pyspark.sql.functions import col\n\nschema = StructType([\n  StructField(\"SMS\", StringType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nfields = schema.fields\n\ndbTest(\"ET1-P-05-02-01\", 1, len(fields))\ndbTest(\"ET1-P-05-02-02\", 'SMS', fields[0].name)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["Apply the schema to the data and save the result as `SMSDF`. This closes the loop on which to iterate and develop an increasingly complex schema. The path to the data is `/mnt/training/UbiqLog4UCI/14_F/log*`. \n\nInclude only records where the column `SMS` is not `Null`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.types import StructType, StructField, StringType\nfrom pyspark.sql.functions import col\n\nschema = StructType([\n  StructField(\"SMS\", StringType(), True)\n])\n\nSMSDF = (spark.read\n  .schema(schema)\n  .json(\"/mnt/training/UbiqLog4UCI/14_F/log*\")\n  .filter(col(\"SMS\").isNotNull())\n)\n\ndisplay(SMSDF)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = SMSDF.columns\n\ndbTest(\"ET1-P-05-03-01\", 1147, SMSDF.count())\ndbTest(\"ET1-P-05-03-02\", ['SMS'], cols)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["-sandbox\n### Step 2: Create the Full Schema for SMS\n\nDefine the Schema for the following fields in the `StructType` `SMS` and name it `schema2`.  Apply it to a new DataFrame `SMSDF2`:  \n<br>\n* `Address`\n* `date`\n* `metadata`\n - `name`\n \n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Note there's `Type` and `type`, which appears to be redundant data."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\nfrom pyspark.sql.types import StructType, StructField, StringType\nfrom pyspark.sql.functions import col\n\nschema2 = StructType([\n  StructField(\"SMS\", StructType([\n    StructField(\"Address\",StringType(),True),\n    StructField(\"date\",StringType(),True),\n    StructField(\"metadata\", StructType([\n      StructField(\"name\",StringType(), True)\n    ]), True),\n  ]), True)\n])\n\n# Here is the full schema as well\n# fullSchema = StructType([\n#   StructField(\"SMS\", StructType([\n#     StructField(\"Address\",StringType(),True),\n#     StructField(\"Type\",StringType(),True),\n#     StructField(\"body\",StringType(),True),\n#     StructField(\"date\",StringType(),True),\n#     StructField(\"metadata\", StructType([\n#       StructField(\"name\",StringType(), True)\n#     ]), True),\n#     StructField(\"type\",StringType(),True)\n#   ]), True)\n# ])\n\nSMSDF2 = (spark.read\n  .schema(schema2)\n  .json(\"/mnt/training/UbiqLog4UCI/14_F/log*\")\n  .filter(col(\"SMS\").isNotNull()))\n\ndisplay(SMSDF2)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = SMSDF2.columns\nschemaJson = SMSDF2.schema.json()\n\ndbTest(\"ET1-P-05-04-01\", 1147, SMSDF2.count())\ndbTest(\"ET1-P-05-04-02\", ['SMS'], cols)\ndbTest(\"ET1-P-05-04-03\", True, 'Address' in schemaJson and 'date' in schemaJson)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["### Step 3: Compare Solution Performance\n\nCompare the dafault schema inference to applying a user defined schema using the `%timeit` function.  Which completed faster?  Which triggered more jobs?  Why?"],"metadata":{}},{"cell_type":"code","source":["%timeit SMSDF = spark.read.schema(schema2).json(\"/mnt/training/UbiqLog4UCI/14_F/log*\").count()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%timeit SMSDF = spark.read.json(\"/mnt/training/UbiqLog4UCI/14_F/log*\").count()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["Providing a schema increases performance two to three times, depending on the size of the cluster used. Since Spark doesn't infer the schema, it doesn't have to read through all of the data. This is also why there are fewer jobs when a schema is provided: Spark doesn't need one job for each partition of the data to infer the schema."],"metadata":{}},{"cell_type":"markdown","source":["## Review\n\n**Question:** What are two ways to attain a schema from data?  \n**Answer:** Allow Spark to infer a schema from your data or provide a user defined schema. Schema inference is the recommended first step; however, you can customize this schema to your use case with a user defined schema.\n\n**Question:** Why should you define your own schema?  \n**Answer:** Benefits of user defined schemas include:\n* Avoiding the extra scan of your data needed to infer the schema\n* Providing alternative data types\n* Parsing only the fields you need\n\n**Question:** Why is JSON a common format in big data pipelines?  \n**Answer:** Semi-structured data works well with hierarchical data and where schemas need to evolve over time.  It also easily contains composite data types such as arrays and maps.\n\n**Question:** By default, how are corrupt records dealt with using `spark.read.json()`?  \n**Answer:** They appear in a column called `_corrupt_record`.  These are the records that Spark can't read (e.g. when characters are missing from a JSON string)."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Corrupt Record Handling]($./06-Corrupt-Record-Handling )."],"metadata":{}}],"metadata":{"name":"05-Applying-Schemas-to-JSON-Data","notebookId":291050440997800},"nbformat":4,"nbformat_minor":0}
