{"cells":[{"cell_type":"markdown","source":["# Connecting to JDBC\n\nApache Spark&trade; and Azure Databricks&reg; allow you to connect to a number of data stores using JDBC."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n## Java Database Connectivity\n\nJava Database Connectivity (JDBC) is an application programming interface (API) that defines database connections in Java environments.  Spark is written in Scala, which runs on the Java Virtual Machine (JVM).  This makes JDBC the preferred method for connecting to data whenever possible. Hadoop, Hive, and MySQL all run on Java and easily interface with Spark clusters.\n\nIn the roadmap for ETL, this is the **Extract and Validate** step:\n\n<img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/ETL-Process-1.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"],"metadata":{}},{"cell_type":"markdown","source":["### Recalling the Design Pattern\n\nRecall the design pattern for connecting to data from the previous lesson:  \n<br>\n1. Define the connection point.\n2. Define connection parameters such as access credentials.\n3. Add necessary options. \n\nAfter adhering to this, read data using `spark.read.options(<option key>, <option value>).<connection_type>(<endpoint>)`.  The JDBC connection uses this same formula with added complexity over what was covered in the lesson."],"metadata":{}},{"cell_type":"markdown","source":["Run the cell below to set up your environment."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["-sandbox\nRun the cell below to confirm you are using the right driver.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Each notebook has a default language that appears in upper corner of the screen next to the notebook name, and you can easily switch between languages in a notebook. To change languages, start your cell with `%python`, `%scala`, `%sql`, or `%r`."],"metadata":{}},{"cell_type":"code","source":["%scala\n// run this regardless of language type\nClass.forName(\"org.postgresql.Driver\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Define your database connection criteria. In this case, you need the hostname, port, and database name. \n\nAccess the database `training` via port `5432` of a Postgres server sitting at the endpoint `server1.databricks.training`.\n\nCombine the connection criteria into a URL."],"metadata":{}},{"cell_type":"code","source":["jdbcHostname = \"server1.databricks.training\"\njdbcPort = 5432\njdbcDatabase = \"training\"\n\njdbcUrl = \"jdbc:postgresql://{0}:{1}/{2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Create a connection properties object with the username and password for the database."],"metadata":{}},{"cell_type":"code","source":["connectionProps = {\n  \"user\": \"readonly\",\n  \"password\": \"readonly\"\n}"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Read from the database by passing the URL, table name, and connection properties into `spark.read.jdbc()`."],"metadata":{}},{"cell_type":"code","source":["accountDF = spark.read.jdbc(url=jdbcUrl, table=\"Account\", properties=connectionProps)\ndisplay(accountDF)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## Exercise 1: Parallelizing JDBC Connections\n\nThe command above was executed as a serial read through a single connection to the database. This works well for small data sets; at scale, parallel reads are neccesary for optimal performance.\n\nSee the [Managing Parallelism](https://docs.azuredatabricks.net/spark/latest/data-sources/sql-databases.html#manage-parallelism) section of the Databricks documentation."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Step 1: Find the Range of Values in the Data\n\nParallel JDBC reads entail assigning a range of values for a given partition to read from. The first step of this divide-and-conquer approach is to find bounds of the data.\n\nCalculate the range of values in the `insertID` column of `accountDF`. Save the minimum to `dfMin` and the maximum to `dfMax`.  **This should be the number itself rather than a DataFrame that contains the number.**  Use `.first()` to get a Scala or Python object.\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** See the `min()` and `max()` functions in Python `pyspark.sql.functions` or Scala `org.apache.spark.sql.functions`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import min, max\n\ndfMin = accountDF.select(min(\"insertID\")).first()[0]\ndfMax = accountDF.select(max(\"insertID\")).first()[0]\n\nprint(\"DataFrame minimum: {}\\nDataFrame maximum: {}\".format(dfMin, dfMax))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\n\ndbTest(\"ET1-P-04-01-01\", 0, dfMin)\ndbTest(\"ET1-P-04-01-02\", 214748365087, dfMax)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["-sandbox\n### Step 2: Define the Connection Parameters.\n\n<a href=\"https://docs.azuredatabricks.net/spark/latest/data-sources/sql-databases.html#manage-parallelism\" target=\"_blank\">Referencing the documentation,</a> define the connection parameters for this read.  Use 12 partitions.\n\nSave the results to `accountDFParallel`.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Setting the column for your parallel read introduces unexpected behavior due to a bug in Spark. To make sure Spark uses the capitalization of your column, use `'\"insertID\"'` for your column. <a href=\"https://github.com/apache/spark/pull/20370#issuecomment-359958843\" target=\"_blank\"> Monitor the issue here.</a>"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\naccountDFParallel = spark.read.jdbc(\n  url=jdbcUrl, \n  table=\"Account\",\n  column='\"insertID\"',\n  lowerBound=dfMin,\n  upperBound=dfMax,\n  numPartitions=12,\n  properties=connectionProps\n)\n\ndisplay(accountDFParallel)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ndbTest(\"ET1-P-04-02-01\", 12, accountDFParallel.rdd.getNumPartitions())\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Step 3: Compare the Serial and Parallel Reads\n\nCompare the two reads with the `%timeit` function."],"metadata":{}},{"cell_type":"markdown","source":["Display the number of partitions in each DataFrame by running the following:"],"metadata":{}},{"cell_type":"code","source":["print(accountDF.rdd.getNumPartitions())\nprint(accountDFParallel.rdd.getNumPartitions())"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Invoke `%timeit` followed by calling a `.describe()`, which computes summary statistics, on both `accountDF` and `accountDFParallel`."],"metadata":{}},{"cell_type":"code","source":["%timeit accountDF.describe()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%timeit accountDFParallel.describe()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["What is the difference between serial and parallel reads?  Note that your results vary drastically depending on the cluster and number of partitions you use"],"metadata":{}},{"cell_type":"markdown","source":["## Review\n\n**Question:** What is JDBC?  \n**Answer:** JDBC stands for Java Database Connectivity, and is a Java API for connecting to databases such as MySQL, Hive, and other data stores.\n\n**Question:** How does Spark read from a JDBC connection by default?  \n**Answer:** With a serial read.  With additional specifications, Spark conducts a faster, parallel read.  Parallel reads take full advantage of Spark's distributed architecture.\n\n**Question:** What is the general design pattern for connecting to your data?  \n**Answer:** The general design patter is as follows:\n0. Define the connection point\n0. Define connection parameters such as access credentials\n0. Add necessary options such as for headers or paralleization"],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Applying Schemas to JSON Data]($./05-Applying-Schemas-to-JSON-Data )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** My tool can't connect via JDBC.  Can I connect via <a href=\"https://en.wikipedia.org/wiki/Open_Database_Connectivity\" target=\"_blank\">ODBC instead</a>?  \n**A:** Yes.  The best practice is generally to use JDBC connetions wherever possible since Spark runs on the JVM.  In cases where JDBC is either not supported or is less performant, use the Simba ODBC driver instead.  See <a href=\"https://docs.azuredatabricks.net/user-guide/bi/jdbc-odbc-bi.html#step-1-download-and-install-a-jdbc-odbc-driver\" target=\"_blank\">the Databricks documentation on connecting BI tools</a> for more details."],"metadata":{}}],"metadata":{"name":"04-Connecting-to-JDBC","notebookId":291050440997688},"nbformat":4,"nbformat_minor":0}
