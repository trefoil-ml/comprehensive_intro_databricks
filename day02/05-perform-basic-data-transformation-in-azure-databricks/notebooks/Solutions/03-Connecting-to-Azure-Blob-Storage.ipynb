{"cells":[{"cell_type":"markdown","source":["# Connecting to Azure Blob Storage\n\nApache Spark&trade; and Azure Databricks&reg; allow you to connect to virtually any data store including Azure Blob Storage."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Spark as a Connector\n\nSpark quickly rose to popularity as a replacement for the [Apache Hadoop&trade;](http://hadoop.apache.org/) MapReduce paradigm in a large part because it easily connected to a number of different data sources.  Most important among these data sources was the Hadoop Distributed File System (HDFS).  Now, Spark engineers connect to a wide variety of data sources including:  \n<br>\n* Traditional databases like Postgres, SQL Server, and MySQL\n* Message brokers like Kafka and Kinesis\n* Distributed databases like Cassandra and Redshift\n* Data warehouses like Hive and Cosmos DB\n* File types like CSV, Parquet, and Avro\n\n<img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/open-source-ecosystem_2.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### DBFS Mounts and Azure Blob Storage\n\nAzure Blob Storage is the backbone of Databricks workflows.  Azure Blob Storage offers data storage that easily scales to the demands of most data applications and, by colocating data with Spark clusters, Databricks quickly reads from and writes to Azure Blob Storage in a distributed manner.\n\nThe Databricks File System (DBFS), is a layer over Azure Blob Storage that allows you to mount Blob containers, making them available to other users in your workspace and persisting the data after a cluster is shut down.\n\nIn our roadmap for ETL, this is the <b>Extract and Validate </b> step:\n\n<img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/ETL-Process-1.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"],"metadata":{}},{"cell_type":"markdown","source":["Run the cell below to set up your environment."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["-sandbox\n\nDefine your Azure Blob credentials.  You need the following elements:<br><br>\n\n* Storage account name\n* Container name\n* Mount point (how the mount will appear in DBFS)\n* Shared Access Signature (SAS) key\n\nBelow these elements are defined, including a read-only SAS key.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> For more information on SAS keys, <a href=\"https://docs.microsoft.com/en-us/azure/storage/common/storage-dotnet-shared-access-signature-part-1\" target=\"_blank\"> see the Azure documentation.</a><br>\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> SAS keys are normally provided as a SAS URI. Of this URI, focus on everything from the `?` on, including the `?`. The following cell provides an example of this."],"metadata":{}},{"cell_type":"code","source":["STORAGE_ACCOUNT = \"dbtraineastus2\"\nCONTAINER = \"training\"\nMOUNT_POINT = \"/mnt/training\"\nSAS_KEY = \"?sv=2017-07-29&ss=b&srt=sco&sp=rl&se=2023-04-19T06:32:30Z&st=2018-04-18T22:32:30Z&spr=https&sig=BB%2FQzc0XHAH%2FarDQhKcpu49feb7llv3ZjnfViuI9IWo%3D\""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["First, unmount the container, since you mounted this container in the classroom setup script."],"metadata":{}},{"cell_type":"code","source":["try:\n  dbutils.fs.unmount(MOUNT_POINT) # Use this to unmount as needed\nexcept:\n  print(\"{} already unmounted\".format(MOUNT_POINT))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Define two strings populated with the storage account and container information.  This will be passed to the `mount` function."],"metadata":{}},{"cell_type":"code","source":["source_str = \"wasbs://{container}@{storage_acct}.blob.core.windows.net/\".format(container=CONTAINER, storage_acct=STORAGE_ACCOUNT)\nconf_key = \"fs.azure.sas.{container}.{storage_acct}.blob.core.windows.net\".format(container=CONTAINER, storage_acct=STORAGE_ACCOUNT)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["-sandbox\n\nNow, mount the container <a href=\"https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html#mount-azure-blob-storage-containers-with-dbfs\" target=\"_blank\"> using the template provided in the docs.</a>\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> The code below includes error handling logic to handle the case where the mount is already mounted."],"metadata":{}},{"cell_type":"code","source":["try:\n  dbutils.fs.mount(\n    source = source_str,\n    mount_point = MOUNT_POINT,\n    extra_configs = {conf_key: SAS_KEY}\n  )\nexcept Exception as e:\n  print(\"ERROR: {} already mounted. Run previous cells to unmount first\".format(MOUNT_POINT))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Next, explore the mount using `%fs ls` and the name of the mount."],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/training"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["-sandbox\n\nIn practice, always secure your credentials.  Do this by either maintaining a single notebook with restricted permissions that holds SAS keys, or delete the cells or notebooks that expose the keys. **After a cell used to mount a container is run, access this mount in any notebook, any cluster, and share the mount between colleagues.**\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See <a href=\"https://docs.azuredatabricks.net/user-guide/secrets/index.html\" target=\"_blank\">secret management to securely store and reference your credentials in notebooks and jobs.</a>"],"metadata":{}},{"cell_type":"markdown","source":["## Adding Options\n\nWhen you import that data into a cluster, you can add options based on the specific characteristics of the data."],"metadata":{}},{"cell_type":"markdown","source":["Display the first few lines of `Chicago-Crimes-2018.csv` using `%fs head`."],"metadata":{}},{"cell_type":"code","source":["%fs head /mnt/training/Chicago-Crimes-2018.csv"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["`option` is a method of `DataFrameReader`. Options are key/value pairs and must be specified before calling `.csv()`.\n\nThis is a tab-delimited file, as seen in the previous cell. Specify the `\"delimiter\"` option in the import statement.  \n\n:NOTE: Find a [full list of parameters here.](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dateformat#pyspark.sql.DataFrameReader.csv)"],"metadata":{}},{"cell_type":"code","source":["display(spark.read\n  .option(\"delimiter\", \"\\t\")\n  .csv(\"/mnt/training/Chicago-Crimes-2018.csv\")\n)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Spark doesn't read the header by default, as demonstrated by the column names of `_c0`, `_c1`, etc. Notice the column names are present in the first row of the DataFrame. \n\nFix this by setting the `\"header\"` option to `True`."],"metadata":{}},{"cell_type":"code","source":["display(spark.read\n  .option(\"delimiter\", \"\\t\")\n  .option(\"header\", True)\n  .csv(\"/mnt/training/Chicago-Crimes-2018.csv\")\n)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Spark didn't infer the schema, or read the timestamp format, since this file uses an atypical timestamp.  Change that by adding the option `\"timestampFormat\"` and pass it the format used in this file.  \n\nSet `\"inferSchema\"` to `True`, which triggers Spark to make an extra pass over the data to infer the schema."],"metadata":{}},{"cell_type":"code","source":["crimeDF = (spark.read\n  .option(\"delimiter\", \"\\t\")\n  .option(\"header\", True)\n  .option(\"timestampFormat\", \"mm/dd/yyyy hh:mm:ss a\")\n  .option(\"inferSchema\", True)\n  .csv(\"/mnt/training/Chicago-Crimes-2018.csv\")\n)\ndisplay(crimeDF)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["## The Design Pattern\n\nOther connections work in much the same way, whether your data sits in Cassandra, Cosmos DB, Redshift, or another common data store.  The general pattern is always:  \n<br>\n1. Define the connection point\n2. Define connection parameters such as access credentials\n3. Add necessary options\n\nAfter adhering to this, read data using `spark.read.options(<option key>, <option value>).<connection_type>(<endpoint>)`."],"metadata":{}},{"cell_type":"markdown","source":["## Exercise 1: Read Wikipedia Data\n\nRead Wikipedia data from Azure Blob Storage, accounting for its delimiter and header."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Get a Sense for the Data\n\nTake a look at the head of the data, located at `/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nprint(dbutils.fs.head('/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv', 100)) # this evaluates to the thing as %fs head /mnt/training/wikipedia/pageviews/pageviews_by_second.tsv"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["### Step 2: Import the Raw Data\n\nImport the data **without any options** and save it to `wikiDF`. Display the result."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\npath = \"/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\"\n\nwikiDF = spark.read.csv(path)\ndisplay(wikiDF)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\n\ndbTest(\"ET1-P-03-01-01\", 7200001, wikiDF.count())\ndbTest(\"ET1-P-03-01-02\", '_c0', wikiDF.columns[0])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["### Step 3: Import the Data with Options\n\nImport the data with options and save it to `wikiWithOptionsDF`.  Display the result.  Your import statement should account for:<br><br>  \n\n - The header\n - The delimiter"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\npath = \"/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\"\n\nwikiWithOptionsDF = (spark.read\n  .option(\"header\", True)\n  .option(\"delimiter\", \"\\t\")\n  .csv(path)\n)\n\ndisplay(wikiWithOptionsDF)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = set(wikiWithOptionsDF.columns)\n\ndbTest(\"ET1-P-03-02-01\", 7200000, wikiWithOptionsDF.count())\ndbTest(\"ET1-P-03-02-02\", {'requests', 'site', 'timestamp'}, cols)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["## Review\n\n**Question:** What accounts for Spark's quick rise in popularity as an ETL tool?  \n**Answer:** Spark easily accesses data virtually anywhere it lives, and the scalable framework lowers the difficulties in building connectors to access data.  Spark offers a unified API for connecting to data making reads from a CSV file, JSON data, or a database, to provide a few examples, nearly identical.  This allows developers to focus on writing their code rather than writing connectors.\n\n**Question:** What is DBFS and why is it important?  \n**Answer:** The Databricks File System (DBFS) allows access to scalable, fast, and distributed storage backed by S3 or the Azure Blob Store.\n\n**Question:** How do you connect your Spark cluster to the Azure Blob?  \n**Answer:** By mounting it. Mounts require Azure credentials such as SAS keys and give access to a virtually infinite store for your data. One other option is to define your keys in a single notebook that only you have permission to access. Click the arrow next to a notebook in the Workspace tab to define access permisions.\n\n**Question:** How do you specify parameters when reading data?  \n**Answer:** Using `.option()` during your read allows you to pass key/value pairs specifying aspects of your read.  For instance, options for reading CSV data include `header`, `delimiter`, and `inferSchema`.\n\n**Question:** What is the general design pattern for connecting to your data?  \n**Answer:** The general design pattern is as follows:\n0. Define the connection point\n0. Define connection parameters such as access credentials\n0. Add necessary options such as for headers or paralleization"],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Connecting to JDBC]($./04-Connecting-to-JDBC )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I find more information on DBFS?  \n**A:** <a href=\"https://docs.azuredatabricks.net/user-guide/dbfs-databricks-file-system.html#dbfs\" target=\"_blank\">Take a look at the Databricks documentation for more details"],"metadata":{}}],"metadata":{"name":"03-Connecting-to-Azure-Blob-Storage","notebookId":291050440997719},"nbformat":4,"nbformat_minor":0}
