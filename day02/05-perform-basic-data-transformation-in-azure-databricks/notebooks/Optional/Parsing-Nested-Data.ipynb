{"cells":[{"cell_type":"markdown","source":["# Capstone Project: Parsing Nested Data\n\nMount JSON data using DBFS, define and apply a schema, parse fields, and save the cleaned results back to DBFS.\n\n## Instructions\n\nA common source of data in ETL pipelines is <a href=\"https://kafka.apache.org/\" target=\"_blank\">Apache Kafka</a>, or the managed alternative <a href=\"https://docs.microsoft.com/azure/event-hubs/event-hubs-for-kafka-ecosystem-overview\" target=\"_blank\">Azure Event Hubs</a>. A common data type in these use cases is newline-separated JSON.\n\nFor this exercise, Tweets were streamed from the <a href=\"https://developer.twitter.com/en/docs\" target=\"_blank\">Twitter firehose API</a>.\nUse these four exercises to perform ETL on the data in this bucket:  \n<br>\n1. Extracting and Exploring the Data\n2. Defining and Applying a Schema\n3. Creating the Tables\n4. Loading the Results"],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell."],"metadata":{}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Exercise 1: Extracting and Exploring the Data\n\nFirst, review the data."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Explore the Folder Structure\n\nExplore the mount and review the directory structure. Use `%fs ls`.  The data is located in `/mnt/training/twitter/firehose/`"],"metadata":{}},{"cell_type":"code","source":["# TODO\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Step 2: Explore a Single File\n\n> \"Premature optimization is the root of all evil.\" -Sir Tony Hoare\n\nThere are a few gigabytes of Twitter data available in the directory. Hoare's law about premature optimization is applicable here.  Instead of building a schema for the entire data set and then trying it out, an iterative process is much less error prone and runs much faster. Start by working on a single file before you apply your proof of concept across the entire data set."],"metadata":{}},{"cell_type":"markdown","source":["Read a single file.  Start with `twitterstream-1-2018-01-08-18-48-00-bcf3d615-9c04-44ec-aac9-25f966490aa4`. Find this in `/mnt/training/twitter/firehose/2018/01/08/18/`.  Save the results to the variable `df`."],"metadata":{}},{"cell_type":"code","source":["# TODO\ndf = # <<FILL_IN>>"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = df.columns\n\ndbTest(\"ET1-P-08-02-01\", 1744, df.count())\ndbTest(\"ET1-P-08-02-02\", True, \"id\" in cols)\ndbTest(\"ET1-P-08-02-03\", True, \"text\" in cols)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Display the schema."],"metadata":{}},{"cell_type":"code","source":["# TODO\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Count the records in the file. Save the result to `dfCount`."],"metadata":{}},{"cell_type":"code","source":["# TODO\ndfCount = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ndbTest(\"ET1-P-08-03-01\", 1744, dfCount)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["## Exercise 2: Defining and Applying a Schema\n\nApplying schemas is especially helpful for data with many fields to sort through. With a complex dataset like this, define a schema **that captures only the relevant fields**.\n\nCapture the hashtags and dates from the data to get a sense for Twitter trends. Use the same file as above."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Understanding the Data Model\n\nIn order to apply structure to semi-structured data, you first must understand the data model.  \n\nThere are two forms of data models to employ: a relational or non-relational model.<br><br>\n* **Relational models** are within the domain of traditional databases. [Normalization](https://en.wikipedia.org/wiki/Database_normalization) is the primary goal of the data model. <br>\n* **Non-relational data models** prefer scalability, performance, or flexibility over normalized data.\n\nUse the following relational model to define a number of tables to join together on different columns, in order to reconstitute the original data. Regardless of the data model, the ETL principles are roughly the same.\n\nCompare the following [Entity-Relationship Diagram](https://en.wikipedia.org/wiki/Entity%E2%80%93relationship_model) to the schema you printed out in the previous step to get a sense for how to populate the tables."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/ER-diagram.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Step 2: Create a Schema for the `Tweet` Table\n\nCreate a schema for the JSON data to extract just the information that is needed for the `Tweet` table, parsing each of the following fields in the data model:\n\n| Field | Type|\n|-------|-----|\n| tweet_id | integer |\n| user_id | integer |\n| language | string |\n| text | string |\n| created_at | string* |\n\n*Note: Start with `created_at` as a string. Turn this into a timestamp later.\n\nSave the schema to `tweetSchema`, use it to create a dataframe named `tweetDF`, and use the same file used in the exercise above: `\"/mnt/training/twitter/firehose/2018/01/08/18/twitterstream-1-2018-01-08-18-48-00-bcf3d615-9c04-44ec-aac9-25f966490aa4\"`.\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** You might need to reexamine the data schema. <br>\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** [Import types from `pyspark.sql.types`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=pyspark%20sql%20types#module-pyspark.sql.types)."],"metadata":{}},{"cell_type":"code","source":["# TODO\npath = \"/mnt/training/twitter/firehose/2018/01/08/18/twitterstream-1-2018-01-08-18-48-00-bcf3d615-9c04-44ec-aac9-25f966490aa4\"\n\ntweetSchema = # FILL_IN\ntweetDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nfrom pyspark.sql.functions import col\n\nschema = tweetSchema.fieldNames()\nschema.sort()\ntweetCount = tweetDF.filter(col(\"id\").isNotNull()).count()\n\ndbTest(\"ET1-P-08-04-01\", 'created_at', schema[0])\ndbTest(\"ET1-P-08-04-02\", 'id', schema[1])\ndbTest(\"ET1-P-08-04-03\", 1491, tweetCount)\n\nassert schema[0] == 'created_at' and schema[1] == 'id'\nassert tweetCount == 1491\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### Step 3: Create a Schema for the Remaining Tables\n\nFinish off the full schema, save it to `fullTweetSchema`, and use it to create the DataFrame `fullTweetDF`. Your schema should parse all the entities from the ER diagram above.  Remember, smart small, run your code, and then iterate."],"metadata":{}},{"cell_type":"code","source":["# TODO\npath = \"/mnt/training/twitter/firehose/2018/01/08/18/twitterstream-1-2018-01-08-18-48-00-bcf3d615-9c04-44ec-aac9-25f966490aa4\"\nfullTweetSchema = # FILL_IN\nfullTweetDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nfrom pyspark.sql.functions import col\n\nschema = fullTweetSchema.fieldNames()\nschema.sort()\ntweetCount = fullTweetDF.filter(col(\"id\").isNotNull()).count()\n\nassert tweetCount == 1491\n\ndbTest(\"ET1-P-08-05-01\", \"created_at\", schema[0])\ndbTest(\"ET1-P-08-05-02\", \"entities\", schema[1])\ndbTest(\"ET1-P-08-05-03\", 1491, tweetCount)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["## Exercise 3: Creating the Tables\n\nApply the schema you defined to create tables that match the relational data model."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Filtering Nulls\n\nThe Twitter data contains both deletions and tweets.  This is why some records appear as null values. Create a DataFramed called `fullTweetFilteredDF` that filters out the null values."],"metadata":{}},{"cell_type":"code","source":["# TODO\nfullTweetFilteredDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ndbTest(\"ET1-P-08-06-01\", 1491, fullTweetFilteredDF.count())\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["-sandbox\n### Step 2: Creating the `Tweet` Table\n\nTwitter uses a non-standard timestamp format that Spark doesn't recognize. Currently the `created_at` column is formatted as a string. Create the `Tweet` table and save it as `tweetDF`. Parse the timestamp column using `unix_timestamp`, and cast the result as `TimestampType`. The timestamp format is `EEE MMM dd HH:mm:ss ZZZZZ yyyy`.\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** Use `alias` to alias the name of your columns to the final name you want for them.  \n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** `id` corresponds to `tweet_id` and `user.id` corresponds to `user_id`."],"metadata":{}},{"cell_type":"code","source":["# TODO\ntimestampFormat = \"EEE MMM dd HH:mm:ss ZZZZZ yyyy\"\ntweetDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nfrom pyspark.sql.types import TimestampType\nt = tweetDF.select(\"createdAt\").schema[0]\n\ndbTest(\"ET1-P-08-07-01\", TimestampType(), t.dataType)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### Step 3: Creating the Account Table\n\nSave the account table as `accountDF`."],"metadata":{}},{"cell_type":"code","source":["# TODO\naccountDF = # <<FILL_IN>>"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = accountDF.columns\n\ndbTest(\"ET1-P-08-08-01\", True, \"friendsCount\" in cols)\ndbTest(\"ET1-P-08-08-02\", True, \"screenName\" in cols)\ndbTest(\"ET1-P-08-08-03\", 1491, accountDF.count())\n\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["-sandbox\n### Step 4: Creating Hashtag and URL Tables Using `explode`\n\nEach tweet in the data set contains zero, one, or many URLs and hashtags. Parse these using the `explode` function so each URL or hashtag has its own row.\n\nIn this example, `explode` gives one row from the original column `hashtags` for each value in an array. All other columns are left untouched.\n\n```\n+---------------+--------------------+----------------+\n|     screenName|            hashtags|explodedHashtags|\n+---------------+--------------------+----------------+\n|        zooeeen|[[Tea], [GoldenGl...|           [Tea]|\n|        zooeeen|[[Tea], [GoldenGl...|  [GoldenGlobes]|\n|mannydidthisone|[[beats], [90s], ...|         [beats]|\n|mannydidthisone|[[beats], [90s], ...|           [90s]|\n|mannydidthisone|[[beats], [90s], ...|     [90shiphop]|\n|mannydidthisone|[[beats], [90s], ...|           [pac]|\n|mannydidthisone|[[beats], [90s], ...|        [legend]|\n|mannydidthisone|[[beats], [90s], ...|          [thug]|\n|mannydidthisone|[[beats], [90s], ...|         [music]|\n|mannydidthisone|[[beats], [90s], ...|     [westcoast]|\n|mannydidthisone|[[beats], [90s], ...|        [eminem]|\n|mannydidthisone|[[beats], [90s], ...|         [drdre]|\n|mannydidthisone|[[beats], [90s], ...|          [trap]|\n|  Satish0919995|[[BB11], [BiggBos...|          [BB11]|\n|  Satish0919995|[[BB11], [BiggBos...|    [BiggBoss11]|\n|  Satish0919995|[[BB11], [BiggBos...| [WeekendKaVaar]|\n+---------------+--------------------+----------------+\n```\n\nThe concept of `explode` is similar to `pivot`.\n\nCreate the rest of the tables and save them to the following DataFrames:<br><br>\n\n* `hashtagDF`\n* `urlDF`\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=explode#pyspark.sql.functions.explode\" target=\"_blank\">Find the documentation for `explode` here</a>"],"metadata":{}},{"cell_type":"code","source":["# TODO\nhashtagDF = # FILL_IN\nurlDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nhashtagCols = hashtagDF.columns\nurlCols = urlDF.columns\nhashtagDFCounts = hashtagDF.count()\nurlDFCounts = urlDF.count()\n\ndbTest(\"ET1-P-08-09-01\", True, \"hashtag\" in hashtagCols)\ndbTest(\"ET1-P-08-09-02\", True, \"displayURL\" in urlCols)\ndbTest(\"ET1-P-08-09-03\", 394, hashtagDFCounts)\ndbTest(\"ET1-P-08-09-04\", 368, urlDFCounts)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["-sandbox\n## Exercise 4: Loading the Results\n\nUse DBFS as your target warehouse for your transformed data. Save the DataFrames in Parquet format to the following endpoints:  \n\n| DataFrame    | Endpoint              |\n|:-------------|:----------------------|\n| `accountDF`  | `/tmp/account.parquet`|\n| `tweetDF`    | `/tmp/tweet.parquet`  |\n| `hashtagDF`  | `/tmp/hashtag.parquet`|\n| `urlDF`      | `/tmp/url.parquet`    |\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> If you run out of storage in `/tmp`, use `.limit(10)` to limit the size of your DataFrames to 10 records."],"metadata":{}},{"cell_type":"code","source":["# TODO\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nfrom pyspark.sql.dataframe import DataFrame\n\naccountDF = spark.read.parquet(\"/tmp/account.parquet\")\ntweetDF = spark.read.parquet(\"/tmp/tweet.parquet\")\nhashtagDF = spark.read.parquet(\"/tmp/hashtag.parquet\")\nurlDF = spark.read.parquet(\"/tmp/url.parquet\")\n\ndbTest(\"ET1-P-08-10-01\", DataFrame, type(accountDF))\ndbTest(\"ET1-P-08-10-02\", DataFrame, type(tweetDF))\ndbTest(\"ET1-P-08-10-03\", DataFrame, type(hashtagDF))\ndbTest(\"ET1-P-08-10-04\", DataFrame, type(urlDF))\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":40}],"metadata":{"name":"Parsing-Nested-Data","notebookId":291050440997920},"nbformat":4,"nbformat_minor":0}
