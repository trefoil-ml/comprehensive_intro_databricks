{"cells":[{"cell_type":"markdown","source":["# Advanced UDFs\n\nApache Spark&trade; and Azure Databricks&reg; allow you to create your own User Defined Functions (UDFs) specific to the needs of your data."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Complex Transformations\n \nUDFs provide custom, generalizable code that you can apply to ETL workloads when Spark's built-in functions won't suffice.  \nIn the last lesson we covered a simple version of this: UDFs that take a single DataFrame column input and return a primitive value. Often a more advanced solution is needed.\n\nUDFs can take multiple column inputs. While UDFs cannot return multiple columns, they can return complex, named types that are easily accessible. This approach is especially helpful in ETL workloads that need to clean complex and challenging data structures.\n\nAnother other option is the new vectorized, or pandas, UDFs available in Spark 2.3. These allow for more performant UDFs written in Python.<br><br>\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-2/pandas-udfs.png\" style=\"height: 400px; margin: 20px\"/></div>"],"metadata":{}},{"cell_type":"markdown","source":["### UDFs with Multiple Columns\n\nTo begin making more complex UDFs, start by using multiple column inputs.  This is as simple as adding extra inputs to the function or lamda you convert to the UDF."],"metadata":{}},{"cell_type":"markdown","source":["Run the cell below to mount the data."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Write a basic function that combines two columns."],"metadata":{}},{"cell_type":"code","source":["def manual_add(x, y):\n  return x + y\n\nmanual_add(1, 2)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Register the function as a UDF by binding it with a Python variable, adding a name to access it in the SQL API and giving it a return type."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\n\nmanualAddPythonUDF = spark.udf.register(\"manualAddSQLUDF\", manual_add, IntegerType())"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Create a dummy DataFrame to apply the UDF."],"metadata":{}},{"cell_type":"code","source":["integerDF = (spark.createDataFrame([\n  (1, 2),\n  (3, 4),\n  (5, 6)\n], [\"col1\", \"col2\"]))\n\ndisplay(integerDF)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Apply the UDF to your DataFrame."],"metadata":{}},{"cell_type":"code","source":["integerAddDF = integerDF.select(\"*\", manualAddPythonUDF(\"col1\", \"col2\").alias(\"sum\"))\n\ndisplay(integerAddDF)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### UDFs with Complex Output\n\nComplex outputs are helpful when you need to return multiple values from your UDF. The UDF design pattern involves returning a single column to drill down into, to pull out the desired data."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nStart by determining the desired output.  This will look like a schema with a high level `StructType` with numerous `StructFields`.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> For a refresher on this, see the lesson **Applying Schemas to JSON Data** in ETL Part 1 module"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import FloatType, StructType, StructField\n\nmathOperationsSchema = StructType([\n  StructField(\"sum\", FloatType(), True), \n  StructField(\"multiplication\", FloatType(), True), \n  StructField(\"division\", FloatType(), True) \n])"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Create a function that returns a tuple of your desired output."],"metadata":{}},{"cell_type":"code","source":["def manual_math(x, y):\n  return (float(x + y), float(x * y), x / float(y))\n\nmanual_math(1, 2)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Register your function as a UDF and apply it.  In this case, your return type is the schema you created."],"metadata":{}},{"cell_type":"code","source":["manualMathPythonUDF = spark.udf.register(\"manualMathSQLUDF\", manual_math, mathOperationsSchema)\n\ndisplay(integerDF.select(\"*\", manualMathPythonUDF(\"col1\", \"col2\").alias(\"sum\")))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Vectorized UDFs in Python\n\nStarting in Spark 2.3, vectorized UDFs can be written in Python called Pandas UDFs.  This alleviates some of the serialization and invocation overhead of conventional Python UDFs.  While there are a number of types of these UDFs, this walkthrough focuses on scalar UDFs. This is an ideal solution for Data Scientists needing performant UDFs written in Python.\n\n:NOTE: Your cluster will need to run Spark 2.3 in order to execute the following code."],"metadata":{}},{"cell_type":"markdown","source":["Use the decorator syntax to designate a Pandas UDF.  The input and outputs are both Pandas series of doubles."],"metadata":{}},{"cell_type":"code","source":["%python\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\n\n@pandas_udf('double', PandasUDFType.SCALAR)\ndef pandas_plus_one(v):\n    return v + 1"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Create a DataFrame to apply the UDF."],"metadata":{}},{"cell_type":"code","source":["%python\nfrom pyspark.sql.functions import col, rand\n\ndf = spark.range(0, 10 * 1000 * 1000)\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Apply the UDF"],"metadata":{}},{"cell_type":"code","source":["%python\ndisplay(df.withColumn('id_transformed', pandas_plus_one(\"id\")))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["%md ## Exercise 1: Multiple Column Inputs to Complex Type\n\nGiven a DataFrame of weather in various units, write a UDF that translates a column for temperature and a column for units into a complex type for temperature in three units:<br><br>\n\n* fahrenheit\n* celsius\n* kelvin"],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Import and Explore the Data\n\nImport the data sitting in `/mnt/training/weather/StationData/stationData.parquet` and save it to `weatherDF`."],"metadata":{}},{"cell_type":"code","source":["# TODO\nweatherDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = set(weatherDF.columns)\n\ndbTest(\"ET2-P-04-01-01\", 2559, weatherDF.count())\ndbTest(\"ET2-P-04-01-02\", True, \"TAVG\" in cols and \"UNIT\" in cols)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### Step 2: Define Complex Output Type\n\nDefine the complex output type for your UDF.  This should look like the following:\n\n| Field Name | Type |\n|:-----------|:-----|\n| fahrenheit | Float |\n| celsius | Float |\n| kelvin | Float |"],"metadata":{}},{"cell_type":"code","source":["# TODO\nschema = FILL_IN"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nfrom pyspark.sql.types import FloatType\nnames = [i.name for i in schema.fields]\n\ndbTest(\"ET2-P-04-02-01\", 3, len(schema.fields))\ndbTest(\"ET2-P-04-02-02\", [FloatType(), FloatType(), FloatType()], [i.dataType for i in schema.fields])\ndbTest(\"ET2-P-04-02-03\", True, \"fahrenheit\" in names and \"celsius\" in names and \"kelvin\" in names)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["### Step 3: Create the Function\n\nCreate a function that takes `temperature` as a Float and `unit` as a String.  `unit` will either be `F` for fahrenheit or `C` for celsius.  Return a tuple of floats of that value as `(fahrenheit, celsius, kelvin)`.  Use the following equations:\n\n| From | To Fahrenheit | To Celsius | To Kelvin |\n|:-----|:--------------|:-----------|:-----------|\n| Fahrenheit | F | (F - 32) * 5/9 | (F - 32) * 5/9 + 273.15 |\n| Celsius | (C * 9/5) + 32 | C | C + 273.15 |\n| Kelvin | (K - 273.15) * 9/5 + 32 | K - 273.15 | K |"],"metadata":{}},{"cell_type":"code","source":["# TODO\ndef temperatureConverter( # FILL_IN ):"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ndbTest(\"ET2-P-04-03-01\", (194.0, 90, 363.15), temperatureConverter(90, \"C\"))\ndbTest(\"ET2-P-04-03-02\", (0, -17.77777777777778, 255.3722222222222), temperatureConverter(0, \"F\"))\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["### Step 4: Register the UDF\n\nRegister the UDF as `temperatureConverterUDF`"],"metadata":{}},{"cell_type":"code","source":["# TODO\ntemperatureConverterUDF = FILL_IN"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ndbTest(\"ET2-P-04-04-01\", (194.0, 90, 363.15), temperatureConverterUDF.func(90, \"C\"))\ndbTest(\"ET2-P-04-04-02\", (0, -17.77777777777778, 255.3722222222222), temperatureConverterUDF.func(0, \"F\"))\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["### Step 5: Apply your UDF\n\nCreate `weatherEnhancedDF` with a new column `TAVGAdjusted` that applies your UDF."],"metadata":{}},{"cell_type":"code","source":["# TODO\nweatherEnhancedDF = FILL_IN\n\ndisplay(weatherEnhancedDF)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nresult = weatherEnhancedDF.select(\"TAVGAdjusted\").first()[0].asDict()\n\ndbTest(\"ET2-P-04-05-01\", {'fahrenheit': 61.0, 'celsius': 16.11111068725586, 'kelvin': 289.2611083984375}, result)\ndbTest(\"ET2-P-04-05-02\", 2559, weatherEnhancedDF.count())\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["## Review\n\n**Question:** How do UDFs handle multiple column inputs and complex outputs?   \n**Answer:** UDFs allow for multiple column inputs.  Complex outputs can be designated with the use of a defined schema encapsulate in a `StructType()` or a Scala case class.\n\n**Question:** How can I do vectorized UDFs in Python and are they as performant as built-in functions?   \n**Answer:** Spark 2.3 includes the use of vectorized UDFs using Pandas syntax. Even though they are vectorized, these UDFs will not be as performant built-in functions, though they will be more performant than non-vectorized Python UDFs."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Joins and Lookup Tables]($./05-Joins-and-Lookup-Tables )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I find out more about UDFs?  \n**A:** Take a look at the <a href=\"https://docs.azuredatabricks.net/spark/latest/spark-sql/udf-scala.html#user-defined-functions-scala\" target=\"_blank\">Databricks documentation for more details</a>\n\n**Q:** Where can I find out more about vectorized UDFs in Python?  \n**A:** Take a look at the <a href=\"https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html\" target=\"_blank\">Databricks blog for more details</a>\n\n**Q:** Where can I find out more about User Defined Agregate Funcations?  \n**A:** Take a look at the <a href=\"https://docs.azuredatabricks.net/spark/latest/spark-sql/udaf-scala.html#user-defined-aggregate-functions-scala\" target=\"_blank\">Databricks documentation for more details</a>"],"metadata":{}}],"metadata":{"name":"04-Advanced-UDFs","notebookId":291050440996748},"nbformat":4,"nbformat_minor":0}
