{"cells":[{"cell_type":"markdown","source":["# Joins and Lookup Tables\n\nApache Spark&trade; and Azure Databricks&reg; allow you to join new records to existing tables in an ETL job."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Shuffle and Broadcast Joins\n\nA common use case in ETL jobs involves joining new data to either lookup tables or historical data. You need different considerations to guide this process when working with distributed technologies such as Spark, rather than traditional databases that sit on a single machine.\n\nTraditional databases join tables by pairing values on a given column. When all the data sits in a single database, it often goes unnoticed how computationally expensive row-wise comparisons are.  When data is distributed across a cluster, the expense of joins becomes even more apparent.\n\n**A standard (or shuffle) join** moves all the data on the cluster for each table to a given node on the cluster. This is expensive not only because of the computation needed to perform row-wise comparisons, but also because data transfer across a network is often the biggest performance bottleneck of distributed systems.\n\nBy contrast, **a broadcast join** remedies this situation when one DataFrame is sufficiently small. A broadcast join duplicates the smaller of the two DataFrames on each node of the cluster, avoiding the cost of shuffling the bigger DataFrame.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-2/shuffle-and-broadcast-joins.png\" style=\"height: 400px; margin: 20px\"/></div>"],"metadata":{}},{"cell_type":"markdown","source":["### Lookup Tables\n\nLookup tables are normally small, historical tables used to enrich new data passing through an ETL pipeline."],"metadata":{}},{"cell_type":"markdown","source":["Run the cell below to mount the data."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Import a small table that will enrich new data coming into a pipeline."],"metadata":{}},{"cell_type":"code","source":["labelsDF = spark.read.parquet(\"/mnt/training/day-of-week\")\n\ndisplay(labelsDF)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Import a larger DataFrame that gives a column to combine back to the lookup table. In this case, use Wikipedia site requests data."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, date_format\n\npageviewsDF = (spark.read\n  .parquet(\"/mnt/training/wikipedia/pageviews/pageviews_by_second.parquet/\")\n  .withColumn(\"dow\", date_format(col(\"timestamp\"), \"u\").alias(\"dow\"))\n)\n\ndisplay(pageviewsDF)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Join the two DataFrames together."],"metadata":{}},{"cell_type":"code","source":["pageviewsEnhancedDF = pageviewsDF.join(labelsDF, \"dow\")\n\ndisplay(pageviewsEnhancedDF)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Now aggregate the results to see trends by day of the week.\n\n:NOTE: `pageviewsEnhancedDF` is a large DataFrame so it can take a while to process depending on the size of your cluster."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\naggregatedDowDF = (pageviewsEnhancedDF\n  .groupBy(col(\"dow\"), col(\"longName\"), col(\"abbreviated\"), col(\"shortName\"))  \n  .sum(\"requests\")                                             \n  .withColumnRenamed(\"sum(requests)\", \"Requests\")\n  .orderBy(col(\"dow\"))\n)\n\ndisplay(aggregatedDowDF)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["-sandbox\n### Exploring Broadcast Joins\n\nIn joining these two DataFrames together, no type of join was specified.  In order to examine this, look at the physical plan used to return the query. This can be done with the `.explain()` DataFrame method. Look for **BroadcastHashJoin** and/or **BroadcastExchange**.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-2/broadcasthashjoin.png\" style=\"height: 400px; margin: 20px\"/></div>"],"metadata":{}},{"cell_type":"code","source":["aggregatedDowDF.explain()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["By default, Spark did a broadcast join rather than a shuffle join.  In other words, it broadcast `labelsDF` to the larger `pageviewsDF`, replicating the smaller DataFrame on each node of our cluster.  This avoided having to mover the larger DataFrame across the cluster.\n\nTake a look at the broadcast threshold by accessing the configuration settings."],"metadata":{}},{"cell_type":"code","source":["threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\nprint(\"Threshold: {0:,}\".format( int(threshold) ))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["This is the maximize size in bytes for a table that broadcast to worker nodes.  Dropping it to `-1` disables broadcasting."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Now notice the lack of broadcast in the query physical plan."],"metadata":{}},{"cell_type":"code","source":["pageviewsDF.join(labelsDF, \"dow\").explain()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Next reset the original threshold."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", threshold)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Explicitly Broadcasting Tables\n\nThere are two ways of telling Spark to explicitly broadcast tables. The first is to change the Spark configuration, which affects all operations. The second is to declare it using the `broadcast()` function in the `functions` package."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import broadcast\n\npageviewsDF.join(broadcast(labelsDF), \"dow\").explain()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["## Exercise 1: Join a Lookup Table\n\nJoin a table that includes country name to a lookup table containing the full country name."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Import the Data\n\nCreate the following DataFrames:<br><br>\n\n- `countryLookupDF`: A lookup table with ISO country codes located at `/mnt/training/countries/ISOCountryCodes/ISOCountryLookup.parquet`\n- `logWithIPDF`: A server log including the results from an IPLookup table located at `/mnt/training/EDGAR-Log-20170329/enhanced/logDFwithIP.parquet`"],"metadata":{}},{"cell_type":"code","source":["# TODO\ncountryLookupDF = # FILL_IN\nlogWithIPDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ndbTest(\"ET2-P-05-01-01\", 249, countryLookupDF.count())\ndbTest(\"ET2-P-05-01-02\", 5000, logWithIPDF.count())\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["### Step 2: Broadcast the Lookup Table\n\nComplete the following:<br><br>\n\n- Create a new DataFrame `logWithIPEnhancedDF`\n- Get the full country name by performing a broadcast join that broadcasts the lookup table to the server log\n- Drop all columns other than `EnglishShortName`"],"metadata":{}},{"cell_type":"code","source":["# TODO\nlogWithIPEnhancedDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = set(logWithIPEnhancedDF.columns)\n\ndbTest(\"ET2-P-05-02-01\", True, \"EnglishShortName\" in cols and \"ip\" in cols)\ndbTest(\"ET2-P-05-02-02\", True, \"alpha2Code\" not in cols and \"ISO31662SubdivisionCode\" not in cols)\ndbTest(\"ET2-P-05-02-03\", 5000, logWithIPEnhancedDF.count())\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["## Review\n**Question:** Why are joins expensive operations?  \n**Answer:** Joins perform a large number of row-wise comparisons, making the cost associated with joining tables grow with the size of the data in the tables.\n\n**Question:** What is the difference between a shuffle and broadcast join? How does Spark manage these differences?  \n**Answer:** A shuffle join shuffles data between nodes in a cluster. By contrast, a broadcast join moves the smaller of two DataFrames to where the larger DataFrame sits, minimizing the overall data transfer. By default, Spark performs a broadcast join if the total number of records is below a certain threshold. The threshold can be manually specified or you can manually specify that a broadcast join should take place. Since the automatic determination of whether a shuffle join should take place is by number of records, this could mean that really wide data would take up significantly more space per record and should therefore be specified manually.\n\n**Question:** What is a lookup table?  \n**Answer:** A lookup table is small table often used for referencing commonly used data such as mapping cities to countries."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Database Writes]($./06-Database-Writes )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I get more information on optimizing table joins where data skew is an issue?  \n**A:** Check out the Databricks documentation on <a href=\"https://docs.azuredatabricks.net/spark/latest/spark-sql/skew-join.html\" target=\"_blank\">Skew Join Optimization</a>"],"metadata":{}}],"metadata":{"name":"05-Joins-and-Lookup-Tables","notebookId":291050440996712},"nbformat":4,"nbformat_minor":0}
