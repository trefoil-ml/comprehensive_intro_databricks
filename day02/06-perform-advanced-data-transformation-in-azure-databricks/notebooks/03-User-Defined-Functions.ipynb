{"cells":[{"cell_type":"markdown","source":["# User Defined Functions\n\nApache Spark&trade; and Azure Databricks&reg; allow you to create your own User Defined Functions (UDFs) specific to the needs of your data."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Custom Transformations with User Defined Functions\n\nSpark's built-in functions provide a wide array of functionality, covering the vast majority of data transformation use cases. Often what differentiates strong Spark programmers is their ability to utilize built-in functions since Spark offers many highly optimized options to manipulate data. This matters for two reasons:<br><br>\n\n- First, *built-in functions are finely tuned* so they run faster than less efficient code provided by the user.  \n- Secondly, Spark (or, more specifically, Spark's optimization engine, the Catalyst Optimizer) knows the objective of built-in functions so it can *optimize the execution of your code by changing the execution order of your tasks.* \n\nIn brief, use built-in functions whenever possible.\n\nThere are, however, many specific use cases not covered by built-in functions. **User Defined Functions (UDFs) are useful when you need to define logic specific to your use case and when you need to encapsulate that solution for reuse.** They should only be used when there is no clear way to accomplish a task using built-in functions.\n\nUDFs are generally more performant in Scala than Python since for Python, Spark has to spin up a Python interpreter on every exector to run the function. This causes a substantial performance bottleneck due to communication accross the Py4J bridge (how the JVM interoperates with Python) and the slower nature of Python execution.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-2/built-in-vs-udfs.png\" style=\"height: 400px; margin: 20px\"/></div>"],"metadata":{}},{"cell_type":"markdown","source":["### A Basic UDF\n\nUDFs take a function or lambda and make it available for Spark to use.  Start by writing code in your language of choice that will operate on a single row of a single column in your DataFrame."],"metadata":{}},{"cell_type":"markdown","source":["Run the cell below to mount the data."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Write a basic function that splits a string on an `e`."],"metadata":{}},{"cell_type":"code","source":["def manual_split(x):\n  return x.split(\"e\")\n\nmanual_split(\"this is my example string\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Register the function as a UDF by designating the following:\n\n* A name for access in Python (`manualSplitPythonUDF`)\n* A name for access in SQL (`manualSplitSQLUDF`)\n* The function itself (`manual_split`)\n* The return type for the function (`StringType`)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StringType\n\nmanualSplitPythonUDF = spark.udf.register(\"manualSplitSQLUDF\", manual_split, StringType())"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Create a dataframe of 100k values with a string to index. Do this by using a hash function."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import sha1, rand\nrandomDF = (spark.range(1, 10000 * 10 * 10 * 10)\n  .withColumn(\"random_value\", rand(seed=10).cast(\"string\"))\n  .withColumn(\"hash\", sha1(\"random_value\"))\n  .drop(\"random_value\")\n)\n\ndisplay(randomDF)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Apply the UDF by using it just like any other Spark function."],"metadata":{}},{"cell_type":"code","source":["randomAugmentedDF = randomDF.select(\"*\", manualSplitPythonUDF(\"hash\").alias(\"augmented_col\"))\n\ndisplay(randomAugmentedDF)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### DataFrame and SQL APIs\n\nWhen you registered the UDF, it was named `manualSplitSQLUDF` for access in the SQL API. This gives us the same access to the UDF you had in the python DataFrames API."],"metadata":{}},{"cell_type":"markdown","source":["Register `randomDF` to access it within SQL."],"metadata":{}},{"cell_type":"code","source":["randomDF.createOrReplaceTempView(\"randomTable\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Now switch to the SQL API and use the same UDF."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT id,\n  hash,\n  manualSplitSQLUDF(hash) as augmented_col\nFROM\n  randomTable"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["This is an easy way to generalize UDFs, allowing teams to share their code."],"metadata":{}},{"cell_type":"markdown","source":["### Performance Tradeoffs\n\nThe performance of custom UDFs normally trail far behind built-in functions.  Take a look at this other example to compare built-in functions to custom UDFs."],"metadata":{}},{"cell_type":"markdown","source":["Create a large DataFrame of random values, cache the result, and perform a `.count()` to actualize the cache."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, rand\n\nrandomFloatsDF = (spark.range(0, 100 * 1000 * 1000)\n  .withColumn(\"id\", (col(\"id\") / 1000).cast(\"integer\"))\n  .withColumn(\"random_float\", rand())\n)\n\nrandomFloatsDF.cache()\nrandomFloatsDF.count()\n\ndisplay(randomFloatsDF)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Register a new UDF that increments a column by 1.  Here, use a lambda instead of a function."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import FloatType\n  \nplusOneUDF = spark.udf.register(\"plusOneUDF\", lambda x: x + 1, FloatType())"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Compare the results using the `%timeit` function.  Run it a few times and examine the results."],"metadata":{}},{"cell_type":"code","source":["%timeit randomFloatsDF.withColumn(\"incremented_float\", plusOneUDF(\"random_float\")).count()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%timeit randomFloatsDF.withColumn(\"incremented_float\", col(\"random_float\") + 1).count()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Which was faster, the UDF or the built-in functionality?  By how much?  This can differ based upon whether you work through this course in Scala (which is much faster) or Python."],"metadata":{}},{"cell_type":"markdown","source":["## Exercise 1: Converting IP Addresses to Decimals\n\nWrite a UDF that translates an IPv4 address string (e.g. `123.123.123.123`) into a numeric value."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Create a Function\n\nIP addresses pose challenges for efficient database lookups.  One way of dealing with this is to store an IP address in numerical form.  Write the function `IPConvert` that satisfies the following:\n\nInput: IP address as a string (e.g. `123.123.123.123`)  \nOutput: an integer representation of the IP address (e.g. `2071690107`)\n\nIf the input string is `1.2.3.4`, think of it like `A.B.C.D` where A is 1, B is 2, etc. Solve this with the following steps:\n\n&nbsp;&nbsp;&nbsp; (A x 256^3) + (B x 256^2) + (C x 256) + D <br>\n&nbsp;&nbsp;&nbsp; (1 x 256^3) + (2 x 256^2) + (3 x 256) + 4 <br>\n&nbsp;&nbsp;&nbsp; 116777216 + 131072 + 768 + 4 <br>\n&nbsp;&nbsp;&nbsp; 16909060\n\nMake a function to implement this."],"metadata":{}},{"cell_type":"code","source":["# TODO\ndef IPConvert(IPString):\n  FILL_IN\n\nIPConvert(\"1.2.3.4\") # should equal 16909060"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ndbTest(\"ET2-P-03-01-01\", 16909060, IPConvert(\"1.2.3.4\"))\ndbTest(\"ET2-P-03-01-02\", 168430090, IPConvert(\"10.10.10.10\"))\ndbTest(\"ET2-P-03-01-03\", 386744599, IPConvert(\"23.13.65.23\"))\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["### Step 2: Register a UDF\n\nRegister your function as `IPConvertUDF`.  Be sure to use `LongType` as your output type."],"metadata":{}},{"cell_type":"code","source":["# TODO\nIPConvertUDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ntestDF = spark.createDataFrame((\n  (\"1.2.3.4\", ),\n  (\"10.10.10.10\", ),\n  (\"23.13.65.23\", )\n), (\"ip\",))\nresult = [i[0] for i in testDF.select(IPConvertUDF(\"ip\")).collect()]\n\ndbTest(\"ET2-P-03-02-01\", 16909060, result[0])\ndbTest(\"ET2-P-03-02-02\", 168430090, result[1])\ndbTest(\"ET2-P-03-02-03\", 386744599, result[2])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["### Step 3: Apply the UDF\n\nApply the UDF on the `IP` column of the DataFrame created below, creating the new column `parsedIP`."],"metadata":{}},{"cell_type":"code","source":["# TODO\nIPDF = spark.createDataFrame([[\"123.123.123.123\"], [\"1.2.3.4\"], [\"127.0.0.0\"]], ['ip'])\n\nIPDFWithParsedIP = # FILL_IN\n\ndisplay(IPDFWithParsedIP)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nresult2 = [i[1] for i in IPDFWithParsedIP.collect()]\n\ndbTest(\"ET2-P-03-03-01\", 2071690107, result2[0])\ndbTest(\"ET2-P-03-03-02\", 16909060, result2[1])\ndbTest(\"ET2-P-03-03-03\", 2130706432, result2[2])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["## Review\n**Question:** What are the performance tradeoffs between UDFs and built-in functions?  When should I use each?  \n**Answer:** Built-in functions are normally faster than UDFs and should be used when possible.  UDFs should be used when specific use cases arise that aren't addressed by built-in functions.\n\n**Question:** How can I use UDFs?  \n**Answer:** UDFs can be used in any Spark API. They can be registered for use in SQL and can otherwise be used in Scala, Python, R, and Java.\n\n**Question:** Why are built-in functions faster?  \n**Answer:** Reasons include:\n* The catalyst optimizer knows how to optimize built-in functions\n* They are written in highly optimized Scala\n* There is no serialization cost at the time of running a built-in function\n\n**Question:** Can UDFs have multiple column inputs and outputs?  \n**Answer:** Yes, UDFs can have multiple column inputs and multiple complex outputs. This is covered in the following lesson."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Advanced UDFs]($./04-Advanced-UDFs )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I find out more about UDFs?  \n**A:** Take a look at the <a href=\"https://docs.azuredatabricks.net/spark/latest/spark-sql/udf-scala.html#user-defined-functions-scala\" target=\"_blank\">Databricks documentation for more details</a>"],"metadata":{}}],"metadata":{"name":"03-User-Defined-Functions","notebookId":291050440996670},"nbformat":4,"nbformat_minor":0}
