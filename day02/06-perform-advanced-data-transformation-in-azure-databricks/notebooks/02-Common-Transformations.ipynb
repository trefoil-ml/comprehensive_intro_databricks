{"cells":[{"cell_type":"markdown","source":["# Common Transformations\n\nApache Spark&trade; and Azure Databricks&reg; allow you to manipulate data with built-in functions that accommodate common design patterns."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Transformations in ETL\n\nThe goal of transformations in ETL is to transform raw data in order to populate a data model.  The most common models are **relational models** and **snowflake (or star) schemas,** though other models such as query-first modeling also exist. \n\nTransforming data can range in complexity from simply parsing relavant fields to handling null values without affecting downstream operations and applying complex conditional logic.  Common transformations include:<br><br>\n\n* Normalizing values\n* Imputing null or missing data\n* Deduplicating data\n* Performing database rollups\n* Exploding arrays\n* Pivoting DataFrames\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-2/data-models.png\" style=\"height: 400px; margin: 20px\"/></div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Built-In Functions\n\nBuilt-in functions offer a range of performant options to manipulate data. This includes options familiar to:<br><br>\n\n1. SQL users such as `.select()` and `.groupBy()`\n2. Python, Scala and R users such as `max()` and `sum()`\n3. Data warehousing options such as `rollup()` and `cube()`"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Getting Started\n\nRun the following cell to configure our \"classroom.\"\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Remember to attach your notebook to a cluster. Click <b>Detached</b> in the upper left hand corner and then select your preferred cluster.\n\n<img src=\"https://files.training.databricks.com/images/eLearning/attach-to-cluster.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"],"metadata":{}},{"cell_type":"markdown","source":["Run the cell below to mount the data."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Normalizing Data\n\nNormalizing refers to different practices including restructuring data in normal form to reduce redundancy, and scaling data down to a small, specified range. For this case, bound a range of integers between 0 and 1."],"metadata":{}},{"cell_type":"markdown","source":["Start by taking a DataFrame of a range of integers"],"metadata":{}},{"cell_type":"code","source":["integerDF = spark.range(1000, 10000)\n\ndisplay(integerDF)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["-sandbox\n\nTo normalize these values between 0 and 1, subtract the minimum and divide by the maximum, minus the minimum.\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=minmaxscaler#pyspark.ml.feature.MinMaxScaler\" target=\"_blank\">Also see the bult-in function `MinMaxScaler`</a>"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, max, min\n\ncolMin = integerDF.select(min(\"id\")).first()[0]\ncolMax = integerDF.select(max(\"id\")).first()[0]\n\nnormalizedIntegerDF = (integerDF\n  .withColumn(\"normalizedValue\", (col(\"id\") - colMin) / (colMax - colMin) )\n)\n\ndisplay(normalizedIntegerDF)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["-sandbox\n### Imputing Null or Missing Data\n\nNull values refer to unknown or missing data as well as irrelevant responses. Strategies for dealing with this scenerio include:<br><br>\n\n* **Dropping these records:** Works when you do not need to use the information for downstream workloads\n* **Adding a placeholder (e.g. `-1`):** Allows you to see missing data later on without violating a schema\n* **Basic imputing:** Allows you to have a \"best guess\" of what the data could have been, often by using the mean of non-missing data\n* **Advanced imputing:** Determines the \"best guess\" of what data should be using more advanced strategies such as clustering machine learning algorithms or oversampling techniques \n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=imputer#pyspark.ml.feature.Imputer\" target=\"_blank\">Also see the bult-in function `Imputer`</a>"],"metadata":{}},{"cell_type":"markdown","source":["Take a look at the following DataFrame, which has missing values."],"metadata":{}},{"cell_type":"code","source":["corruptDF = spark.createDataFrame([\n  (11, 66, 5),\n  (12, 68, None),\n  (1, None, 6),\n  (2, 72, 7)], \n  [\"hour\", \"temperature\", \"wind\"]\n)\n\ndisplay(corruptDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Drop any records that have null values."],"metadata":{}},{"cell_type":"code","source":["corruptDroppedDF = corruptDF.dropna(\"any\")\n\ndisplay(corruptDroppedDF)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Impute values with the mean."],"metadata":{}},{"cell_type":"code","source":["corruptImputedDF = corruptDF.na.fill({\"temperature\": 68, \"wind\": 6})\n\ndisplay(corruptImputedDF)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Deduplicating Data\n\nDuplicate data comes in many forms. The simple case involves records that are complete duplicates of another record. The more complex cases involve duplicates that are not complete matches, such as matches on one or two columns or \"fuzzy\" matches that account for formatting differences or other non-exact matches."],"metadata":{}},{"cell_type":"markdown","source":["Take a look at the following DataFrame that has duplicate values."],"metadata":{}},{"cell_type":"code","source":["duplicateDF = spark.createDataFrame([\n  (15342, \"Conor\", \"red\"),\n  (15342, \"conor\", \"red\"),\n  (12512, \"Dorothy\", \"blue\"),\n  (5234, \"Doug\", \"aqua\")], \n  [\"id\", \"name\", \"favorite_color\"]\n)\n\ndisplay(duplicateDF)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Drop duplicates on `id` and `favorite_color`."],"metadata":{}},{"cell_type":"code","source":["duplicateDedupedDF = duplicateDF.dropDuplicates([\"id\", \"favorite_color\"])\n\ndisplay(duplicateDedupedDF)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Other Helpful Data Manipulation Functions\n\n| Function    | Use                                                                                                                        |\n|:------------|:---------------------------------------------------------------------------------------------------------------------------|\n| `explode()` | Returns a new row for each element in the given array or map                                                               |\n| `pivot()`   | Pivots a column of the current DataFrame and perform the specified aggregation                                             |\n| `cube()`    | Create a multi-dimensional cube for the current DataFrame using the specified columns, so we can run aggregation on them   |\n| `rollup()`  | Create a multi-dimensional rollup for the current DataFrame using the specified columns, so we can run aggregation on them |"],"metadata":{}},{"cell_type":"markdown","source":["## Exercise 1: Deduplicating Data\n\nA common ETL workload involves cleaning duplicated records that don't completely match up.  The source of the problem can be anything from user-generated content to schema evolution and data corruption.  Here, you match records and reduce duplicate records."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Step 1: Import and Examine the Data\n\nThe file is sitting in `/mnt/training/dataframes/people-with-dups.txt`.\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** You have to deal with the header and delimiter."],"metadata":{}},{"cell_type":"code","source":["# TODO\ndupedDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = set(dupedDF.columns)\n\ndbTest(\"ET2-P-02-01-01\", 103000, dupedDF.count())\ndbTest(\"ET2-P-02-01-02\", True, \"salary\" in cols and \"lastName\" in cols)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["### Step 2: Add Columns to Filter Duplicates\n\nAdd columns following to allow you to filter duplicate values.  Add the following:\n\n- `lcFirstName`: first name lower case\n- `lcLastName`: last name lower case\n- `lcMiddleName`: middle name lower case\n- `ssnNums`: social security number without hyphens between numbers\n\nSave the results to `dupedWithColsDF`."],"metadata":{}},{"cell_type":"code","source":["# TODO\ndupedWithColsDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = set(dupedWithColsDF.columns)\n\ndbTest(\"ET2-P-02-02-01\", 103000, dupedWithColsDF.count())\ndbTest(\"ET2-P-02-02-02\", True, \"lcFirstName\" in cols and \"lcLastName\" in cols)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### Step 3: Deduplicate the Data\n\nDeduplicate the data by dropping duplicates of all records except for the original names (first, middle, and last) and the original `ssn`.  Save the result to `dedupedDF`.  Drop the columns you added in step 2."],"metadata":{}},{"cell_type":"code","source":["# TODO\ndedupedDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = set(dedupedDF.columns)\n\ndbTest(\"ET2-P-02-03-01\", 100000, dedupedDF.count())\ndbTest(\"ET2-P-02-03-02\", 7, len(cols))\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["## Review\n**Question:** What built-in functions are available in Spark?  \n**Answer:** Built-in functions include SQL functions, common programming language primitives, and data warehousing specific functions.  See the Spark API Docs for more details. (<a href=\"http://spark.apache.org/docs/latest/api/python/index.html\" target=\"_blank\">Python</a> or <a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.package\" target=\"_blank\">Scala</a>).\n\n**Question:** What's the best way to handle null values?  \n**Answer:** The answer depends largely on what you hope to do with your data moving forward. You can drop null values or impute them with a number of different techniques.  For instance, clustering your data to fill null values with the values of nearby neighbors often gives more insight to machine learning models than using a simple mean.\n\n**Question:** What are potential challenges of deduplicating data and imputing null values?  \n**Answer:** Challenges include knowing which is the correct record to keep and how to define logic that applies to the root cause of your situation. This decision making process depends largely on how removing or imputing data will affect downstream operations like database queries and machine learning workloads. Knowing the end application of the data helps determine the best strategy to use."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [User Defined Functions]($./03-User-Defined-Functions )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** How can I do ACID transactions with Spark?  \n**A:** ACID compliance refers to a set of properties of database transactions that guarantee the validity of you data.  <a href=\"https://databricks.com/product/databricks-delta\" target=\"_blank\">Databricks Delta</a> is an ACID compliant solution to transactionality with Spark workloads.\n\n**Q:** How can I handle more complex conditional logic in Spark?  \n**A:** You can handle more complex if/then conditional logic using the `when()` function and its `.otherwise()` method.\n\n**Q:** How can I handle data warehousing functions like rollups?  \n**A:** Spark allows for rollups and cubes, which are common in star schemas, using the `rollup()` and `cube()` functions."],"metadata":{}}],"metadata":{"name":"02-Common-Transformations","notebookId":291050440996488},"nbformat":4,"nbformat_minor":0}
