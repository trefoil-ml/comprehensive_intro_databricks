{"cells":[{"cell_type":"markdown","source":["#Advanced EDA with Azure Databricks"],"metadata":{}},{"cell_type":"markdown","source":["In order to run this notebook you should have previously run the <a href=\"$./03 Basic EDA with Azure Databricks\">Basic EDA with Azure Databricks</a> notebook to have everything prepared for this step."],"metadata":{}},{"cell_type":"markdown","source":["You are now done with exploring the dataset feature by feature, which is the main block in an EDA.   \nThe slightly more advanced section consist of four parts:\n\n* Creating a simple baseline model (the parsimonious model)\n* One hot encoding and feature scaling\n* Dimensionality reduction\n* Estimate feature importance by training a random forest regressor\n\nSince this is a lot of new material that we have not covered in depth yet we have done most of the coding for you. Your job is then to evaluate and understand the results."],"metadata":{}},{"cell_type":"markdown","source":["###Creating a simple baseline model (the parsimonious model)"],"metadata":{}},{"cell_type":"markdown","source":["Load the clean version of the data.\n\nBe sure to update the table name  \"usedcars\\_clean\\_#####\" with the unique name created while running the <a href=\"$./02.03 Basic EDA with Azure Databricks\">Basic EDA with Azure Databricks</a> notebook."],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\n\ndf = spark.sql(\"SELECT * FROM usedcars_clean_#####\")"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["In this section we will train a parsimonious model, a basic model to get a sense of the predictive capability of our data. \n\nWe are going to try and build a model that can answer the question \"Can I afford a car that is X months old and has Y kilometers on it, given I have $12,000 to spend?\"\n\nThe model will respond with a 1 (Yes) or no 0 (No). \n\nIn order to train a classifier, we need labels that go along with our used car features. The only features our model will be trained with are Age and KM. \n\nWe will engineer the label for Affordable. Our logic will be simple, if the car costs less than $12,000 (our stated budget), then we will label that row in our data with a 1, meaning Yes it is affordable. Otherwise we will label it with a 0.\n\nThe following cell will create a new Spark DataFrame that has our two desired features and the engineered label."],"metadata":{}},{"cell_type":"code","source":["df_affordability = df.selectExpr(\"Age\",\"KM\", \"CASE WHEN Price < 12000 THEN 1 ELSE 0 END as Affordable\")\ndisplay(df_affordability)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["While we could use matplotlib or ggplot to create a scatter plot of our data, the Azure Databricks notebook has a built in way for us to plot the data from the DataFrame without any material code, just by calling `display()` and passing it the DataFrame. \n\nWe've already configured the plot, so you just need to run the next cell. If you are curious as to the settings we used, select the Plot Options button that appears underneath the chart."],"metadata":{}},{"cell_type":"code","source":["display(df_affordability)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["**Challenge #1**\n\nGiven the above chart, at approximately what age does it look we start to afford a car irrespective of it's distance driven?"],"metadata":{}},{"cell_type":"markdown","source":["**Training the classifier**\n\nIn this particular case, we have chosen to train our classifier using the LogisticRegression module from SciKit Learn, since it's a good starting point for a model, especially when our data is not too large. \n\nThe LogisticRegression module does not understand Spark DataFrames natively. Given our small dataset, one option is to collect the data on to the driver node and then process represent using arrays. The following converts our Spark DataFrame into a Pandas DataFrame. Then the features (Age and KM) are stored in the X array and the labels (Affordability are stored in the y array)."],"metadata":{}},{"cell_type":"code","source":["X = df_affordability.select(\"Age\", \"KM\").toPandas().values\ny = df_affordability.select(\"Affordable\").toPandas().values"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Run the next two cells to get a quick look at the resulting arrays:"],"metadata":{}},{"cell_type":"code","source":["X"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["y"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Now one challenge we will face with the LogisticRegression is that it expects the inputs to be normalized. To make a long story short, if we were just to train the model using KM and Age without normalizing them to a smaller range around 0, then the model would give undue importance to the KM values because they are simply so much larger than the age (e.g., consider 80 months and 100,000 KM). \n\nTo normalize the values, we use the StandardScaler, again from SciKit-Learn."],"metadata":{}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["In the next line we look at the result of scaling. The first table of output shows the statistics for the original values. The second table shows the stats for the scaled values. Column 0 is Age and column 1 is KM."],"metadata":{}},{"cell_type":"code","source":["print(pd.DataFrame(X).describe().round(2))\nprint(pd.DataFrame(X_scaled).describe().round(2))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["**Challenge 2**\n\nAfter scaling, what is the range of values possible for the KM feature?"],"metadata":{}},{"cell_type":"markdown","source":["Next we will train the model."],"metadata":{}},{"cell_type":"code","source":["from sklearn import linear_model\n# Create a linear model for Logistic Regression\nclf = linear_model.LogisticRegression(C=1)\n\n# we create an instance of Neighbours Classifier and fit the data.\nclf.fit(X_scaled, y)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Now that we have a trained model, let's examine a feature of Azure Databricks notebooks that can help us play with the inputs to our model- widgets. \n\nWhen you run the following cell, two new text inputs will appear near the top of this notebook. When you edit their value move out of the input field, any cells that depend on that widget's value will be automatically re-run. \n\nFor now, run the following cell and observe the Age and Distance Driven widgets that appear. Notice they have been defaulted to Age of 40 months and Distance Driven of 40000 KM."],"metadata":{}},{"cell_type":"code","source":["dbutils.widgets.text(\"Age\",\"40\", \"Age (months)\")\ndbutils.widgets.text(\"Distance Driven\", \"40000\",\"Distance Driven (KM)\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Now run the following cell. It will take as input the values you specified in the widgets, scale the values and then use our classifier to predict the affordability."],"metadata":{}},{"cell_type":"code","source":["age = int(dbutils.widgets.get(\"Age\"))\nkm = int(dbutils.widgets.get(\"Distance Driven\"))\n\nscaled_input = scaler.transform([[age, km]])\n  \nprediction = clf.predict(scaled_input)\n\nprint(\"Can I afford a car that is {} month(s) old with {} KM's on it?\".format(age,km))\nprint(\"Yes (1)\" if prediction[0] == 1 else \"No (1)\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Experiment with changing the values for Age and Distance Driven by editing the values in the widgets. Notice that every time you edit a value and exit the input field, the above cell is re-executed (HINT: Look at the timestamp output that appears at the bottom of the above cell)."],"metadata":{}},{"cell_type":"markdown","source":["The above approach let's us experiment one prediction at a time. But what if we want to score a list of inputs at once? The following cell shows how we could score all of our original features to see what our model would predict."],"metadata":{}},{"cell_type":"code","source":["scaled_inputs = scaler.transform(X)\npredictions = clf.predict(scaled_inputs)\nprint(predictions)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["Now we can \"grade\" our model's performance using the accuracy measure. To do this we are effectively comparing what the model predicted versus what the label actually was for each row in our data. \n\nAn easy way to do this is by using the `accuracy_score` method from SciKit-Learn."],"metadata":{}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\nscore = accuracy_score(y, predictions)\nprint(\"Model Accuracy: {}\".format(score.round(3)))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["**Challenge #3**\n\nWhat grade would you give your model based on this score alone? Assume an A is 90% or better, a B is 80%-90% and so on."],"metadata":{}},{"cell_type":"markdown","source":["###One hot encoding and feature scaling"],"metadata":{}},{"cell_type":"markdown","source":["Until now we have not encoded the feature FuelType, but before we can use this feature as input to a model or a dimensionality reduction we need to apply one hot encoding. In Machine Learning literature, one hot encoding is defined as an approach to encode categorical integer features using a one-hot aka one-of-K scheme. In a nutshell, every distinct value of the categorical integer feature becomes a new column which has all zero values except for rows where that value is present, where it has a value of 1. This is a way to transform categorical values into a form that can be more efficiently used by Machine Learning algorithms.\n\nRunning the next cell will store an encoded version of the dataset in a new dataframe called `df_ohe`."],"metadata":{}},{"cell_type":"code","source":["df_ohe = df.toPandas().copy(deep=True)\ndf_ohe['FuelType'] = df_ohe['FuelType'].astype('category')\ndf_ohe = pd.get_dummies(df_ohe)\n\ndf_ohe.head(15)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["To be prepared for any model in the modelling phase, we also make a scaled dataset.    \nThe code below makes a new dataframe called `df_ohe_scaled`"],"metadata":{}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ncolumns_to_scale = ['Age', 'KM', 'HP', 'CC','Weight']\ndf_ohe_scaled = df_ohe.dropna().copy()\ndf_ohe_scaled[columns_to_scale] = scaler.fit_transform(df_ohe.dropna()[columns_to_scale])\n\ndf_ohe_scaled.head(15)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["###Dimensionality reduction\n\nDimensionality rediction is the operation that transforms data with n dimensions (in pandas world n columns in the dataframe) to a representation of the data in m dimensions. Obviously m is less than n, and for visualizations we set m to be 2 or 3. \n\nTo reduce the dimensionality of our dataset we use a method called Principal Component Analysis (PCA). With this method we can reduce the dimensionality in a way that preserves as much variance as possible. \n\nYou can play around with the selection of features to see which features affect the PCA.   \nYou can also try the PCA using the dataframe we didn't scale to see how scale affects the transformation. \n\nWhat makes PCA interesting in the context of an EDA is that we can use it to explore the relationship between higher dimensional data and a respons variable. \n\nBelow we send all features (not price) to the PCA to transform it to two dimensions. When we plot the two dimensional data and color it with price we get a graphical representation of the relationship between Price and all the features combined."],"metadata":{}},{"cell_type":"code","source":["from sklearn.decomposition import PCA \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots()\n\nfeatures = ['Age', 'KM', 'HP', 'Weight', 'CC', 'Doors',  'Automatic', 'MetColor', 'FuelType_cng', 'FuelType_diesel', 'FuelType_petrol']\n\nx_2d = PCA(n_components=2).fit_transform(df_ohe_scaled[features])\nsc = plt.scatter(x_2d[:,0], x_2d[:,1], c=df_ohe_scaled['Price'], s=10, alpha=0.7)\nplt.colorbar(sc) \n\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["###Estimate feature importance by training a random forest regressor"],"metadata":{}},{"cell_type":"markdown","source":["The model Random Forest has a very valuable side-product. After training the model it can provide a list over all features ranked by importance (we will bump into this concept again later in the workshop). By running the cell below you get one of these feature importance rankings. \n\n__Question:__ Does the output with feature importance match what you experienced when exploring the dataset?"],"metadata":{}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\nfig, ax = plt.subplots()\n\nfeatures_RFR = ['Age', 'KM', 'HP', 'Weight', 'CC', 'Doors', 'Automatic', 'MetColor', 'FuelType_cng', 'FuelType_diesel', 'FuelType_petrol']\n\n# Create train and test data\nX = df_ohe[features_RFR].as_matrix()\ny = df.toPandas()['Price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state =0)\n\n# Initialize  a random forest regressor\n# 'Train' the model\nRandomForestReg = RandomForestRegressor()\nRandomForestReg.fit(X_train, y_train)\n\nimp = pd.DataFrame(\n        RandomForestReg.feature_importances_ ,\n        columns = ['Importance'] ,\n        index = features_RFR\n    )\nimp = imp.sort_values( [ 'Importance' ] , ascending = True )\nimp['Importance'].plot(kind='barh')\n\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["This concludes the Exploratory Data Analysis lab.\n\nIn this lab you investigated a dataset with sale prices in $ for used (second-hand) Toyota Corollas.   \nDuring the lab you used a lot of the techniques we introduced in the presentation about EDA (Exploratory data analysis)."],"metadata":{}},{"cell_type":"markdown","source":["# Answers to Challenges\n\n1. Somewhere between 40 and 50 months in age.\n2. The scaled range for KM is -1.83 to 4.65. \n3. The percentage score is 92.6%, so this would get an A."],"metadata":{}}],"metadata":{"name":"04 Advanced EDA with Azure Databricks","notebookId":291050440996106},"nbformat":4,"nbformat_minor":0}
