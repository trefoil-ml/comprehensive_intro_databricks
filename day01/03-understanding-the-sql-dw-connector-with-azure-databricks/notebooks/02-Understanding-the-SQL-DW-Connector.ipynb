{"cells":[{"cell_type":"markdown","source":["# Understanding the SQL DW Connector"],"metadata":{}},{"cell_type":"markdown","source":["You can access Azure SQL Data Warehouse (SQL DW) from Azure Databricks using the SQL Data Warehouse connector (referred to as the SQL DW connector), a data source implementation for Apache Spark that uses Azure Blob storage, and PolyBase in SQL DW to transfer large volumes of data efficiently between a Databricks cluster and a SQL DW instance.\n\nBoth the Databricks cluster and the SQL DW instance access a common Blob storage container to exchange data between these two systems. In Databricks, Spark jobs are triggered by the SQL DW connector to read data from and write data to the Blob storage container. On the SQL DW side, data loading and unloading operations performed by PolyBase are triggered by the SQL DW connector through JDBC.\n\nThe SQL DW connector is more suited to ETL than to interactive queries, because each query execution can extract large amounts of data to Blob storage. If you plan to perform several queries against the same SQL DW table, we recommend that you save the extracted data in a format such as Parquet."],"metadata":{}},{"cell_type":"markdown","source":["## SQL Data Warehouse Pre-Requisites"],"metadata":{}},{"cell_type":"markdown","source":["There are two pre-requisites for connecting Azure Databricks with SQL Data Warehouse that apply to the SQL Data Warehouse:\n1. You need to [create a database master key](https://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/create-a-database-master-key) for the Azure SQL Data Warehouse. \n\n    **The key is encrypted using the password.**\n\n    USE [databricks-sqldw];  \n    GO  \n    CREATE MASTER KEY ENCRYPTION BY PASSWORD = '980AbctotheCloud427leet';  \n    GO\n\n2. You need to ensure that the [Firewall](https://docs.microsoft.com/en-us/azure/sql-database/sql-database-firewall-configure#manage-firewall-rules-using-the-azure-portal) on the Azure SQL Server that contains your SQL Data Warehouse is configured to allow Azure services to connect (e.g., Allow access to Azure services is set to On).\n\nYou should have already completed the above requirements in earlier steps of the lab."],"metadata":{}},{"cell_type":"markdown","source":["## Azure Storage Pre-Requisites"],"metadata":{}},{"cell_type":"markdown","source":["Azure Storage blobs are used as the intermediary for the exchange of data between Azure Databricks and Azure SQL Data Warehouse. As a result of this, you will need:\n1. To create a general purpose Azure Storage account v1\n2. Acquire the Account Name and Account Key for that Storage Account \n3. Create a container that will be used to store data used during the exchange, for example \"dwtemp\" (this must exists before you run an queries against SQL DW)\n\nYou should have already performed these steps in the setup steps for this lab. You will need to refer to the Account Name and Account Key you saved during setup for the section below."],"metadata":{}},{"cell_type":"markdown","source":["## Enabling access for a notebook session"],"metadata":{}},{"cell_type":"markdown","source":["You can enable access for the lifetime of your notebook session to SQL Data Warehouse by executing the cell below. Be sure to replace the **\"name-of-your-storage-account\"** and **\"your-storage-key\"** values with your own before executing."],"metadata":{}},{"cell_type":"code","source":["storage_account_name = \"name-of-your-storage-account\"\nstorage_account_key = \"your-storage-key\"\nstorage_container_name = \"dwtemp\"\n\ntemp_dir_url = \"wasbs://{}@{}.blob.core.windows.net/\".format(storage_container_name, storage_account_name)\n\nspark_config_key = \"fs.azure.account.key.{}.blob.core.windows.net\".format(storage_account_name)\nspark_config_value = storage_account_key\n\nspark.conf.set(spark_config_key, spark_config_value)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["You will need the JDBC connection string for your Azure SQL Data Warehouse. You should copy this value exactly as it appears in the Azure Portal. \n\nPlease replace the missing **`servername`**, **`databasename`**, and **`your-password`** values in the command below:"],"metadata":{}},{"cell_type":"code","source":["servername = \"servername\"\ndatabasename = \"databasename\"\npassword = \"your-password\"\n\nsql_dw_connection_string = \"jdbc:sqlserver://{}.database.windows.net:1433;database={};user=dwlab@{};password={};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\".format(servername, databasename, servername, password)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["print(sql_dw_connection_string)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## Accessing data from SQL Data Warehouse"],"metadata":{}},{"cell_type":"markdown","source":["You can load data into your Azure Databricks environment by issuing a SQL query against the SQL Data Warehouse, as follows:"],"metadata":{}},{"cell_type":"code","source":["query = \"SELECT * FROM EmployeeBasic\"\n\ndf = spark.read \\\n  .format(\"com.databricks.spark.sqldw\") \\\n  .option(\"url\", sql_dw_connection_string) \\\n  .option(\"tempdir\", temp_dir_url) \\\n  .option(\"forward_spark_azure_storage_credentials\", \"true\") \\\n  .option(\"query\", query) \\\n  .load()\n  \ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Once you have the data in a DataFrame, you can query it just as you would any other DataFrame. This includes visualizing it as follows:"],"metadata":{}},{"cell_type":"code","source":["summary = df.select(\"EmployeeName\")\ndisplay(summary)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Writing new data to SQL Data Warehouse"],"metadata":{}},{"cell_type":"markdown","source":["You can also go in the opposite direction, saving the contents of an existing DataFrame out to a new table in Azure SQL Data Warehouse.\n\nFirst, let's load the `weblogs` Databricks table contents into a new DataFrame. As you recall, we created this table in the previous notebook."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\npurchasedWeblogDF = spark.sql(\"select * from weblogs where Action == 'Purchased'\")"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Execute the cell below to add one additional column containing the transaction date/time. This column will be derived from the `TransactionDate` field by casting it to date type."],"metadata":{}},{"cell_type":"code","source":["purchasedProductsDF = purchasedWeblogDF.select('*',to_date(unix_timestamp(purchasedWeblogDF.TransactionDate,'MM/dd/yyyy HH:mm:ss').cast(\"timestamp\")).alias(\"TransactionDateTime\"))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Execute the cell below to filter out products purchased in month of May (05)."],"metadata":{}},{"cell_type":"code","source":["filteredProductsDF = purchasedProductsDF.where(month(purchasedProductsDF.TransactionDateTime)==5)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Execute the cell below to select the required columns."],"metadata":{}},{"cell_type":"code","source":["finalProductsDF = filteredProductsDF.select(\"SessionId\",\"UserId\",\"ProductId\",\"Quantity\",\"TransactionDateTime\")\ndisplay(finalProductsDF)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["Now that we have our desired DataFrame, let's create a new table in the Azure SQL Data Warehouse database and populate it with the contents of the DataFrame."],"metadata":{}},{"cell_type":"code","source":["new_table_name = \"PurchasedProducts\"\n\nfinalProductsDF.write \\\n  .format(\"com.databricks.spark.sqldw\") \\\n  .option(\"url\", sql_dw_connection_string) \\\n  .option(\"forward_spark_azure_storage_credentials\", \"true\") \\\n  .option(\"dbtable\", new_table_name) \\\n  .option(\"tempdir\", temp_dir_url) \\\n  .save()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["To verify that we have indeed created a new table in the SQL DW database, write a query to select the data from the new table:"],"metadata":{}},{"cell_type":"code","source":["query = \"SELECT * FROM PurchasedProducts\"\n\ndf = spark.read \\\n  .format(\"com.databricks.spark.sqldw\") \\\n  .option(\"url\", sql_dw_connection_string) \\\n  .option(\"tempdir\", temp_dir_url) \\\n  .option(\"forward_spark_azure_storage_credentials\", \"true\") \\\n  .option(\"query\", query) \\\n  .load()\n  \ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["You may also open Azure Data Studio and refresh the Tables list to see the new PurchasedProducts table.\n\nRun the following query in a new query window:\n\n```sql\nSELECT TOP (1000) [SessionId]\n      ,[UserId]\n      ,[ProductId]\n      ,[Quantity]\n      ,[TransactionDateTime]\n  FROM [dbo].[PurchasedProducts]\n```\n\nYou should have an output similar to the following:\n\n<img src=\"https://databricksdemostore.blob.core.windows.net/images/02-SQL-DW/sql-dw-new-table.png\" style=\"border: 1px solid #aaa; padding: 10px; border-radius: 10px 10px 10px 10px\"/>"],"metadata":{}}],"metadata":{"name":"02-Understanding-the-SQL-DW-Connector","notebookId":291050441001911},"nbformat":4,"nbformat_minor":0}
