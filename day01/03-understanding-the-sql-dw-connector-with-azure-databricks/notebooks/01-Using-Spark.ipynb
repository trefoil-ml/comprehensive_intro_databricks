{"cells":[{"cell_type":"markdown","source":["# Using Spark in Azure Databricks\n\nThis notebook will give you a quick overview on how to harness the power of the Apache Spark engine within Azure Databricks by executing cells in an interactive web document, called a notebook.\n\nSpark is used to do many things, including reading and writing huge files and data sets. It provides a query engine capable of processing data in very, very large data files.  Some of the largest Spark jobs in the world run on Petabytes of data.\n\nUp to this point in the lab, you have worked with reading and writing flat files using Azure SQL Data Warehouse (SQL DW) and PolyBase. An alternative to working with these flat files is to use the Apache Spark engine. The reasons you would want to do this include having the ability to more rapidly work with the files in an interactive way, combine both batch and stream processing using the same engine and code base, and include machine learning and deep learning as part of your big data process. Use SQL Data Warehouse as a key component of a big data solution. Import big data into SQL Data Warehouse with simple PolyBase T-SQL queries, and then use the power of massively parallel processing (MPP) to run high-performance analytics. As you integrate and analyze, the data warehouse will become the single version of truth your business can count on for insights."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Getting Started\n\nRun the following cell to configure your module.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Remember to attach your notebook to a cluster before running any cells in your notebook. In the notebook's toolbar, select the drop down arrow next to Detached, then select your cluster under Attach to.\n\n![Attached to cluster](https://databricksdemostore.blob.core.windows.net/images/03/03/databricks-cluster-attach.png)"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Step 3\n\nNow that you have opened this notebook, we can use it to run some code.\n1. In the cell below, we have a simple caluclation we would like to run: `1 + 1`.\n2. Run the cell by clicking the run icon and selecting **Run Cell**.\n<div><img src=\"https://files.training.databricks.com/images/eLearning/run-notebook-1.png\" style=\"width:600px; margin-bottom:1em; border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/></div>\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> You can also run a cell by typing **Ctrl-Enter**."],"metadata":{}},{"cell_type":"code","source":["1 + 1"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Data sources for this lab\n\nFor completion of this lab, it is mandatory to complete the environment setup found in the Microsoft learning module.\n\nThe PowerShell script executed during setup creates an Azure Storage account with two containers: `labdata` and `dwtemp`. The `dwtemp` container acts as a common Blob Storage container that both the Azure Databricks cluster and the SQL DW instance use to exchange data between the two systems.\n\nThe PowerShell script copied datasets into the `labdata` container that are required for this lab. Those files are contained within `/retaildata/rawdata/`.\n\nHere is a description of the datasets we will use:\n\n1.  **ProductFile**: Contains products data e.g. ProductId, ProductName,\n    Department, DepartmentId, Category, CategoryId and price for single\n    product.\n\n2.  **UserFile**: Contains user data e.g. UserId, FirstName, LastName,\n    Age, Gender, RegistrationDate.\n\n3.  **Weblog**: Contains weblog data which provides users activity log\n    on retail site e.g. SessionId, UserId, CustomerAction, ProductId,\n    TransactionTime etc. CustomerAction field determines if user has\n    purchased, browsed or added product in cart."],"metadata":{}},{"cell_type":"markdown","source":["### How to mount an Azure Blob to DBFS\n\nAzure Databricks has its own file system called Databricks File System, or DBFS. DBFS provides a common place to read and write files that are stored across one or more mount points. A mount point is how you attach file storage from one or more services, such as Azure Blob storage or Azure Data Lake Store. We will mount the Azure storage account as a new DBFS directory to make it easier to access the files contained within.\n\nOnce the blob is mounted as a DBFS directory, access it without exposing your Azure Blob Store keys."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n\n### Creating a Shared Access Signature (SAS) URL\nAzure provides you with a secure way to create and share access keys for your Azure Blob Store without compromising your account keys.\n\nMore details are provided <a href=\"http://docs.microsoft.com/azure/storage/common/storage-dotnet-shared-access-signature-part-1\" target=\"_blank\"> in this document</a>.\n\nThis allows access to your Azure Blob Store data directly from Databricks distributed file system (DBFS).\n\nAs shown in the screen shot, in the Azure Portal, go to the storage account containing the blob to be mounted. Then:\n\n1. Select Shared access signature from the menu.\n2. Click the Generate SAS button.\n3. Copy the entire Blob service SAS URL to the clipboard.\n4. Use the URL in the mount operation, as shown below.\n\n<img src=\"https://files.training.databricks.com/images/eLearning/DataFrames-MSFT/create-sas-keys.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; margin-top: 20px; padding: 10px\"/>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nCreate the mount point with `dbutils.fs.mount(source = .., mountPoint = .., extraConfigs = ..)`.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> If the directory is already mounted, you receive the following error:\n\n> Directory already mounted: /mnt/dw-training\n\nIn this case, use a different mount point such as `dw-training-2`, and ensure you update all three references below. Be sure to udate **SasURL**, and **StorageAccount**. The ContainerName value should be \"labdata\"."],"metadata":{}},{"cell_type":"code","source":["SasURL = \"https://azuredatabricksstore03.blob.core.windows.net/?sv=2018-03-28&ss=bfqt&srt=sco&sp=rwdlacup&se=2020-01-06T06:44:19Z&st=2019-01-05T22:44:19Z&spr=https&sig=FCoo3O4PjjoeF34yn%2FIMBcKyte%2BMTaKFcrdZiP8cdLE%3D\"\nindQuestionMark = SasURL.index('?')\nSasKey = SasURL[indQuestionMark:len(SasURL)]\nStorageAccount = \"azuredatabricksstore03\"\nContainerName = \"labdata\"\nMountPoint = \"/mnt/dw-training\"\n\ndbutils.fs.mount(\n  source = \"wasbs://%s@%s.blob.core.windows.net/\" % (ContainerName, StorageAccount),\n  mount_point = MountPoint,\n  extra_configs = {\"fs.azure.sas.%s.%s.blob.core.windows.net\" % (ContainerName, StorageAccount) : \"%s\" % SasKey}\n)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Take a look at the file contents of the **/retaildata/rawdata** subdirectory of the mount you just created:"],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/dw-training/retaildata/rawdata"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### Introducing DataFrames\n\nThis new mount point is now available to all Spark nodes of the Databricks cluster, allowing us to parallelize reads and writes as needed. We can now read the data into data structures called DataFrames.\n\nUnder the covers, DataFrames are derived from data structures known as Resilient Distributed Datasets (RDDs). RDDs and DataFrames are immutable distributed collections of data. Let's take a closer look at what some of these terms mean before we understand how they relate to DataFrames:\n\n* **Resilient**: They are fault tolerant, so if part of your operation fails, Spark  quickly recovers the lost computation.\n* **Distributed**: RDDs are distributed across networked machines known as a cluster.\n* **DataFrame**: A data structure where data is organized into named columns, like a table in a relational database, but with richer optimizations under the hood. \n\nWithout the named columns and declared types provided by a schema, Spark wouldn't know how to optimize the executation of any computation. Since DataFrames have a schema, they use the Catalyst Optimizer to determine the optimal way to execute your code.\n\nDataFrames were invented because the business community uses tables in a relational database, Pandas or R DataFrames, or Excel worksheets. A Spark DataFrame is conceptually equivalent to these, with richer optimizations under the hood and the benefit of being distributed across a cluster."],"metadata":{}},{"cell_type":"markdown","source":["#### Interacting with DataFrames\n\nOnce created (instantiated), a DataFrame object has methods attached to it. Methods are operations one can perform on DataFrames such as filtering,\ncounting, aggregating and many others.\n\n> <b>Example</b>: To create (instantiate) a DataFrame, use this syntax: `df = ...`\n\nTo display the contents of the DataFrame, apply a `show` operation (method) on it using the syntax `df.show()`. \n\nThe `.` indicates you are *applying a method on the object*.\n\nIn working with DataFrames, it is common to chain operations together, such as: `df.select().filter().orderBy()`.  \n\nBy chaining operations together, you don't need to save intermediate DataFrames into local variables (thereby avoiding the creation of extra objects).\n\nAlso note that you do not have to worry about how to order operations because the optimizier determines the optimal order of execution of the operations for you.\n\n`df.select(...).orderBy(...).filter(...)`\n\nversus\n\n`df.filter(...).select(...).orderBy(...)`"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n#### DataFrames and SQL\n\nDataFrame syntax is more flexible than SQL syntax. Here we illustrate general usage patterns of SQL and DataFrames.\n\nSuppose we have a data set we loaded as a table called `myTable` and an equivalent DataFrame, called `df`.\nWe have three fields/columns called `col_1` (numeric type), `col_2` (string type) and `col_3` (timestamp type)\nHere are basic SQL operations and their DataFrame equivalents. \n\nNotice that columns in DataFrames are referenced by `col(\"<columnName>\")`.\n\n| SQL                                         | DataFrame (Python)                    |\n| ------------------------------------------- | ------------------------------------- | \n| `SELECT col_1 FROM myTable`                 | `df.select(col(\"col_1\"))`             | \n| `DESCRIBE myTable`                          | `df.printSchema()`                    | \n| `SELECT * FROM myTable WHERE col_1 > 0`     | `df.filter(col(\"col_1\") > 0)`         | \n| `..GROUP BY col_2`                          | `..groupBy(col(\"col_2\"))`             | \n| `..ORDER BY col_2`                          | `..orderBy(col(\"col_2\"))`             | \n| `..WHERE year(col_3) > 1990`                | `..filter(year(col(\"col_3\")) > 1990)` | \n| `SELECT * FROM myTable LIMIT 10`            | `df.limit(10)`                        |\n| `display(myTable)` (text format)            | `df.show()`                           | \n| `display(myTable)` (html format)            | `display(df)`                         |\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** You can also run SQL queries with the special syntax `spark.sql(\"SELECT * FROM myTable\")`\n\nIn this course you see many other usages of DataFrames. It is left up to you to figure out the SQL equivalents \n(left as exercises in some cases)."],"metadata":{}},{"cell_type":"markdown","source":["## Reading data\n\nNow that you understand what DataFrames are, let's create a new DataFrame named `weblogData` by reading the weblogs. These files are in text format, and the data within is delimited by pipe (|) characters. We also know that each file contains a head row. We can let the Spark engine know this by using the `option` properties of the `read` function, as seen below."],"metadata":{}},{"cell_type":"code","source":["weblogData = (spark\n              .read\n              .option(\"header\",\"true\") # Allows us to extract the header\n              .option(\"delimiter\", '|') # Specifies the pipe character as the delimiter\n              .csv(\"mnt/dw-training/retaildata/rawdata/weblognew/[3-5]/{*}/weblog.txt\"))\nweblogData.show()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["In the above cell, we are trying to read data from the mounted path:\n\n`/retaildata/rawdata/weblognew/\\[3-5\\]/{\\*}/weblog.txt`\n\nWe can use wild card characters in a path string to read multiple files.\n\n**\\[3-5\\]** matches with all the sub directories inside the weblognew directory having names 3, 4 and 5.\n\n**\\*** matches with all the sub directories inside parent directory.\n\nThe above code returns a DataFrame and sets it to the variable `weblogData`.\n\n**DataFrame.take(** *n* **)** returns the first n number of elements from a DataFrame."],"metadata":{}},{"cell_type":"code","source":["display(weblogData.take(5))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Now, let's create two new DataFrames; one for users and one for products.\n\nThese files are a bit different than the weblog files. To start, they are comma-delimited, and they also contain no header. As such, we set the delimiter to comma, and we set an alias for each column for clarity."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nusers = (spark\n         .read\n         .option(\"inferSchema\", \"true\") # Automatically applies a schema by evaluating the data types\n         .option(\"delimiter\", ',')\n         .csv(\"mnt/dw-training/retaildata/rawdata/UserFile/part{*}\")\n         .select(col(\"_c0\").alias(\"UserId\"),\n                 col(\"_c3\").alias(\"FirstName\"),\n                 col(\"_c5\").alias(\"LastName\"),\n                 col(\"_c9\").alias(\"Gender\"),\n                 col(\"_c15\").alias(\"Age\"),\n                 col(\"_c18\").alias(\"RegisteredDate\")\n                )\n        )\nproducts = (spark\n            .read\n            .option(\"inferSchema\", \"true\")\n            .option(\"delimiter\", ',')\n            .csv(\"mnt/dw-training/retaildata/rawdata/ProductFile/part{*}\")\n            .select(col(\"_c0\").alias(\"ProductId\"),\n                    col(\"_c1\").alias(\"ProductName\"),\n                    col(\"_c2\").alias(\"BasePrice\"),\n                    col(\"_c3\").alias(\"CategoryId\"),\n                    col(\"_c7\").alias(\"Category\"),\n                    col(\"_c8\").alias(\"Department\")\n                   )\n           )"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Let's take a look at the user data."],"metadata":{}},{"cell_type":"code","source":["display(users)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Now, view the product data."],"metadata":{}},{"cell_type":"code","source":["display(products)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### Perform DataFrame operations\n\nWe will now explore various operations that can be performed on DataFrames, using the three we have created so far."],"metadata":{}},{"cell_type":"markdown","source":["#### Select\n\n**select(\\*cols)**\n\nProjects a set of expressions and returns a new DataFrame.\n\n**Parameters**:\n\ncols -- list of column names (string) or expressions (Column). If\none of the column names is '\\*', that column is expanded to include\nall columns in the current DataFrame.\n\nTry this out in the cell below:"],"metadata":{}},{"cell_type":"code","source":["products.select(\"ProductId\",\"ProductName\").show(5)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["#### OrderBy\n\n**orderBy**(\\*cols, \\*\\*kwargs)\n\nReturns a new DataFrame sorted by the specified column(s).\n\n**Parameters**:\n\ncols -- list of Column or column names to sort by.\n\nascending -- boolean or list of boolean (default True). Sort\nascending vs. descending. Specify list for multiple sort orders. If\na list is specified, length of the list must equal length of the\ncols.\n\nExecute the following 3 cells to try a few of different methods:"],"metadata":{}},{"cell_type":"code","source":["products.orderBy(\"ProductId\",ascending=False).show(5)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["products.orderBy([\"ProductName\",\"BasePrice\"],ascending=[0, 1]).show(5)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["products.orderBy(products.ProductId.desc()).show(5)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["#### Count\n\n**count()**\n\nReturns the number of rows in this DataFrame.\n\nExecute the following cell to see the count of product records:"],"metadata":{}},{"cell_type":"code","source":["products.count()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["#### Agg\n\n**agg(\\*exprs)**\n\nAggregate on the entire DataFrame without groups.\n\nExecute the following two cells for a couple examples of the `agg` operation:"],"metadata":{}},{"cell_type":"code","source":["products.agg({\"BasePrice\" : \"max\"}).show(1)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\nproducts.agg(F.min(products.BasePrice)).show(1)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["#### Where\n\n**where(**condition**)**\n\nFilters rows using the given condition.\n\n`where()` is an alias for `filter()`.\n\nExecute the following cell:"],"metadata":{}},{"cell_type":"code","source":["products.where(products.BasePrice > 500).show(5)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["#### GroupBy\n\n**groupBy**(\\*cols)\n\nGroups the DataFrame using the specified columns, so we can run\naggregation on them. See GroupedData for all the available aggregate\nfunctions.\n\n**Parameters**:\n\ncols -- list of columns to group by. Each element should be a column\nname (string) or an expression (Column).\n\nExecute the following three cells for a few examples of the `groupBy` operation:"],"metadata":{}},{"cell_type":"code","source":["# Without column name\n\nproducts.groupBy().min().show()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# With column name\n\nproducts.groupBy(\"Department\").count().show(5)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# Get minimum BasePrice in each department\n\nproducts.groupBy(products.Department).min(\"BasePrice\").show(5)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["### Joins\n\nBefore exploring joins, we need to create a weblog DataFrame containing a small sample of records.\n\nExecute the following to retrieve 10 records from the weblog DataFrame where the customer action is Purchased:"],"metadata":{}},{"cell_type":"code","source":["weblogSampleData = weblogData.where(weblogData.Action == \"Purchased\").take(10)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["The above code snippet returns a list of Row objects. We have to convert that list object into an RDD.\n\nExecute the following cell to convert the list into an RDD:"],"metadata":{}},{"cell_type":"code","source":["weblogSampleRDD = sc.parallelize(weblogSampleData)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["Now execute below cell to convert the RDD into a DataFrame:"],"metadata":{}},{"cell_type":"code","source":["weblogSampleDF = weblogSampleRDD.toDF()"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["#### Join\n\n**join**(other, joinExprs=None, joinType=None)\n\nJoins with another DataFrame, using the given join expression.\n\n**Parameters**:\n\nother -- Right side of the join\n\njoinExprs -- a string for join column name, or a join expression\n(Column). If joinExprs is a string indicating the name of the join\ncolumn, the column must exist on both sides, and this performs an\ninner equi-join.\n\njoinType -- str, default 'inner'. One of inner, outer, left\\_outer,\nright\\_outer, semijoin.\n\nExecute the following cell to join the sample weblog DataFrame with the products DataFrame based on the Product ID key to get the BasePrice for each product purchased in the weblog DataFrame:"],"metadata":{}},{"cell_type":"code","source":["weblogSampleDF.join(products,weblogSampleDF.ProductId == products.ProductId,'inner').select(weblogSampleDF.SessionId,weblogSampleDF.ProductId,weblogSampleDF.Quantity,products.BasePrice).show(10)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["## Learn Spark SQL \n\nThe **sql** function of **SparkSession** object enables applications to\nrun SQL queries programmatically and returns the result as a DataFrame\nobject. However, to run SQL queries we need to create a table."],"metadata":{}},{"cell_type":"markdown","source":["### Databricks databases and tables\n\nAn Azure Databricks database is a collection of tables. An Azure Databricks table is a collection of structured data. Tables are equivalent to Apache Spark DataFrames. This means that you can cache, filter, and perform any operations supported by DataFrames on tables. You can query tables with Spark APIs and Spark SQL.\n\nThere are two types of tables: global and local. A global table is available across all clusters. Azure Databricks registers global tables to the Hive metastore. A local table is not accessible from other clusters and is not registered in the Hive metastore. This is also known as a temporary table or a view.\n\nA temporary view gives you a name to query from SQL, but unlike a table it exists only for the duration of your Spark Session. As a result, the temporary view will not carry over when you restart the cluster or switch to a new notebook. It also won't show up in the Data button on the menu on the left side of a Databricks notebook which provides easy access to databases and tables.\n\nYou create a temporary view with the `createOrReplaceTempView` operation on a DataFrame.\n\nFor example, to create a temporary view of the products DataFrame, you would execute the following:\n\n`products.createOrReplaceTempView(\"Products\")`\n\nFor our purposes, we will be creating a **global table**. This is because you will be executing a different notebook after you are done with this one, and you will want to have access to the data structures you define here when writing the data back to your Azure SQL Data Warehouse instance. This as opposed to creating the DataFrames all over again.\n\n> Note: To avoid potential consistency issues, the best approach to replacing table contents is to overwrite the table. Just in case the \"products\" table already exists, we will overwrite its contents with our DataFrame.\n\nExecute the cell below to create a new global \"products\" table from the `products` DataFrame:"],"metadata":{}},{"cell_type":"code","source":["products.write.mode(\"OVERWRITE\").saveAsTable(\"products\")"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["Now, execute the following cell to create a new global \"weblogs\" table, overwriting any existing tables:"],"metadata":{}},{"cell_type":"code","source":["weblogSampleDF.write.mode(\"OVERWRITE\").saveAsTable(\"weblogs\")"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["Finally, execute the following cell to create a new global \"users\" table:"],"metadata":{}},{"cell_type":"code","source":["users.write.mode(\"OVERWRITE\").saveAsTable(\"users\")"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["### Run SQL queries\n\nExecute the following cells to learn how to run Spark SQL queries.\n\nNotice that each cell starts with the **%sql** magic. This tells the Spark engine to execute the cell using the SQL language, as opposed to the default python language."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nselect * from products\nlimit 10"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["%sql\n\nselect ProductName from products where ProductId = 30"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["%sql\n\nselect Department, count(ProductId) as ProductCount\nfrom products group by Department order by ProductCount desc"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["%sql\n\nselect w.SessionId, w.ProductId, p.ProductName, p.BasePrice, w.Quantity, (p.BasePrice * w.Quantity) as Total\nfrom weblogs w Join products p on w.ProductId == p.ProductId"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["-sandbox\n## Create visualizations\n\nAzure Databricks provides easy-to-use, built-in visualizations for your data. \n\nDisplay the data by invoking the Spark `display` function.\n\nVisualize the query below by selecting the down arrow button next to the bar graph icon once the table is displayed. Select **Area** to select the Area chart visualization:\n\n<img src=\"https://databricksdemostore.blob.core.windows.net/images/02-SQL-DW/databricks-display-graph-button.png\" style=\"border: 1px solid #aaa; padding: 10px; border-radius: 10px 10px 10px 10px\"/>"],"metadata":{}},{"cell_type":"markdown","source":["Configure the area chart settings by clicking the **Plot Options...** button below the chart. A few controls will appear on the screen. Set the values for these controls as shown below:\n\n* Select **RegistrationDate** for **Keys**.\n* Select **count(UserId)** for **Values**.\n* Select **MAX** for **Aggregation**.\n\n<img src=\"https://databricksdemostore.blob.core.windows.net/images/02-SQL-DW/databricks-display-graph-options.png\" style=\"border: 1px solid #aaa; padding: 10px; border-radius: 10px 10px 10px 10px\"/>\n\nYou should see an area chart that looks like the following:\n\n<img src=\"https://databricksdemostore.blob.core.windows.net/images/02-SQL-DW/databricks-display-graph.png\" style=\"border: 1px solid #aaa; padding: 10px; border-radius: 10px 10px 10px 10px\"/>"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT COUNT(UserId), Year(RegisteredDate) as RegistrationDate FROM users\nGROUP BY Year(RegisteredDate) ORDER BY Year(RegisteredDate)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["Next, create a bar chart of the number of registrations by age."],"metadata":{}},{"cell_type":"markdown","source":["Execute the cell below, then select the **Bar** chart this time.\n\nConfigure the bar chart settings by clicking the **Plot Options...** button below the chart. Set the values for these controls as shown below:\n\n* Select **Age** for **Keys**.\n* Select **count(UserId)** for **Values**.\n* Select **MAX** for **Aggregation**.\n\n<img src=\"https://databricksdemostore.blob.core.windows.net/images/02-SQL-DW/databricks-display-chart-options.png\" style=\"border: 1px solid #aaa; padding: 10px; border-radius: 10px 10px 10px 10px\"/>\n\nYou should see a bar chart that looks like the following:\n\n<img src=\"https://databricksdemostore.blob.core.windows.net/images/02-SQL-DW/databricks-display-chart.png\" style=\"border: 1px solid #aaa; padding: 10px; border-radius: 10px 10px 10px 10px\"/>"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT COUNT(UserId), Age FROM Users GROUP BY Age ORDER BY Age"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["## Next Steps\n\nBy this point, you should have a good grasp on how to process data using Spark on Azure Databricks. The next lesson will build upon these concepts by connecting to your Azure SQL Data Warehouse instance and writing this data to a new external table.\n\nStart the next lesson, [Understanding the SQL DW Connector]($./02-Understanding-the-SQL-DW-Connector )."],"metadata":{}}],"metadata":{"name":"01-Using-Spark","notebookId":291050441001841},"nbformat":4,"nbformat_minor":0}
