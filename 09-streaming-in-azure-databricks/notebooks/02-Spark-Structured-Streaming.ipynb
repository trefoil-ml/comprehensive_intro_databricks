{"cells":[{"cell_type":"markdown","source":["# Spark Structured Streaming\n\nIn this lesson, you will use Spark Structured Streaming to perform near-realtime processing of message."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Getting Started\n\nRun the following cell to configure your module.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Remember to attach your notebook to a cluster before running any cells in your notebook. In the notebook's toolbar, select the drop down arrow next to Detached, then select your cluster under Attach to.\n\n![Attached to cluster](https://databricksdemostore.blob.core.windows.net/images/03/03/databricks-cluster-attach.png)"],"metadata":{}},{"cell_type":"code","source":["%run \"./includes/Module-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Prepare sample data\n\nWe are going to use a data generator function to create a sampling of flight delay data to build a source for streaming. The following command will run for three (3) minutes to generate a small sample of data you can use for this module."],"metadata":{}},{"cell_type":"code","source":["%scala\nDummyDataGenerator.start(3)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["The sample flight delay data is written to a folder under your user directory. To view the files created there, run the following cell."],"metadata":{}},{"cell_type":"code","source":["flightsPath = \"dbfs:/tmp/%(username)s/new-flights/\" %locals()\n\ndbutils.fs.ls(flightsPath)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Batch / Interactive processing\n\nNow that we have some source data in place, let's explore the data, first using interactive processing, then using Spark Structured Streaming.\n\nThe usual first step in attempting to process data is to interactively query the data. \n\nTo accomplish this, import the required libraries into your environment, define a scheme for the data being imported, and then create a static DataFrame."],"metadata":{}},{"cell_type":"code","source":["import json\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DecimalType, StringType, TimestampType\nfrom pyspark.sql.functions import *\n\n# Create a schema for reading the data.\nschema = StructType([\n        StructField('FlightNumber',IntegerType()), \n        StructField('DepartureTime', TimestampType()), \n        StructField('Delay', DecimalType(10, 1), True),\n        StructField('Reason', StringType()),\n        StructField('Airline', StringType())])\n\nstaticFlights = spark.read.schema(schema).csv(flightsPath)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Quickly look at the data contained in the DataFrame, so you have a sense of the structure and data contained within it."],"metadata":{}},{"cell_type":"code","source":["display(staticFlights)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["The flight delay data provides information about flights, departure delays, and the reason for those delays, along with a timestamp of when the flight departure occurred.\n\nNow, let's look at the reasons for flight delays, grouped within one hour windows. To do this, we will group by the `Airline`, and `Reason` columns and one hour windows over the `DepartureTime` column."],"metadata":{}},{"cell_type":"code","source":["staticDelays = (\n  staticFlights\n    .groupBy(\n      staticFlights.Airline,\n      staticFlights.Reason,\n      window(staticFlights.DepartureTime, \"1 hour\")\n    )\n    .count()\n)\nstaticDelays.cache()\n\n# Register the DataFrame as local table 'static_delays'\nstaticDelays.createOrReplaceTempView(\"static_delays\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["display(staticDelays)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["You can now use Databricks visualizations to view the delay reasons by hour graphically. For this, you will use the `%sql` magic command to execute a SQL query against the sample data.\n\nAfter running the cell below, select the chart icon, select bar chart, and select plot options. Configure the chart as show in the following image.\n\n ![Delays by hour](https://databricksdemostore.blob.core.windows.net/images/08/02/static-actions-windowed-bar-chart.png 'Delays by hour')"],"metadata":{}},{"cell_type":"code","source":["%sql SELECT Reason, Airline, date_format(window.end, \"MMM-dd HH:mm\") AS DepartureDate, Count FROM static_delays ORDER BY DepartureDate"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## Stream Processing\n\nNow that you have analyzed the data interactively, let's convert this to a streaming query that continuously updates as data arrives. Since we just have a static set of data, we are going to emulate a stream from the files by reading them one at a time, in the chronological order they were created.\n\nThe query we have to write is similar to the interactive query above, except we replace `read` with `readStream`."],"metadata":{}},{"cell_type":"code","source":["streamingFlights = (spark\n              .readStream\n              .schema(schema)\n              .option(\"maxFilesPerTrigger\", 1) #Treat a sequence of files as a stream by selecting one file at a time\n              .csv(flightsPath)\n            )\n\nstreamingDelays = (\n  streamingFlights\n    .groupBy(\n      streamingFlights.Airline,\n      streamingFlights.Reason,\n      window(streamingFlights.DepartureTime, \"1 hour\")\n    )\n    .count()\n)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Now, let's take a quick look at the isStreaming property. This is a way to verify whether or not a DataFrame is streaming. For our streamingWeblogs DataFrame, you should see `True` returned."],"metadata":{}},{"cell_type":"code","source":["streamingDelays.isStreaming"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["For comparision, run the same command against the weblogs DataFrame you created above to load the weblogs data from CSV files in Azure storage."],"metadata":{}},{"cell_type":"code","source":["staticDelays.isStreaming"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Create an in-memory query for accessing the streaming data\nIn this scenario, you will store the input data as an in-memory table. From here, you can query the dataset using SQL. The name of table is specified from the queryName option.\n\n> NOTE: In-memory output should only be used on small datasets, as the entire output is collected and stored in memory. In a production environment, you would want to write the query to a file or other sink using code like the following:\n  ```scala\n  writeStream\n      .format(\"parquet\")        // can be \"orc\", \"json\", \"csv\", etc.\n      .option(\"path\", \"path/to/destination/dir\")\n      .start()\n  ```"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n\nquery = (\n  streamingDelays\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only)\n    .queryName(\"delays\")     # delays = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["`delays` is the handle to the streaming query running in the background. This query is continously picking up files and updating the windowed actions.\n\nNote the status of `delays` in the above cell. By expanding the `delays` query above, you can view the stream and graphs associated with the ingestion of files by the streaming DataFrame.\n\n ![Query Dashboard](https://databricksdemostore.blob.core.windows.net/images/08/02/streaming-actions-dashboard.png 'Query Dashboard')"],"metadata":{}},{"cell_type":"markdown","source":["With the in-memory query, you can now access the data via Spark SQL, using the queryName, `delays`, as the table name. Let's view the windowed actions graphically, as you did previously."],"metadata":{}},{"cell_type":"code","source":["%sql SELECT Reason, Airline, date_format(window.end, \"MMM-dd HH:mm\") AS DepartureDate, Count FROM delays ORDER BY DepartureDate"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Re-run the above cell a few times, waiting 15 or 20 seconds between runs, and note that data is continually added to the DataFrame, and that the date range on the bottom continously shifts to the right as later days are streamed in."],"metadata":{}},{"cell_type":"markdown","source":["## Clean up\n\nStop the streaming query."],"metadata":{}},{"cell_type":"code","source":["query.stop()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["## Next Steps\n\nNow that you have a better understanding of Spark Structured Streaming, you can move on to the next lesson which combines Structure Streaming with Event Hubs for streaming data.\n\n\nStart the next lesson, [Streaming with Event Hubs]($./03-Event-Hubs)"],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Spark","language":"","name":"sparkkernel"},"language_info":{"codemirror_mode":"text/x-scala","mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala"},"name":"02-Spark-Structured-Streaming","notebookId":291050441001787},"nbformat":4,"nbformat_minor":0}
