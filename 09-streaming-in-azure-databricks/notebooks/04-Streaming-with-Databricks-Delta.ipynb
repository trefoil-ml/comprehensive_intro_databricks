{"cells":[{"cell_type":"markdown","source":["# Databricks Delta Streaming\nDatabricks&reg; Delta allows you to work with streaming data.\n\nIn this lesson you, you will use Databricks Delta to read and write streaming data into a data lake.\n\n### CAUTION\n* Do not use <b>RunAll</b> mode (next to <b>Permissions</b>, in the menu at the top)."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Getting Started\n\nRun the following cell to configure your module.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Remember to attach your notebook to a cluster before running any cells in your notebook. In the notebook's toolbar, select the drop down arrow next to Detached, then select your cluster under Attach to.\n\n![Attached to cluster](https://databricksdemostore.blob.core.windows.net/images/03/03/databricks-cluster-attach.png)"],"metadata":{}},{"cell_type":"code","source":["%run \"./includes/Module-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Set up relevant paths."],"metadata":{}},{"cell_type":"code","source":["dataPath = \"/mnt/training/definitive-guide/data/activity-data\"\noutputPath = userhome + \"/gaming\"\nbasePath = userhome + \"/advanced-streaming\"\ncheckpointPath = basePath + \"/checkpoints\"\nactivityPath = basePath + \"/activityCount\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["-sandbox\n##Streaming Concepts\n\n<b>Stream processing</b> is where you continuously incorporate new data into a data lake and compute results.\n\nThe data is coming in faster than it can be consumed.\n\nTreat a <b>stream</b> of data as a table to which data is continously appended. \n\nIn this course we are assuming Databricks Structured Streaming, which uses the DataFrame API. \n\nThere are other kinds of streaming systems.\n\n<div><img src=\"https://s3-us-west-2.amazonaws.com/curriculum-release/images/eLearning/Delta/stream2rows.png\" style=\"height: 300px\"/></div><br/>\n\nExamples are bank card transactions, Internet of Things (IoT) device data, and video game play events. \n\nData coming from a stream is typically not ordered in any way.\n\nA streaming system consists of \n* <b>Input source</b> such as Kafka, Azure Event Hub, files on a distributed system or TCP-IP sockets\n* <b>Sinks</b> such as Kafka, Azure Event Hub, various file formats, `forEach` sinks, console sinks or memory sinks\n\n### Streaming and Databricks Delta\n\nIn streaming, the problems of traditional data pipelines are exacerbated. \n\nSpecifically, with frequent meta data refreshes, table repairs and accumulation of small files on a secondly- or minutely-basis!\n\nMany small files result because data (may be) streamed in at low volumes with short triggers.\n\nDatabricks Delta is uniquely designed to address these needs."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### READ Stream using Databricks Delta\n\nThe `readStream` method is a <b>transformation</b> that outputs a DataFrame with specific schema specified by `.schema()`. \n\nEach line of the streaming data becomes a row in the DataFrame once an <b>action</b> such as `writeStream` is invoked.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> In this lesson, we limit flow of stream to one file per trigger with `option(\"maxFilesPerTrigger\", 1)` so that you do not exceed file quotas you may have on your end. The default value is 1000.\n\nNotice that nothing happens until you engage an action, i.e. a `writeStream` operation a few cells down.\n\nDo some data normalization as well:\n* Convert `Arrival_Time` to `timestamp` format.\n* Rename `Index` to `User_ID`."],"metadata":{}},{"cell_type":"code","source":["static = spark.read.json(dataPath)\ndataSchema = static.schema\n\ndeltaStreamWithTimestampDF = (spark\n  .readStream\n  .format(\"delta\")\n  .option(\"maxFilesPerTrigger\", 1)\n  .schema(dataSchema)\n  .json(dataPath)\n  .withColumnRenamed('Index', 'User_ID')\n  .selectExpr(\"*\",\"cast(cast(Arrival_Time as double)/1000000000 as timestamp) as event_time\")\n)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["-sandbox\n### WRITE Stream using Databricks Delta\n\n#### General Notation\nUse this format to write a streaming job to a Databricks Delta table.\n\n> `(myDF` <br>\n  `.writeStream` <br>\n  `.format(\"delta\")` <br>\n  `.option(\"checkpointLocation\", checkpointPath)` <br>\n  `.outputMode(\"append\")` <br>\n  `.table(\"my_table\")` or `.start(path)` <br>\n`)`\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> If you use the `.table()` notation, it will write output to a default location. \n\nIn this course, we want everyone to write data to their own directory; so, instead, we use the `.start()` notation.\n\n#### Output Modes\nNotice, besides the \"obvious\" parameters, specify `outputMode`, which can take on these values\n* `append`: add only new records to output sink\n* `complete`: rewrite full output - applicable to aggregations operations\n* `update`: update changed records in place\n\n#### Checkpointing\n\nWhen defining a Delta streaming query, one of the options that you need to specify is the location of a checkpoint directory.\n\n`.writeStream.format(\"delta\").option(\"checkpointLocation\", <path-to-checkpoint-directory>) ...`\n\nThis is actually a structured streaming feature. It stores the current state of your streaming job.\n\nShould your streaming job stop for some reason and you restart it, it will continue from where it left off.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> If you do not have a checkpoint directory, when the streaming job stops, you lose all state around your streaming job and upon restart, you start from scratch.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Also note that every streaming job should have its own checkpoint directory: no sharing."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Let's Do Some Streaming\n\nIn the cell below, we write streaming query to a Databricks Delta table. \n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Notice how we do not need to specify a schema: it is inferred from the data!"],"metadata":{}},{"cell_type":"code","source":["deltaStreamingQuery = (deltaStreamWithTimestampDF\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", checkpointPath)\n  .outputMode(\"append\")\n  .start(basePath)\n)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["See list of active streams."],"metadata":{}},{"cell_type":"code","source":["spark.streams.active"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["-sandbox\n### Table-to-Table Stream\n\nHere we read a stream of data from from `basePath` and write another stream to `activityPath`.\n\nThe data consists of a grouped count of `gt` events.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Make sure the stream using `deltaStreamingQuery` is still running!\n\nTo perform an aggregate operation, what kind of `outputMode` should you use?"],"metadata":{}},{"cell_type":"code","source":["activityCountsQuery = (spark.readStream\n  .format(\"delta\")\n  .load(str(basePath))   \n  .groupBy(\"gt\")\n  .count()\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", checkpointPath + \"/activityCount\")\n  .outputMode(\"complete\")\n  .start(activityPath)\n)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Once the steam has started, run the cell below to test your solution."],"metadata":{}},{"cell_type":"code","source":["# Run this cell to test that your stream has started.\nactivityQueryTruth = spark.streams.get(activityCountsQuery.id).isActive\n\ndbTest(\"Delta-05-activityCountsQuery\", True, activityQueryTruth)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["-sandbox\nPlot the occurrence of all events grouped by `gt`.\n\nIn order to create a LIVE bar chart of the data, you'll need to fill out the <b>Plot Options</b> as shown:\n\n<div><img src=\"https://s3-us-west-2.amazonaws.com/files.training.databricks.com/images/eLearning/Delta/ch5-plot-options.png\"/></div><br/>\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> In the cell below, we use the `withWatermark` and `window` methods, which aren't covered in this course. \n\nTo learn more about watermarking, please see <a href=\"https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\" target=\"_blank\">Event-time Aggregation and Watermarking in Apache Sparkâ€™s Structured Streaming</a>."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import hour, window, col\n\ncountsDF = (deltaStreamWithTimestampDF      \n  .withWatermark(\"event_time\", \"180 minutes\")\n  .groupBy(window(\"event_time\", \"60 minute\"),\"gt\")\n  .count()\n  .withColumn('hour',hour(col('window.start')))     \n)\n\ndisplay(countsDF.withColumn('hour',hour(col('window.start'))))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# compare schemas of countsDF\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, TimestampType\n\nexpectedSchema = StructType([\n   StructField(\"gt\",StringType(), True),\n   StructField(\"count\",LongType(), False),\n   StructField(\"hour\",IntegerType(), True),\n   StructField(\"window\", StructType([\n       StructField(\"start\",TimestampType(), True),\n       StructField(\"end\",TimestampType(), True)\n   ]), False)\n])\n\ndbTest(\"Delta-05-schemas\", set(expectedSchema), set(countsDF.schema))\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Shut Down\n\nStop all the running streams."],"metadata":{}},{"cell_type":"code","source":["for streamingQuery in spark.streams.active:\n  streamingQuery.stop()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# TEST - Run this cell to test that all the streams where stopped.\nnumActiveStreams = len(spark.streams.active)\ndbTest(\"Delta-05-numActiveStreams\", 0, numActiveStreams)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## Summary\n\nDatabricks Delta is ideally suited for use in streaming data lake contexts."],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n**Q:** Why is Databricks Delta so important for a data lake that incorporates streaming data?<br>\n**A:** Frequent meta data refreshes, table repairs and accumulation of small files on a secondly- or minutely-basis!\n\n**Q:** What happens if you shut off your stream before it has fully initialized and started and you try to `CREATE TABLE .. USING DELTA` ? <br>\n**A:** You will get this: `Error in SQL statement: AnalysisException: The user specified schema is empty;`.\n\n**Q:** When you do a write stream command, what does this option do `outputMode(\"append\")` ?<br>\n**A:** This option takes on the following values and their respective meanings:\n* <b>append</b>: add only new records to output sink\n* <b>complete</b>: rewrite full output - applicable to aggregations operations\n* <b>update</b>: update changed records in place\n\n**Q:** What happens if you do not specify `option(\"checkpointLocation\", pointer-to-checkpoint directory)`?<br>\n**A:** When the streaming job stops, you lose all state around your streaming job and upon restart, you start from scratch.\n\n**Q:** How do you view the list of active streams?<br>\n**A:** Invoke `spark.streams.active`.\n\n**Q:** How do you verify whether `streamingQuery` is running (boolean output)?<br>\n**A:** Invoke `spark.streams.get(streamingQuery.id).isActive`."],"metadata":{}},{"cell_type":"markdown","source":["### Congratulations! \n\nYou have completed the Streaming module.\n\n#### Additional Topics & Resources\n\n* <a href=\"https://docs.azuredatabricks.net/delta/delta-streaming.html#as-a-sink\" target=\"_blank\">Delta Streaming Write Notation</a>\n* <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#\" target=\"_blank\">Structured Streaming Programming Guide</a>\n* <a href=\"https://www.youtube.com/watch?v=rl8dIzTpxrI\" target=\"_blank\">A Deep Dive into Structured Streaming</a> by Tagatha Das. This is an excellent video describing how Structured Streaming works."],"metadata":{}}],"metadata":{"name":"04-Streaming-with-Databricks-Delta","notebookId":291050441001738},"nbformat":4,"nbformat_minor":0}
