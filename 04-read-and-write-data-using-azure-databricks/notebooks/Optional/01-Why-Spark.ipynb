{"cells":[{"cell_type":"markdown","source":["# Why Apache Spark?\n\nIdentify the problems Apache Spark&trade; and Databricks&reg; are well suited to solve.\n\n## In this lesson you\n* Identify the types of tasks well suited to Apache Sparkâ€™s Unified Analytics Engine.\n* Identify examples of tasks not well suited for Apache Spark.\n\n## Audience\n* Primary Audience: Data Analysts\n* Additional Audiences: Data Engineers and Data Scientists\n\n## Prerequisites\n* Web browser: Chrome or Firefox\n* Lesson: [Getting Started]($./01-Getting-Started)\n* Concept: <a href=\"https://www.w3schools.com/sql\" target=\"_blank\">Basic SQL</a>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Getting Started\n\nRun the following cell to configure our \"classroom.\"\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> The command `%run` runs another notebook (in this case `Classroom-Setup`), which prepares the data for this lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Run the following cell to mimic a streaming data source"],"metadata":{}},{"cell_type":"code","source":["%run \"../Includes/Stream-Generator\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Lesson\n\n### Use cases for Apache Spark\n* Read and process huge files and data sets\n* Query, explore, and visualize data sets\n* Join disparate data sets found in data lakes\n* Train and evaluate machine learning models\n* Process live streams of data\n* Perform analysis on large graph data sets and social networks\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Focus on learning the types of problems solved by Spark; the code examples are explained either later in this course or future courses."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<div>\n  <div>**Apache Spark is used to...**</div>\n  <div><h3 style=\"margin-top:0; margin-bottom:0.75em\">Read and process huge files and data sets</h3></div>\n</div>\nSpark provides a query engine capable of processing data in very, very large data files.  Some of the largest Spark jobs in the world run on Petabytes of data."],"metadata":{}},{"cell_type":"markdown","source":["The files in `dbfs:/mnt/training/asa/flights/all-by-year/` are stored in Azure Storage and made easily accessible using the Databricks Filesystem (dbfs).\n\nThe `%fs ls` command lists the contents of the bucket.  There are 22 comma-separated-values (CSV) files containing flight data for 1987-2008.  Spark can readily handle petabytes of data given a sufficiently large cluster."],"metadata":{}},{"cell_type":"code","source":["%fs ls dbfs:/mnt/training/asa/flights/all-by-year/"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["The `CREATE TABLE` statement below registers the CSV file as a SQL Table.  The CSV file can then be queried directly using SQL.\n\nIn order to allow this example to run quickly on a small cluster, we'll use the file `small.csv` instead."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nCREATE DATABASE IF NOT EXISTS Databricks;\nUSE Databricks;\n\nCREATE TABLE IF NOT EXISTS AirlineFlight\nUSING CSV\nOPTIONS (\n  header=\"true\",\n  delimiter=\",\",\n  inferSchema=\"true\",\n  path=\"dbfs:/mnt/training/asa/flights/small.csv\"\n);\n\nCACHE TABLE AirlineFlight;\n\nSELECT * FROM AirlineFlight;"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["-sandbox\n<div>\n  <div>**Apache Spark is used to...**</div>\n  <div><h3 style=\"margin-top:0; margin-bottom:0.75em\">Query, explore, and visualize data sets</h3></div>\n</div>\n\nSpark can perform complex queries to extract insights from large files and visualize the results."],"metadata":{}},{"cell_type":"markdown","source":["The example below creates a table from a CSV file listing flight delays by airplane, counts the number of delays per model of airplane, and then graphs it."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nCREATE TABLE IF NOT EXISTS AirlinePlane\nUSING csv\nOPTIONS (\n  header = \"true\",\n  delimiter = \",\",\n  inferSchema = \"false\",\n  path = \"dbfs:/mnt/training/asa/planes/plane-data.csv\"\n);\n\nCACHE TABLE AirlinePlane;\n\nSELECT Model, count(*) AS Delays FROM AirlinePlane WHERE Model IS NOT NULL GROUP BY Model ORDER BY Delays DESC LIMIT 10;"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["-sandbox\n<div>\n  <div>**Apache Spark is used to...**</div>\n  <div><h3 style=\"margin-top:0; margin-bottom:0.75em\">Join disparate data sets found in data lakes</h3></div>\n</div>\n\nCompanies frequently have thousands of large data files gathered from various teams and departments, typically using a diverse variety of formats, including CSV, JSON and XML.\n\nThese are called Data Lakes. Data Lakes differ from Data Warehouses in that they don't require someone to spend weeks or months preparing a unified enterprise schema and then populating it.\n\nFrequently an analyst wishes to run simple queries across various data files, without taking the time required to construct a fully-fledged Data Warehouse.\n\nSpark excels in this type of workload by enabling users to simultaneously query files from many different storage locations, and then formats and join them together using Spark SQL.\n\nSpark can later load the data into a Data Warehouse if desired."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT p.manufacturer AS Manufacturer,\n       avg(depDelay) AS Delay\nFROM AirlinePlane p\nJOIN AirlineFlight f ON p.tailnum = f.tailnum\nWHERE p.manufacturer IS NOT null\nGROUP BY p.manufacturer\nORDER BY Delay DESC\nLIMIT 10"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Not only does Spark bring together files from many different locations, it also brings in disparate data sources and file types such as:\n* JDBC Data Sources like SQL Server, Azure SQL Database, MySQL, PostgreSQL, Oracle,  etc.\n* Parquet files\n* CSV files\n* ORC files\n* JSON files\n* HDFS file systems\n* Apache Kafka\n* And with a little extra work, Web Services Endpoints, TCP-IP sockets, and just about anything else you can imagine!"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<div>\n  <div>**Apache Spark is used to...**</div>\n  <div><h3 style=\"margin-top:0; margin-bottom:0.75em\">Train and evaluate machine learning models</h3></div>\n</div>\n\nSpark performs predictive analytics using machine learning algorithms.\n\nThe example below trains a linear regression model using past flight data to predict delays based on the hour of the day."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, floor, translate, round\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler, OneHotEncoder\nfrom pyspark.ml.regression import LinearRegression\n\ninputDF = (spark.read.table(\"AirlineFlight\")\n  .withColumn(\"HourOfDay\", floor(col(\"CRSDepTime\")/100))\n  .withColumn(\"DepDelay\", translate(col(\"DepDelay\"), \"NA\", \"0\").cast(\"integer\")))\n\n(trainingDF, testDF) = inputDF.randomSplit([0.80, 0.20], seed=999)\n\npipeline = Pipeline(stages=[\n    OneHotEncoder(inputCol=\"HourOfDay\", outputCol=\"HourVector\"),\n    VectorAssembler(inputCols=[\"HourVector\"], outputCol=\"Features\"),\n    LinearRegression(featuresCol=\"Features\", labelCol=\"DepDelay\", predictionCol=\"DepDelayPredicted\", regParam=0.0)\n  ])\n\nmodel = pipeline.fit(trainingDF)\nresultDF = model.transform(testDF)\n\ndisplayDF = resultDF.select(\"Year\", \"Month\", \"DayOfMonth\", \"CRSDepTime\", \"UniqueCarrier\", \"FlightNum\", \"DepDelay\", round(\"DepDelayPredicted\", 2).alias(\"DepDelayPredicted\"))\ndisplay(displayDF)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["display(\n  resultDF\n    .groupBy(\"HourOfDay\")\n    .avg(\"DepDelay\", \"DepDelayPredicted\")\n    .toDF(\"HourOfDay\", \"Actual\", \"Predicted\")\n    .orderBy(\"HourOfDay\")\n)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["-sandbox\n<div>\n  <div>**Apache Spark is used to...**</div>\n  <div><h3 style=\"margin-top:0; margin-bottom:0.75em\">Process live streams of data</h3></div>\n</div>\n\nBesides aggregating static data sets, Spark can also process live streams of data such as:\n* File Streams\n* TCP-IP Streams\n* Apache Kafka\n* Custom Streams like Twitter & Facebook"],"metadata":{}},{"cell_type":"markdown","source":["Before processing streaming data, a data source is required.\n\nThe cell below first deletes any temp files, and then generates a stream of fake flight data for up to 30 minutes."],"metadata":{}},{"cell_type":"code","source":["%scala\n\n// Clean any temp files from previous runs.\nDummyDataGenerator.clean()\n\n// Generate data for 5 minutes.\n// To force it to stop rerun with 0.\nDummyDataGenerator.start(5)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["The example below connects to and processes a fake, fire-hose of flight data by:\n0. Reading in a stream of constantly updating CSV files.\n0. Parsing the flight's date and time.\n0. Computing the average delay for each airline based on the most recent 15 seconds of flight data.\n0. Plotting the results in near-real-time.\n\n**Disclaimer:** The real-time data represented here is completely fictional and is not intended to reflect actual airline performance."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, date_format, unix_timestamp, window\nfrom pyspark.sql.types import StructType\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n\nflightSchema = (StructType()\n  .add(\"FlightNumber\", \"integer\")\n  .add(\"DepartureTime\", \"string\")\n  .add(\"Delay\", \"double\")\n  .add(\"Airline\", \"string\")\n)\nstreamingDF = (spark.readStream\n  .schema(flightSchema)\n  .csv(DummyDataGenerator.streamDirectory)\n  .withColumn(\"DepartureTime\", unix_timestamp(\"DepartureTime\", \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"timestamp\"))\n  .withWatermark(\"DepartureTime\", \"5 minute\")\n  .groupBy( window(\"DepartureTime\", \"15 seconds\"), \"Airline\" )\n  .avg(\"Delay\")\n  .select(col(\"window.start\").alias(\"Start\"), \"Airline\", col(\"avg(delay)\").alias(\"Average Delay\"))\n  .orderBy(\"start\", \"Airline\")\n  .select(date_format(\"start\", \"HH:mm:ss\").alias(\"Time\"), \"Airline\", \"Average Delay\")\n)\ndisplay(streamingDF)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["-sandbox\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Remember to stop your stream by clicking the **Cancel** link up above."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<div>\n  <div>**Apache Spark is used to...**</div>\n  <div><h3 style=\"margin-top:0; margin-bottom:0.75em\">Perform analysis on large graph data sets and social networks</h3></div>\n</div>\n\nThe open source <a href=\"https://graphframes.github.io/\" target=\"_blank\">GraphFrames</a> library extends Spark to study not the data itself, but the network of relationships between entities.  This facilitates queries such as:\n* **Shortest Path:** What is the shortest route from Springfield, IL to Austin, TX?\n* **Page Rank:** Which airports are the most important hubs in the USA?\n* **Connected Components:** Find strongly connected groups of friends on Facebook.\n* (just to name a few)"],"metadata":{}},{"cell_type":"markdown","source":["#### Connected Graphs\n\nThe example below is a visualization of a network of airports connected by flight routes.\n\nDatabricks can display this network using popular third-party visualization library such as:\n* <a href=\"https://d3js.org/\" target=\"_blank\">D3.js - Data-Driven Documents</a>\n* <a href=\"https://matplotlib.org/\" target=\"_blank\">Matplotlib: Python plotting</a>\n* <a href=\"http://ggplot.yhathq.com/\" target=\"_blank\">ggplot</a>\n* <a href=\"https://plot.ly/\" target=\"_blank\">Plotly<a/>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n<iframe style='border-style:none; position:absolute; left:-150px; width:1170px; height:700px'\n          src='https://mbostock.github.io/d3/talk/20111116/#14'\n/>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n#### PageRank algorithm\nThe <a href=\"https://en.wikipedia.org/wiki/PageRank\" target=\"_blank\">PageRank</a> algorithm, named after Google co-founder Larry Page, assesses the importance of a hub in a network.\n\nThe example below uses the <a href=\"https://graphframes.github.io/\" target=\"_blank\">GraphFrames</a> API  to compute the **PageRank** of each airport in the United States and shows the top 10 most important US airports.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> This example requires the GraphFrames library that may not have been setup on your cluster.  Read the example below rather than running it."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, concat_ws, round\nfrom graphframes import GraphFrame\n\nflightVerticesDF = (spark.read\n  .option(\"header\", True)\n  .option(\"delimiter\", \"\\t\")\n  .csv(\"dbfs:/mnt/training/asa/airport-codes/airport-codes.txt\")\n  .withColumnRenamed(\"IATA\", \"id\"))\n\nflightEdgesDF = (spark.table(\"Databricks.AirlineFlight\")\n  .withColumnRenamed(\"Origin\", \"src\")\n  .withColumnRenamed(\"Dest\", \"dst\"))\n\nflightGF = GraphFrame(flightVerticesDF, flightEdgesDF)\npageRankDF = flightGF.pageRank(tol=0.05)\n\nresultsDF = (pageRankDF.vertices\n  .select(concat_ws(\", \", col(\"city\"), col(\"state\")).alias(\"Location\"),\n          round(col(\"pagerank\"), 1).alias(\"Rank\"))\n  .orderBy(col(\"pagerank\").desc()))\n\ndisplay(resultsDF)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["## Review\n**Question:** Which of the following are good applications for Apache Spark? (Select all that apply.)\n0. Querying, exploring, and analyzing very large files and data sets\n0. Joining data lakes\n0. Machine learning and predictive analytics\n0. Processing streaming data\n0. Graph analytics\n0. Overnight batch processing of very large files\n0. Updating individual records in a database\n\n**Answer:** All but #7. Apache Spark uses SQL to read and performs analysis on large files, but it is not a Database."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** What makes Spark different than Hadoop?\n**A:** Spark on Databricks performs 10-2000x faster than Hadoop Map-Reduce.  It does this by providing a high-level query API which allows Spark to highly optimize the internal execution without adding complexity for the user.  Internally, Spark employs a large number of optimizations such as pipelining related tasks together into a single operation, communicating in memory, using just-in-time code generation, query optimization, efficient tabular memory (Tungsten), caching, and more.\n\n**Q:** What are the visualization options in Databricks?\n**A:** Databricks provides a wide variety of <a href=\"https://docs.databricks.com/user-guide/visualizations/index.html\" target=\"_blank\">built-in visualizations</a>.  Databricks also supports a variety of 3rd party visualization libraries, including <a href=\"https://d3js.org/\" target=\"_blank\">d3.js</a>, <a href=\"https://matplotlib.org/\" target=\"_blank\">matplotlib</a>, <a href=\"http://ggplot.yhathq.com/\" target=\"_blank\">ggplot</a>, and <a href=\"https://plot.ly/\" target=\"_blank\">plotly<a/>.\n\n**Q:** Where can I learn more about DBFS?\n**A:** See the document <a href=\"https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html\" target=\"_blank\">Databricks File System - DBFS</a>.\n\n**Q:** Where can I find a list of the machine learning algorithms supported by Spark?\n**A:** The Spark documentation for Machine Learning describes the algorithms for classification, regression, clustering, recommendations (ALS), neural networks, and more.  The documentation doesn't provide a single consolidated list, but by browsing through the <a href=\"http://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\">Spark MLLib documentation</a> you can find the supported algorithms.  Additionally, <a href=\"https://spark-packages.org/\" target=\"_blank\">3rd party libraries</a> provide even more algorithms and capabilities.\n\n**Q:** Where can I learn more about stream processing in Spark?\n**A:** See the <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\" target=\"_blank\">Structured Streaming Programming Guide</a>.\n\n**Q:** Where can I learn more about GraphFrames?\n**A:** See the <a href=\"http://graphframes.github.io/\" target=\"_blank\">GraphFrames Overview</a>.  The Databricks blog has an <a href=\"https://databricks.com/blog/2016/03/16/on-time-flight-performance-with-graphframes-for-apache-spark.html\">example</a> which uses d3 to perform visualizations of GraphFrame data."],"metadata":{}}],"metadata":{"name":"01-Why-Spark","notebookId":291050441001101},"nbformat":4,"nbformat_minor":0}
