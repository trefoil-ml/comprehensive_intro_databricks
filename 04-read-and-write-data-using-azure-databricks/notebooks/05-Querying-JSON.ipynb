{"cells":[{"cell_type":"markdown","source":["# Querying JSON & Hierarchical Data with DataFrames\n\nApache Spark&trade; and Azure Databricks&reg; make it easy to work with hierarchical data, such as nested JSON records."],"metadata":{}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Examining the Contents of a JSON file\n\nJSON is a common file format used in big data applications and in data lakes (or large stores of diverse data).  File formats such as JSON arise out of a number of data needs.  For instance, what if:\n<br>\n* Your schema, or the structure of your data, changes over time?\n* You need nested fields like an array with many values or an array of arrays?\n* You don't know how you're going use your data yet, so you don't want to spend time creating relational tables?\n\nThe popularity of JSON is largely due to the fact that JSON allows for nested, flexible schemas.\n\nThis lesson uses the `DatabricksBlog` table, which is backed by JSON file `dbfs:/mnt/training/databricks-blog.json`. If you examine the raw file, notice it contains compact JSON data. There's a single JSON object on each line of the file; each object corresponds to a row in the table. Each row represents a blog post on the <a href=\"https://databricks.com/blog\" target=\"_blank\">Databricks blog</a>, and the table contains all blog posts through August 9, 2017."],"metadata":{}},{"cell_type":"code","source":["%fs head dbfs:/mnt/training/databricks-blog.json"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Create a DataFrame out of the syntax introduced in the previous lesson:"],"metadata":{}},{"cell_type":"code","source":["databricksBlogDF = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").json(\"/mnt/training/databricks-blog.json\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Take a look at the schema by invoking `printSchema` method."],"metadata":{}},{"cell_type":"code","source":["databricksBlogDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Run a query to view the contents of the table.\n\nNotice:\n* The `authors` column is an array containing one or more author names.\n* The `categories` column is an array of one or more blog post category names.\n* The `dates` column contains nested fields `createdOn`, `publishedOn` and `tz`."],"metadata":{}},{"cell_type":"code","source":["display(databricksBlogDF.select(\"authors\",\"categories\",\"dates\",\"content\"))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Nested Data\n\nThink of nested data as columns within columns. \n\nFor instance, look at the `dates` column."],"metadata":{}},{"cell_type":"code","source":["datesDF = databricksBlogDF.select(\"dates\")\ndisplay(datesDF)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Pull out a specific subfield with `.` (object) notation."],"metadata":{}},{"cell_type":"code","source":["display(databricksBlogDF.select(\"dates.createdOn\", \"dates.publishedOn\"))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Create a DataFrame, `databricksBlog2DF` that contains the original columns plus the new `publishedOn` column obtained\nfrom flattening the dates column."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndatabricksBlog2DF = databricksBlogDF.withColumn(\"publishedOn\",col(\"dates.publishedOn\"))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["With this temporary view, apply the `printSchema` method to check its schema and confirm the timestamp conversion."],"metadata":{}},{"cell_type":"code","source":["databricksBlog2DF.printSchema()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Both `createdOn` and `publishedOn` are stored as strings.\n\nCast those values to SQL timestamps:\n\nIn this case, use a single `select` method to:\n0. Cast `dates.publishedOn` to a `timestamp` data type\n0. \"Flatten\" the `dates.publishedOn` column to just `publishedOn`"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import date_format\ndisplay(databricksBlogDF.select(\"title\",date_format(\"dates.publishedOn\",\"yyyy-MM-dd\").alias(\"publishedOn\")))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Create another DataFrame, `databricksBlog2DF` that contains the original columns plus the new `publishedOn` column obtained\nfrom flattening the dates column."],"metadata":{}},{"cell_type":"code","source":["databricksBlog2DF = databricksBlogDF.withColumn(\"publishedOn\", date_format(\"dates.publishedOn\",\"yyyy-MM-dd\")) \ndisplay(databricksBlog2DF)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["With this temporary view, apply the `printSchema` method to check its schema and confirm the timestamp conversion."],"metadata":{}},{"cell_type":"code","source":["databricksBlog2DF.printSchema()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["-sandbox\nSince the dates are represented by a `timestamp` data type, we need to convert to a data type that allows `<` and `>`-type comparison operations in order to query for articles within certain date ranges (such as a list of all articles published in 2013). This is accopmplished by using the `to_date` function in Scala or Python.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> See the Spark documentation on <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\" target=\"_blank\">built-in functions</a>, for a long list of date-specific functions."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import to_date, year, col\n          \nresultDF = (databricksBlog2DF.select(\"title\", to_date(col(\"publishedOn\"),\"MMM dd, yyyy\").alias('date'),\"link\") \n  .filter(year(col(\"publishedOn\")) == '2013') \n  .orderBy(col(\"publishedOn\"))\n)\n\ndisplay(resultDF)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["## Array Data\n\nThe DataFrame also contains array columns. \n\nEasily determine the size of each array using the built-in `size(..)` function with array columns."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import size\ndisplay(databricksBlogDF.select(size(\"authors\"),\"authors\"))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Pull the first element from the array `authors` using an array subscript operator.\n\nFor example, in Scala, the 0th element of array `authors` is `authors(0)`\nwhereas, in Python, the 0th element of `authors` is `authors[0]`."],"metadata":{}},{"cell_type":"code","source":["display(databricksBlogDF.select(col(\"authors\")[0].alias(\"primaryAuthor\")))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### Explode\n\nThe `explode` method allows you to split an array column into multiple rows, copying all the other columns into each new row. \n\nFor example, split the column `authors` into the column `author`, with one author per row."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import explode\ndisplay(databricksBlogDF.select(\"title\",\"authors\",explode(col(\"authors\")).alias(\"author\"), \"link\"))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["It's more obvious to restrict the output to articles that have multiple authors, and then sort by the title."],"metadata":{}},{"cell_type":"code","source":["databricksBlog2DF = (databricksBlogDF \n  .select(\"title\",\"authors\",explode(col(\"authors\")).alias(\"author\"), \"link\") \n  .filter(size(col(\"authors\")) > 1) \n  .orderBy(\"title\")\n)\n\ndisplay(databricksBlog2DF)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["## Exercise 1\n\nIdentify all the articles written or co-written by Michael Armbrust."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Step 1\n\nStarting with the `databricksBlogDF` DataFrame, create a DataFrame called `articlesByMichaelDF` where:\n0. Michael Armbrust is the author.\n0. The data set contains the column `title` (it may contain others).\n0. It contains only one record per article.\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** See the Spark documentation on <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\" target=\"_blank\">built-in functions</a>.  \n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** Include the column `authors` in your view to help you debug your solution."],"metadata":{}},{"cell_type":"code","source":["# TODO\nfrom pyspark.sql.functions import array_contains\narticlesByMichaelDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nfrom pyspark.sql import Row\n\nresultsCount = articlesByMichaelDF.count()\ndbTest(\"DF-L5-articlesByMichael-count\", 3, resultsCount)  \n\nresults = articlesByMichaelDF.collect()\n\ndbTest(\"DF-L5-articlesByMichael-0\", Row(title=u'Spark SQL: Manipulating Structured Data Using Apache Spark'), results[0])\ndbTest(\"DF-L5-articlesByMichael-1\", Row(title=u'Exciting Performance Improvements on the Horizon for Spark SQL'), results[1])\ndbTest(\"DF-L5-articlesByMichael-2\", Row(title=u'Spark SQL Data Sources API: Unified Data Access for the Apache Spark Platform'), results[2])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["### Step 2\nShow the list of Michael Armbrust's articles in HTML format."],"metadata":{}},{"cell_type":"markdown","source":["## Exercise 2\n\nIdentify the complete set of categories used in the Databricks blog articles."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1\n\nStarting with the `databricksBlogDF` DataFrame, create another DataFrame called `uniqueCategoriesDF` where:\n0. The data set contains the one column `category` (and no others).\n0. This list of categories should be unique."],"metadata":{}},{"cell_type":"code","source":["# TODO\nuniqueCategoriesDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nresultsCount =  uniqueCategoriesDF.count()\n\ndbTest(\"DF-L5-uniqueCategories-count\", 12, resultsCount)\n\nresults = uniqueCategoriesDF.collect()\n\ndbTest(\"DF-L5-uniqueCategories-0\", Row(category=u'Announcements'), results[0])\ndbTest(\"DF-L5-uniqueCategories-1\", Row(category=u'Apache Spark'), results[1])\ndbTest(\"DF-L5-uniqueCategories-2\", Row(category=u'Company Blog'), results[2])\n\ndbTest(\"DF-L5-uniqueCategories-9\", Row(category=u'Platform'), results[9])\ndbTest(\"DF-L5-uniqueCategories-10\", Row(category=u'Product'), results[10])\ndbTest(\"DF-L5-uniqueCategories-11\", Row(category=u'Streaming'), results[11])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["### Step 2\nShow the complete list of categories."],"metadata":{}},{"cell_type":"code","source":["# TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["## Exercise 3\n\nCount how many times each category is referenced in the Databricks blog."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Step 1\n\nStarting with the `databricksBlogDF` DataFrame, create another DataFrame called `totalArticlesByCategoryDF` where:\n0. The new DataFrame contains two columns, `category` and `total`.\n0. The `category` column is a single, distinct category (similar to the last exercise).\n0. The `total` column is the total number of articles in that category.\n0. Order by `category`.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Because articles can be tagged with multiple categories, the sum of the totals adds up to more than the total number of articles."],"metadata":{}},{"cell_type":"code","source":["# TODO\n\nfrom pyspark.sql.functions import count\ntotalArticlesByCategoryDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nresults = totalArticlesByCategoryDF.count()\n\ndbTest(\"DF-L5-articlesByCategory-count\", 12, results)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nresults = totalArticlesByCategoryDF.collect()\n\ndbTest(\"DF-L5-articlesByCategory-0\", Row(category=u'Announcements', total=72), results[0])\ndbTest(\"DF-L5-articlesByCategory-1\", Row(category=u'Apache Spark', total=132), results[1])\ndbTest(\"DF-L5-articlesByCategory-2\", Row(category=u'Company Blog', total=224), results[2])\n\ndbTest(\"DF-L5-articlesByCategory-9\", Row(category=u'Platform', total=4), results[9])\ndbTest(\"DF-L5-articlesByCategory-10\", Row(category=u'Product', total=83), results[10])\ndbTest(\"DF-L5-articlesByCategory-11\", Row(category=u'Streaming', total=21), results[11])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["### Step 2\nDisplay the totals of each category in html format (should be ordered by `category`)."],"metadata":{}},{"cell_type":"code","source":["# TODO\n\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["## Summary\n\n* Spark DataFrames allows you to query and manipulate structured and semi-structured data.\n* Spark DataFrames built-in functions provide powerful primitives for querying complex schemas."],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n**Q:** What is the syntax for accessing nested columns?  \n**A:** Use the dot notation:\n`select(\"dates.publishedOn\")`\n\n**Q:** What is the syntax for accessing the first element in an array?  \n**A:** Use the [subscript] notation: \n`select(\"col(authors)[0]\")`\n\n**Q:** What is the syntax for expanding an array into multiple rows?  \n**A:** Use the explode method:  `select(explode(col(\"authors\")).alias(\"Author\"))`"],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Querying Data Lakes with DataFrames]($./06-Data-Lakes)."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\" target=\"_blank\">Spark SQL, DataFrames and Datasets Guide</a>"],"metadata":{}}],"metadata":{"name":"05-Querying-JSON","notebookId":291050441000809},"nbformat":4,"nbformat_minor":0}
