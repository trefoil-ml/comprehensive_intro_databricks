{"cells":[{"cell_type":"markdown","source":["# Azure Cosmos DB - Connect using Key Vault\n\nIn an earlier lesson ([Key Vault-backed Secret Scopes]($./08-Key-Vault-backed-secret-scopes)), you created a Key Vault-backed secret scope for Azure Databricks, and securely stored your Azure Cosmos DB key within.\n\nIn this lesson, you will use the Azure Cosmos DB secrets that are securely stored within the Key Vault-backed secret scope to connect to your Azure Cosmos DB instance. Next, you will use the DataFrame API to execute SQL queries and control the parallelism of reads through the JDBC interface.\n\nIf you are running in an Azure Databricks environment that is already pre-configured with the libraries you need, you can skip to the next cell. To use this notebook in your own Databricks environment, you will need to create libraries, using the [Create Library](https://docs.azuredatabricks.net/user-guide/libraries.html) interface in Azure Databricks. Follow the steps below to attach the `azure-eventhubs-spark` library to your cluster:\n\n1. In the left-hand navigation menu of your Databricks workspace, select **Workspace**, select the down chevron next to **Shared**, and then select **Create** and **Library**.\n\n  ![Create Databricks Library](https://databricksdemostore.blob.core.windows.net/images/08/03/databricks-create-library.png 'Create Databricks Library')\n\n2. On the New Library screen, do the following:\n\n  - **Source**: Select Maven Coordinate.\n  - **Coordinate**: Enter \"azure-cosmosdb-spark\", and then select **com.microsoft.azure:azure-cosmosdb-spark_2.3.0_2.11:1.2.7** (or later version).\n  - Select **Create Library**.\n  \n  ![Databricks new Maven library](https://databricksdemostore.blob.core.windows.net/images/04-MDW/cosmos-db-create-library.png 'Databricks new Maven library')\n\n3. On the library page that is displayed, check the **Attach** checkbox next to the name of your cluster to run the library on that cluster.\n\n  ![Databricks attach library](https://databricksdemostore.blob.core.windows.net/images/04-MDW/cosmos-db-attach-library.png 'Databricks attach library')\n\nOnce complete, return to this notebook to continue with the lesson."],"metadata":{}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Access Key Vault secrets and configure the Cosmos DB connector\n\nIn a previous lesson, you created two secrets in Key Vault for your Azure Cosmos DB instance: **cosmos-uri** and **cosmos-key**. These two values will be used to configure the Cosmos DB connector. Let's start out by retrieving those values and storing them in new variables."],"metadata":{}},{"cell_type":"code","source":["uri = dbutils.secrets.get(scope = \"key-vault-secrets\", key = \"cosmos-uri\")\nkey = dbutils.secrets.get(scope = \"key-vault-secrets\", key = \"cosmos-key\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["In order to query Cosmos DB, you need to first create a configuration object that contains the configuration information. \n\nIf you are curious, read the [configuration reference](https://github.com/Azure/azure-cosmosdb-spark/wiki/Configuration-references) for details on all of the options. \n\nThe core items you need to provide are:\n\n**Endpoint**: Your Cosmos DB url (i.e. https://youraccount.documents.azure.com:443/)\n\n**Masterkey**: The primary or secondary key string for you Cosmos DB account\n\n**Database**: The name of the database\n\n**Collection**: The name of the collection that you wish to query"],"metadata":{}},{"cell_type":"code","source":["readConfig = {\n\"Endpoint\" : uri, # from Key Vault\n\"Masterkey\" : key, # from Key Vault\n\"Database\" : \"demos\",\n\"Collection\" : \"documents\",\n\"preferredRegions\" : \"West US\", # or whichver region you added Cosmos DB to\n\"SamplingRatio\" : \"1.0\",\n\"schema_samplesize\" : \"1000\",\n\"query_pagesize\" : \"2147483647\",\n}"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Load parquet data into DataFrames and normalize\n\nWe want to normalize New York and Boston data sets and join them into a new DataFrame. This way, we can write the joined data into our Cosmos DB collection so that data can be accessed by external services and applications.  \n\nStart by creating a DataFrame of the data from New York and then Boston:\n\n| City          | Table Name              | Path to DBFS file\n| ------------- | ----------------------- | -----------------\n| **New York**  | `CrimeDataNewYork`      | `dbfs:/mnt/training/crime-data-2016/Crime-Data-New-York-2016.parquet`\n| **Boston**    | `CrimeDataBoston`       | `dbfs:/mnt/training/crime-data-2016/Crime-Data-Boston-2016.parquet`\n\nThen, normalize the data structure of each table so all the columns (and their values) line up with each other.\n\nIn the case of New York and Boston, here are the unique characteristics of each data set:\n\n| | Offense-Column        | Offense-Value          | Reported-Column  | Reported-Data Type |\n|-|-----------------------|------------------------|-----------------------------------|\n| New York | `offenseDescription`  | starts with \"murder\" or \"homicide\" | `reportDate`     | `timestamp`    |\n| Boston | `OFFENSE_CODE_GROUP`  | \"Homicide\"             | `MONTH`          | `integer`      |"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import lower, upper, month, col\n\ncrimeDataNewYorkDF = spark.read.parquet(\"/mnt/training/crime-data-2016/Crime-Data-New-York-2016.parquet\")\ncrimeDataBostonDF = spark.read.parquet(\"/mnt/training/crime-data-2016/Crime-Data-Boston-2016.parquet\")\n\nhomicidesNewYorkDF = (crimeDataNewYorkDF \n  .select(month(col(\"reportDate\")).alias(\"month\"), col(\"offenseDescription\").alias(\"offense\")) \n  .filter(lower(col(\"offenseDescription\")).contains(\"murder\") | lower(col(\"offenseDescription\")).contains(\"homicide\"))\n)\n\nhomicidesBostonDF = (crimeDataBostonDF \n  .select(\"month\", col(\"OFFENSE_CODE_GROUP\").alias(\"offense\")) \n  .filter(lower(col(\"OFFENSE_CODE_GROUP\")).contains(\"homicide\"))\n)\n\n# Join both DataFrames into a new one\nhomicidesBostonAndNewYorkDF = homicidesNewYorkDF.union(homicidesBostonDF)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Let's take a look at the new DataFrame to get a sense of the data we will be writing to Cosmos DB."],"metadata":{}},{"cell_type":"code","source":["display(homicidesBostonAndNewYorkDF)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Write data to Cosmos DB"],"metadata":{}},{"cell_type":"markdown","source":["You write data back to Cosmos DB by creating a DataFrame that contains the desired changes and then use the Write API to save the changes back. \n\nAs Spark DataFrames are immutable, if you want to insert new rows to a collection in Cosmos DB, you will need to create a new DataFrame. If you want to both modify and insert, the connector supports the upsert operation."],"metadata":{}},{"cell_type":"markdown","source":["The Write API requires similar configuration to the Read API we used previously. Take note of the Upsert parameter we use here, which enables Upsert actions on the Cosmos DB side."],"metadata":{}},{"cell_type":"code","source":["writeConfig = {\n\"Endpoint\" : uri,\n\"Masterkey\" : key,\n\"Database\" : \"demos\",\n\"Collection\" : \"documents\",\n\"Upsert\" : \"true\"\n}"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["By default, the Connector will not allow us to write to a collection that has documents already. In order to perform an upsert, we must specify that mode is set to `overwrite`."],"metadata":{}},{"cell_type":"code","source":["homicidesBostonAndNewYorkDF.write.format(\"com.microsoft.azure.cosmosdb.spark\").mode(\"overwrite\").options(**writeConfig).save()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Challenge"],"metadata":{}},{"cell_type":"markdown","source":["Read the data back out of Cosmos DB, using the Read Connector you configured earlier."],"metadata":{}},{"cell_type":"code","source":["# TODO\n\n# Add your code here, saving to a DataFrame named 'documents'\n\ndocuments = spark.read.format(\"com.microsoft.azure.cosmosdb.spark\").options(**readConfig).load()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Display the contents of the `documents` DataFrame to ensure the data was successfully retrieved."],"metadata":{}},{"cell_type":"code","source":["display(documents)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Reference\n\nIf you wish to apply what you have learned so far to a new set of challenges, complete the optional exercise: [Exploratory Data Analysis]($./Optional/Exploratory-Data-Analysis)"],"metadata":{}}],"metadata":{"name":"10-Cosmos-DB-Connect-Using-Key-Vault","notebookId":291050441001450},"nbformat":4,"nbformat_minor":0}
