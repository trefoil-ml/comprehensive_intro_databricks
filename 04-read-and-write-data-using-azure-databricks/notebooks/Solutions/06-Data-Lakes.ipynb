{"cells":[{"cell_type":"markdown","source":["# Querying Data Lakes with DataFrames\n\nApache Spark&trade; and Azure Databricks&reg; make it easy to access and work with files stored in Data Lakes, such as Azure Data Lake Storage (ADLS).\n\nCompanies frequently store thousands of large data files gathered from various teams and departments, typically using a diverse variety of formats including CSV, JSON, and XML. Data scientists often wish to extract insights from this data.\n\nThe classic approach to querying this data is to load it into a central database called a **data warehouse**. Traditionally, data engineers must design the schema for the central database, extract the data from the various data sources, transform the data to fit the warehouse schema, and load it into the central database. A data scientist can then query the data warehouse directly or query smaller data sets created to optimize specific types of queries. The data warehouse approach works well, but requires a great deal of up front effort to design and populate schemas. It also limits historical data, which is constrained to only the data that fits the warehouse’s schema.\n\nAn alternative approach is a **Data Lake**, which:\n\n* Is a storage repository that cheaply stores a vast amount of raw data in its native format.\n* Consists of current and historical data dumps in various formats including XML, JSON, CSV, Parquet, etc.\n* May contain operational relational databases with live transactional data.\n\nSpark is ideal for querying Data Lakes. Spark DataFrames can be used to read directly from raw files contained in a Data Lake and then execute queries to join and aggregate the data.\n\nThis lesson illustrates how to perform exploratory data analysis (EDA) to gain insights from a Data Lake.\n\n## Prerequisites\n* **IMPORTANT**: You must have permissions within your Azure subscription to create an App Registration and service principal within Azure Active Directory to complete this lesson.\n* Lesson: <a href=\"$./02-Querying-Files\">Querying Files with SQL</a>"],"metadata":{}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Create Azure Data Lake Storage Gen1 (ADLS)\n\n1. In the [Azure portal](https://portal.azure.com), select **+ Create a resource**, enter \"data lake\" into the Search the Marketplace box, select **Data Lake Storage Gen1** from the results, and then select **Create**.\n\n   ![In the Azure portal, +Create a resource is highlighted in the navigation pane, \"data lake\" is entered into the Search the Marketplace box, and Data Lake Storage Gen1 is highlighted in the results.](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-create-resource.png 'Create Azure Data Lake Storage Gen1')\n\n2. On the New Data Lake Storage Gen1 blade, enter the following:\n\n   - **Name**: Enter a globally unique name (indicated by a green check mark).\n   - **Subscription**: Select the subscription you are using for this module.\n   - **Resource group**: Choose your module resource group.\n   - **Location**: Select the closest location.\n   - **Pricing package**: Choose Pay-as-You-Go.\n   - **Encryption settings**: Leave set to the default value of Enabled.\n\n   ![The New Data Lake Storage Gen1 blade is displayed, with the previously mentioned settings entered into the appropriate fields.](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-create-new.png 'New Data Lake Storage Gen1')\n\n3. Select **Create** to provision the new ADLS instance.\n\n4. In the cell below, set the value of the `adlsAccountName` variable to the same name you used for the **Name** field when creating your ADLS instance above, and then run the cell."],"metadata":{}},{"cell_type":"code","source":["adlsAccountName = \"<your-data-lake-store-account-name>\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Create Azure Active Directory application and service principal\n\n> **IMPORTANT**: You must have permissions within your Azure subscription to create an App registration and service principal within Azure Active Directory to complete this lesson.\n\nADLS uses Azure Active Directory for authentication. To provide access to your ADLS instance from Azure Databricks, you will use [service-to-service authentication](https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-service-to-service-authenticate-using-active-directory). For this, you need to create an identity in Azure Active Directory (Azure AD) known as a service principal.\n\n1. In the [Azure portal](https://portal.azure.com), select **Azure Active Directory** from the left-hand navigation menu, select **App registrations**, and then select **+ New application registration**.\n\n   ![Register new app in Azure Active Directory](https://databricksdemostore.blob.core.windows.net/images/04/06/aad-app-registration.png 'Register new app in Azure Active Directory')\n\n2. On the Create blade, enter the following:\n\n  * **Name**: Enter a unique name, such as databricks-demo (this name must be unique, as indicated by a green check mark).\n  * **Application type**: Select Web app / API.\n  * **Sign-on URL**: Enter https://databricks-demo.com.\n\n   ![Create a new app registration](https://databricksdemostore.blob.core.windows.net/images/04/06/aad-app-create.png 'Create a new app registration')\n\n3. Select **Create**.\n\n4. To access your ADLS instance from Azure Databricks you will need to provide the credentials of your newly created service principal within Databricks. On the Registered app blade that appears, copy the **Application ID** and paste it into the cell below as the value for the `clientId` variable.\n\n   ![Copy the Registered App Application ID](https://databricksdemostore.blob.core.windows.net/images/04/06/registered-app-id.png 'Copy the Registered App Application ID')\n\n5. Next, select **Settings** on the Registered app blade, and then select **Keys**.\n\n   ![Open Keys blade for the Registered App](https://databricksdemostore.blob.core.windows.net/images/04/06/registered-app-settings-keys.png 'Open Keys blade for the Registered App')\n\n6. On the Keys blade, you will create a new password by doing the following under Passwords:\n\n  * **Description**: Enter a description, such as ADLS Auth.\n  * **Expires**: Select a duration, such as In 1 year.\n\n  ![Create new password](https://databricksdemostore.blob.core.windows.net/images/04/06/registered-app-create-key.png 'Create new password')\n\n7. Select **Save**, and then copy the key displayed under **Value**, and paste it into the cell below for the value of the `clientKey` variable. **Note**: This value will not be accessible once you navigate away from this screen, so make sure you copy it before leaving the Keys blade.\n\n  ![Copy key value](https://databricksdemostore.blob.core.windows.net/images/04/06/registered-app-key-value.png 'Copy key value')\n\n8. Run the cell below."],"metadata":{}},{"cell_type":"code","source":["clientId = \"<your-service-client-id>\"\nclientKey = \"<your-service-credentials>\""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Retrieve your Azure AD tenant ID\n\nTo perform authentication using the service principal account, Databricks uses OAUTH2. For this, you need to provide your Azure AD Tenant ID.\n\n1. To retrieve your tenant ID, select **Azure Active Directory** from the left-hand navigation menu in the Azure portal, then select **Properties**, and select the copy button next to **Directory ID** on the Directory Properties blade.\n\n   ![Retrieve Tenant ID](https://databricksdemostore.blob.core.windows.net/images/04/06/aad-tenant-id.png 'Retrieve Tenant ID')\n\n2. Paste the copied value into the cell below for the value of the `tenantId` variable, and then run the cell."],"metadata":{}},{"cell_type":"code","source":["tenantId = \"<your-directory-id>\""],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Assign permissions to the service principal in ADLS\n\nNext, you need to assign the required permissions to the service principal in ADLS.\n\n1. In the [Azure portal](https://portal.azure.com), navigate to the ADLS instance you created above, and on the Overview blade, select **Data explorer**.\n\n   ![ADLS Overview blade](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-overview.png 'ADLS Overview blade')\n\n2. In the Data Explorer blade, select **Access** on the toolbar.\n\n   ![ADLS Data Explorer toolbar](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-data-explorer-toolbar.png 'ADLS Data Explorer toolbar')\n\n3. On the Access blade, select **+ Add**.\n\n   ![ADLS Data Explorer add access](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-access.png 'ADLS Data Explorer add access')\n\n4. On the Assign permissions -> Select user or group blade, enter the name of your Registered app (e.g., databricks-demo) into the **Select** box, choose your app from the list, and select **Select**.\n\n   ![ADLS assign permissions to user or group](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-assign-permissions-select-user-or-group.png 'ADLS assign permissions to user or group')\n\n5. On the Assign permissions -> Select permissions blade, set the following:\n\n  * **Permissions**: Check **Read**, **Write**, and **Execute**.\n  * **Add to**: Choose This folder and all children.\n  * **Add as**: Choose An access permission entry.\n  \n  ![ADLS assign permissions](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-assign-permissions.png 'ADLS assign permissions')\n\n6. Select **Ok**\n\n7. You will now see the service principal listed under **Assigned permissions** on the Access blade.\n\n  ![ADLS assigned permissions](https://databricksdemostore.blob.core.windows.net/images/04/06/adls-assigned-permissions.png 'ADLS assigned permissions')"],"metadata":{}},{"cell_type":"markdown","source":["## Mount ADLS to DBFS\n\nYou are now ready to access your ADLS account from Azure Databricks. Run the cell below to set the required configuration and mount ADLS to DBFS."],"metadata":{}},{"cell_type":"code","source":["configs = {\"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\n           \"dfs.adls.oauth2.client.id\": clientId,\n           \"dfs.adls.oauth2.credential\": clientKey,\n           \"dfs.adls.oauth2.refresh.url\": \"https://login.microsoftonline.com/\" + tenantId + \"/oauth2/token\"}\n\ndbutils.fs.mount(\n  source = \"adl://\" + adlsAccountName + \".azuredatalakestore.net/\",\n  mount_point = \"/mnt/adls\",\n  extra_configs = configs)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## Copy data to ADLS\n\nRun the following cell to copy the Crime-data-2016 dataset from the Training folder into your ADLS instance, in a folder named \"training\". This will take a few minutes to complete."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.cp(\"/mnt/training/crime-data-2016\", \"mnt/adls/training/crime-data-2016\", true)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Looking at the files in our Data Lake\n\nStart by reviewing which files are in our Data Lake.\n\nIn `dbfs:/mnt/adls/training/crime-data-2016`, there are Parquet files containing 2016 crime data from several United States cities.\n\nIn the cell below we have data for Boston, Chicago, New Orleans, and more."],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/adls/training/crime-data-2016"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["The next step in looking at the data is to create a DataFrame for each file.  \n\nStart by creating a DataFrame of the data from New York and then Boston:\n\n| City          | Table Name              | Path to DBFS file\n| ------------- | ----------------------- | -----------------\n| **New York**  | `CrimeDataNewYork`      | `dbfs:/mnt/adls/training/crime-data-2016/Crime-Data-New-York-2016.parquet`\n| **Boston**    | `CrimeDataBoston`       | `dbfs:/mnt/adls/training/crime-data-2016/Crime-Data-Boston-2016.parquet`"],"metadata":{}},{"cell_type":"code","source":["crimeDataNewYorkDF = spark.read.parquet(\"/mnt/adls/training/crime-data-2016/Crime-Data-New-York-2016.parquet\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["crimeDataBostonDF = spark.read.parquet(\"/mnt/adls/training/crime-data-2016/Crime-Data-Boston-2016.parquet\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["With the two DataFrames created, it is now possible to review the first couple records of each file.\n\nNotice in the example below:\n* The `crimeDataNewYorkDF` and `crimeDataBostonDF` DataFrames use different names for the columns.\n* The data itself is formatted differently and different names are used for similar concepts.\n\nThis is common in a Data Lake. Often files are added to a Data Lake by different groups at different times. The advantage of this strategy is that anyone can contribute information to the Data Lake and that Data Lakes scale to store arbitrarily large and diverse data. The tradeoff for this ease in storing data is that it doesn’t have the rigid structure of a traditional relational data model, so the person querying the Data Lake will need to normalize data before extracting useful insights.\n\nThe alternative to a Data Lake is a data warehouse.  In a data warehouse, a committee often regulates the schema and ensures data is normalized before being made available.  This makes querying much easier but also makes gathering the data much more expensive and time-consuming.  Many companies choose to start with a Data Lake to accumulate data.  Then, as the need arises, they normalize data and produce higher quality tables for querying.  This reduces the up front costs while still making data easier to query over time.  The normalized tables can later be loaded into a formal data warehouse through nightly batch jobs.  In this way, Apache Spark is used to manage and query both Data Lakes and data warehouses."],"metadata":{}},{"cell_type":"code","source":["display(crimeDataNewYorkDF)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["display(crimeDataBostonDF)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Same Type of Data, Different Structure\n\nIn this section, we examine crime data to determine how to extract homicide statistics.\n\nBecause the data sets are pooled together in a Data Lake, each city may use different field names and values to indicate homicides, dates, etc.\n\nFor example:\n* Some cities use the value \"HOMICIDE\", \"CRIMINAL HOMICIDE\" or \"MURDER\".\n* In the New York data, the column is named `offenseDescription` while in the Boston data, the column is named `OFFENSE_CODE_GROUP`.\n* In the New York data, the date of the event is in the `reportDate`, while in the Boston data, there is a single column named `MONTH`."],"metadata":{}},{"cell_type":"markdown","source":["To get started, create a temporary view containing only the homicide-related rows.\n\nAt the same time, normalize the data structure of each table so all the columns (and their values) line up with each other.\n\nIn the case of New York and Boston, here are the unique characteristics of each data set:\n\n| | Offense-Column        | Offense-Value          | Reported-Column  | Reported-Data Type |\n|-|-----------------------|------------------------|-----------------------------------|\n| New York | `offenseDescription`  | starts with \"murder\" or \"homicide\" | `reportDate`     | `timestamp`    |\n| Boston | `OFFENSE_CODE_GROUP`  | \"Homicide\"             | `MONTH`          | `integer`      |\n\nFor the upcoming aggregation, you need to alter the New York data set to include a `month` column which can be computed from the `reportDate` column using the `month()` function. Boston already has this column.\n\nIn this example, we use several functions in the `pyspark.sql.functions` library, and need to import:\n\n* `month()` to extract the month from `reportDate` timestamp data type.\n* `lower()` to convert text to lowercase.\n* `contains(mySubstr)` to indicate a string contains substring `mySubstr`.\n\nAlso, note we use  `|`  to indicate a logical `or` of two conditions in the `filter` method."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import lower, upper, month, col\n\nhomicidesNewYorkDF = (crimeDataNewYorkDF \n  .select(month(col(\"reportDate\")).alias(\"month\"), col(\"offenseDescription\").alias(\"offense\")) \n  .filter(lower(col(\"offenseDescription\")).contains(\"murder\") | lower(col(\"offenseDescription\")).contains(\"homicide\"))\n)\n\ndisplay(homicidesNewYorkDF)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Notice how the same kind of information is presented differently in the Boston data:\n\n`offense` is called `OFFENSE_CODE_GROUP` and there is only one category `homicide`."],"metadata":{}},{"cell_type":"code","source":["homicidesBostonDF = (crimeDataBostonDF \n  .select(\"month\", col(\"OFFENSE_CODE_GROUP\").alias(\"offense\")) \n  .filter(lower(col(\"OFFENSE_CODE_GROUP\")).contains(\"homicide\"))\n)\n\ndisplay(homicidesBostonDF)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["See below the structure of the two tables is now identical."],"metadata":{}},{"cell_type":"code","source":["display(homicidesNewYorkDF.limit(5))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["display(homicidesBostonDF.limit(5))"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["## Analyzing the Data"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nNow that you normalized the homicide data for each city, combine the two by taking their union.\n\nWhen done, aggregate that data to compute the number of homicides per month.\n\nStart by creating a new DataFrame called `homicidesBostonAndNewYorkDF` that consists of the `union` of `homicidesNewYorkDF` with `homicidesBostonDF`."],"metadata":{}},{"cell_type":"code","source":["homicidesBostonAndNewYorkDF = homicidesNewYorkDF.union(homicidesBostonDF)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["See all the data in one table below:"],"metadata":{}},{"cell_type":"code","source":["display(homicidesBostonAndNewYorkDF.orderBy(\"month\"))"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["And finally, perform a simple aggregation to see the number of homicides per month:"],"metadata":{}},{"cell_type":"code","source":["display(homicidesBostonAndNewYorkDF.select(\"month\").orderBy(\"month\").groupBy(\"month\").count())"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["## Exercise 1\n\nMerge the crime data for Chicago with the data for New York and Boston, and then update our final aggregation of counts-by-month."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1\n\nCreate the initial DataFrame of the Chicago data.\n0. The source file is `dbfs:/mnt/adls/training/crime-data-2016/Crime-Data-Chicago-2016.parquet`.\n0. Name the view `crimeDataChicagoDF`.\n0. View the data by invoking the `show()` method."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\ncrimeDataChicagoDF = spark.read.parquet(\"/mnt/adls/training/crime-data-2016/Crime-Data-Chicago-2016.parquet\")\ndisplay(crimeDataChicagoDF)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\ntotal = crimeDataChicagoDF.count()\n\ndbTest(\"DF-L6-crimeDataChicago-count\", 267872, total)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["-sandbox\n### Step 2\n\nCreate a new view that normalizes the data structure.\n0. Name the DataFrame `homicidesChicagoDF`.\n0. The DataFrame should have at least two columns: `month` and `offense`.\n0. Filter the data to include only homicides.\n0. View the data by invoking the `show()` method.\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** Use the `month()` function to extract the month-of-the-year.\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** To find out which values for each offense constitutes a homicide, produce a distinct list of values from the`crimeDataChicagoDF` DataFrame."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n\ndisplay(crimeDataChicagoDF.select(\"primaryType\").distinct().orderBy(\"primaryType\"))"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["# ANSWER\n\nhomicidesChicagoDF = (crimeDataChicagoDF \n  .select(month(col(\"date\")).alias(\"month\"), col(\"primaryType\").alias(\"offense\")) \n  .filter(lower(col(\"primaryType\")) == \"homicide\")\n)\n\ndisplay(homicidesChicagoDF)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nhomicidesChicago = homicidesChicagoDF.select(\"month\").groupBy(\"month\").count().orderBy(\"month\").collect()\n\ndbTest(\"DF-L6-homicideChicago-len\", 12, len(homicidesChicago))\ndbTest(\"DF-L6-homicideChicago-0\", 54, homicidesChicago[0][1])\ndbTest(\"DF-L6-homicideChicago-6\", 71, homicidesChicago[6][1])\ndbTest(\"DF-L6-homicideChicago-11\", 58, homicidesChicago[11][1])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["-sandbox\n### Step 3\n\nCreate a new DataFrame that merges all three data sets (New York, Boston, Chicago):\n0. Name the view `allHomicidesDF`.\n0. Use the `union()` method introduced earlier to merge all three tables.\n  * `homicidesNewYorkDF`\n  * `homicidesBostonDF`\n  * `homicidesChicagoDF`\n0. View the data by invoking the `show()` method.\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** To union three tables together, copy the previous example and apply a `union()` method again."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nallHomicidesDF = homicidesNewYorkDF.union(homicidesBostonDF).union(homicidesChicagoDF)\ndisplay(allHomicidesDF)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\n\nallHomicides = allHomicidesDF.count()\ndbTest(\"DF-L6-allHomicides-count\", 1203, allHomicides)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["### Step 4\n\nCreate a new DataFrame that counts the number of homicides per month.\n0. Name the DataFrame `homicidesByMonthDF`.\n0. Rename the column `count(1)` to `homicides`.\n0. Group the data by `month`.\n0. Sort the data by `month`.\n0. Count the number of records for each aggregate.\n0. View the data by invoking the `show()` method."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import count\nhomicidesByMonthDF = allHomicidesDF.select(\"month\").groupBy(\"month\").agg(count('*').alias(\"homicides\")).orderBy(\"month\")\ndisplay(homicidesByMonthDF)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nallHomicides = homicidesByMonthDF.collect()\n\ndbTest(\"DF-L6-homicidesByMonth-len\", 12, len(allHomicides))\ndbTest(\"DF-L6-homicidesByMonth-0\", 1, allHomicides[0][0])\ndbTest(\"DF-L6-homicidesByMonth-11\", 12, allHomicides[11][0])\ndbTest(\"DF-L6-allHomicides-0\", 83, allHomicides[0][1])\ndbTest(\"DF-L6-allHomicides-1\", 83, allHomicides[0][1])\ndbTest(\"DF-L6-allHomicides-2\", 68, allHomicides[1][1])\ndbTest(\"DF-L6-allHomicides-3\", 72, allHomicides[2][1])\ndbTest(\"DF-L6-allHomicides-4\", 76, allHomicides[3][1])\ndbTest(\"DF-L6-allHomicides-5\", 105, allHomicides[4][1])\ndbTest(\"DF-L6-allHomicides-6\", 120, allHomicides[5][1])\ndbTest(\"DF-L6-allHomicides-7\", 116, allHomicides[6][1])\ndbTest(\"DF-L6-allHomicides-8\", 144, allHomicides[7][1])\ndbTest(\"DF-L6-allHomicides-9\", 109, allHomicides[8][1])\ndbTest(\"DF-L6-allHomicides-10\", 109, allHomicides[9][1])\ndbTest(\"DF-L6-allHomicides-11\", 111, allHomicides[10][1])\ndbTest(\"DF-L6-allHomicides-12\", 90, allHomicides[11][1])\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["## Unmount ADLS from DBFS"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.unmount(\"/mnt/adls\")"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["## Summary\n\n* Spark DataFrames allow you to easily manipulate data in a Data Lake."],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n**Q:** What is a Data Lake?  \n**A:** A Data Lake is a collection of data files gathered from various sources.  Spark loads each file as a table and then executes queries by joining and aggregating these files.\n\n**Q:** What are advantages of Data Lakes over classic Data Warehouses?  \n**A:** Data Lakes allow for large amounts of data to be aggregated from many sources with minimal preparatory steps.  Data Lakes also allow for very large files.  Powerful query engines such as Spark can read the diverse collection of files and execute complex queries efficiently.\n\n**Q:** What are some advantages of Data Warehouses?  \n**A:** Data warehouses are neatly curated to ensure data from all sources fit a common schema.  This makes them easy to query.\n\n**Q:** What's the best way to combine the advantages of Data Lakes and Data Warehouses?  \n**A:** Start with a Data Lake.  As you query, you will discover cases where the data needs to be cleaned, combined, and made more accessible.  Create periodic Spark jobs to read these raw sources and write new \"golden\" DataFrames that are cleaned and more easily queried."],"metadata":{}}],"metadata":{"name":"06-Data-Lakes","notebookId":291050441001602},"nbformat":4,"nbformat_minor":0}
