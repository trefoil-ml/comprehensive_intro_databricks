{"cells":[{"cell_type":"markdown","source":["# SQL Database - Connect using Key Vault\n\nIn the previous lession ([Key Vault-backed Secret Scopes]($./08-Key-Vault-backed-secret-scopes)), you created a Key Vault-backed secret scope for Azure Databricks, and securely stored your Azure SQL username and password within.\n\nIn this lesson, you will use the Azure SQL Database secrets that are securely stored within the Key Vault-backed secret scope to connect to your Azure SQL Database instance, using the JDBC drivers that come with Databricks Runtime version 3.4 and above. Next, you will use the DataFrame API to execute SQL queries and control the parallelism of reads through the JDBC interface.\n\nYou will be using a new data set, as the customer has decided to make their e-commerce SQL data available for querying and processing within Azure Databricks. You will prove that you can successfully read data from and write data to the database using the JDBC driver."],"metadata":{}},{"cell_type":"markdown","source":["## Access Key Vault secrets and configure JDBC connection\n\nIn the previous lesson, you created two secrets in Key Vault for your Azure SQL Database instance: **sql-username** and **sql-password**. You should have also made note of the Azure SQL Server host name and Database name. If you do not have this information, take a moment to retrieve those details from the Azure portal."],"metadata":{}},{"cell_type":"code","source":["%scala\nval jdbcUsername = dbutils.secrets.get(scope = \"key-vault-secrets\", key = \"sql-username\")\nval jdbcPassword = dbutils.secrets.get(scope = \"key-vault-secrets\", key = \"sql-password\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Notice that the values of `jdbcUsername` and `jdbcPassword` when printed out are `[REDACTED]`. This is to prevent your secrets from being exposed.\n\nThe next step is to ensure the JDBC driver is available."],"metadata":{}},{"cell_type":"code","source":["%scala\nClass.forName(\"com.microsoft.sqlserver.jdbc.SQLServerDriver\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Now create a connection to the Azure SQL Database using the username and password from Key Vault, as well as the SQL Server host name and database name.\n\n> The **host name** value will be in the following format: `<YOUR_SERVER_NAME>.databrickssqlserver.database.windows.net`."],"metadata":{}},{"cell_type":"code","source":["# Create input widgets to store the host name and database values. This will allow us to access those same values from cells that use different languages.\n# Execute this cell to display the widgets on top of the page, then fill the information before continuing to the next cell.\ndbutils.widgets.text(\"hostName\", \"\", \"SQL Server Host Name\")\ndbutils.widgets.text(\"database\", \"\", \"Database Name\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%scala\nval jdbcHostname = dbutils.widgets.get(\"hostName\")\nval jdbcPort = 1433\nval jdbcDatabase = dbutils.widgets.get(\"database\")\n\n// Create the JDBC URL without passing in the user and password parameters.\nval jdbcUrl = s\"jdbc:sqlserver://${jdbcHostname}:${jdbcPort};database=${jdbcDatabase}\"\n\n// Create a Properties() object to hold the parameters.\nimport java.util.Properties\nval connectionProperties = new Properties()\n\nconnectionProperties.put(\"user\", s\"${jdbcUsername}\")\nconnectionProperties.put(\"password\", s\"${jdbcPassword}\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Make sure you can connect to the Azure SQL Database, using the JDBC driver."],"metadata":{}},{"cell_type":"code","source":["%scala\nval driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\nconnectionProperties.setProperty(\"Driver\", driverClass)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["If there are no errors, then the connection was successful."],"metadata":{}},{"cell_type":"markdown","source":["## Read data from JDBC\n\nTo start, we'll use a single JDBC connection to pull an Azure SQL Database table into the Spark environment. Your queries will be executed through the DataFrame API, providing a consistent experience whether you are querying flat files, Databricks tables, Azure Cosmos DB, SQL Server, or any other of the number of data sources you can access from Databricks."],"metadata":{}},{"cell_type":"code","source":["%scala\nval product_table = spark.read.jdbc(jdbcUrl, \"SalesLT.Product\", connectionProperties)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Spark automatically reads the schema from the database table and maps its types back to Spark SQL types. You can see the schema using the `printSchema` command:"],"metadata":{}},{"cell_type":"code","source":["%scala\nproduct_table.printSchema"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Now that we have our DataFrame, we can run queries against the JDBC table. Below, we are calculating the average list price by product category."],"metadata":{}},{"cell_type":"code","source":["%scala\ndisplay(product_table.select(\"ProductCategoryID\", \"ListPrice\")\n        .groupBy(\"ProductCategoryID\")\n        .avg(\"ListPrice\"))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["It is possible to save the DataFrame out to a temporary view. This opens opportunities, such as querying using the SQL syntax, as you will see a few cells below."],"metadata":{}},{"cell_type":"code","source":["%scala\nproduct_table.createOrReplaceTempView(\"Product\")\ndisplay(spark.sql(\"SELECT * FROM Product\"))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Use the `%sql` magic to change the language of a cell to SQL. In the cell below, execute a SQL query that performs the same average `ListPrice` and product category grouping that was done using the DataFrame earlier."],"metadata":{}},{"cell_type":"code","source":["%sql\nselect ProductCategoryID, AVG(ListPrice) from Product\nGroup By ProductCategoryID"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["You can also use the JDBC driver using Python. The cell below establishes a JDBC connection to the Azure SQL Database just as was done earlier using Scala.\n\nNotice that you must define new parameters for the host name, database, username, and password. This is because those variables were defined using a different language earlier, which are inaccessible by a different language engine."],"metadata":{}},{"cell_type":"code","source":["jdbcHostname = dbutils.widgets.get(\"hostName\")\njdbcDatabase = dbutils.widgets.get(\"database\")\njdbcPort = 1433\n\n# Retrieve the database username and password from Key Vault\njdbcUsername = dbutils.secrets.get(scope = \"key-vault-secrets\", key = \"sql-username\")\njdbcPassword = dbutils.secrets.get(scope = \"key-vault-secrets\", key = \"sql-password\")\n\njdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\nconnectionProperties = {\n  \"user\" : jdbcUsername,\n  \"password\" : jdbcPassword,\n  \"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n}"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Now that the JDBC connection has been created in Python, let's query the JDBC connections across multiple workers. You do this by setting the `numPartitions` property. This determines how many connections used to push data through the JDBC API."],"metadata":{}},{"cell_type":"code","source":["df = spark.read.jdbc(url=jdbcUrl, table='SalesLT.SalesOrderDetail', properties=connectionProperties, column='SalesOrderDetailID', lowerBound=1, upperBound=100000, numPartitions=100)\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["## Challenge\n\nWrite new records to the `SalesLT.ProductModel` Azure SQL Database table.\n\nThe first step is to return a count of records in the `SalesLT.ProductModel` table. We've provided the code to do this as a pushdown query (where you push down an entire query to the database and return just the result) below."],"metadata":{}},{"cell_type":"code","source":["productmodel_count_query = \"(select COUNT(*) count from SalesLT.ProductModel) count\"\nproductmodel_count = spark.read.jdbc(url=jdbcUrl, table=productmodel_count_query, properties=connectionProperties)\ndisplay(productmodel_count)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["The second step is to define a schema that matches the data types in SQL. Since the primary key is an automatically generated identity column, you must exclude it from your schema since you will not be setting the value.\n\n**Note:** You can get the schema with data types by retrieving the first row into a new DataFrame and printing the schema:\n\n```python\nproductmodel_df = spark.read.jdbc(url=jdbcUrl, table=\"SalesLT.ProductModel\", properties=connectionProperties).limit(1)\nproductmodel_df.printSchema\n```"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nschema = StructType([\n  StructField(\"Name\", StringType(), True),\n  StructField(\"CatalogDescription\", StringType(), True),\n  StructField(\"rowguid\", StringType(), True),\n  StructField(\"ModifiedDate\", TimestampType(), True)\n])"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Now, write new rows to the table, applying the schema from the `productModel_df` DataFrame."],"metadata":{}},{"cell_type":"code","source":["# Import libraries to generate UUIDs (GUIDs), get the current date/time, set the save mode, and to use the Row object\nimport uuid\nimport datetime\nfrom pyspark.sql import Row\nnewRows = [\n  Row(\"Biking Shorts\",\"They make you ride faster and look awesome doing it!\", str(uuid.uuid4()), datetime.datetime.now()),\n  Row(\"Biking Shirt\",\"Glide through the atmosphere like Phoenix rising from the ashes\", str(uuid.uuid4()), datetime.datetime.now())\n]\nparallelizeRows = spark.sparkContext.parallelize(newRows)\nnew_df = spark.createDataFrame(parallelizeRows, schema)\n\n# TODO\n\n# Add code here to write to the SalesLT.ProductModel table\n\nnew_df.write.jdbc(url=jdbcUrl, table=\"SalesLT.ProductModel\", mode=\"append\", properties=connectionProperties)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Do a final count on the `SalesLT.ProductModel` table to make sure the two new rows were inserted. You can also use the Query Editor in the Azure SQL Database portal interface to view your new rows there."],"metadata":{}},{"cell_type":"code","source":["# TODO\n\n# Get a count of rows once more. The value should be 130\nproductmodel_count = spark.read.jdbc(url=jdbcUrl, table=productmodel_count_query, properties=connectionProperties)\ndisplay(productmodel_count)"],"metadata":{},"outputs":[],"execution_count":33}],"metadata":{"name":"09-SQL-Database-Connect-Using-Key-Vault","notebookId":291050441001349},"nbformat":4,"nbformat_minor":0}
