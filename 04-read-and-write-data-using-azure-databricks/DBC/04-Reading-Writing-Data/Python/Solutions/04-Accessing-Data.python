{"version":"NotebookV1","origId":1745794912677588,"name":"04-Accessing-Data","language":"python","commands":[{"version":"CommandV1","origId":1745794912677589,"guid":"ea0d863d-c4a9-4f33-8d73-9e1ec14fb6e6","subtype":"command","commandType":"auto","position":2.0,"command":"%md-sandbox\n# Accessing Data\n\nApache Spark&trade; and Azure Databricks&reg; provide numerous ways to access your data.\n\n<h2 style=\"color:red\">WARNING!</h2> This notebook must be run using Databricks runtime 4.0 or better.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":null,"commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"ee66dbcb-385a-431f-b9d8-8c04b9c080c2"},{"version":"CommandV1","origId":1745794912677590,"guid":"0da29449-7b03-40e8-bc81-ca33ec36bf05","subtype":"command","commandType":"auto","position":3.0,"command":"%md\n### Getting Started\n\nRun the following cell to configure our \"classroom.\"","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"bc645318-83ce-453b-89b6-419048da4ac7"},{"version":"CommandV1","origId":1745794912677591,"guid":"5958b50f-3fa5-40a5-8fb9-77fe9748c289","subtype":"command","commandType":"auto","position":4.0,"command":"%run \"./Includes/Classroom-Setup\"","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"62743d33-ff3e-4caa-a8f5-8b8d0e9e926c"},{"version":"CommandV1","origId":1745794912677592,"guid":"24e312eb-20ab-4502-af74-cc0a9707df54","subtype":"command","commandType":"auto","position":6.0,"command":"%md\n### Create a DataFrame From an Existing File\n\nThe <a href=\"https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html\" target=\"_blank\">Databricks File System</a> (DBFS) is the built-in, Azure-blob-backed, alternative to the <a href=\"http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html\" target=\"_blank\">Hadoop Distributed File System</a> (HDFS).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"55c709a1-3735-4e16-a273-faa7b0ee510b"},{"version":"CommandV1","origId":1745794912677593,"guid":"7abba652-f72a-4075-aff0-ddde2bd8100e","subtype":"command","commandType":"auto","position":7.0,"command":"%md-sandbox\nThe example below creates a DataFrame from the **ip-geocode.parquet** file (if it doesn't exist).\n\nFor Parquet files, you need to specify only one option: the path to the file.\n\nA Parquet \"file\" is actually a collection of files stored in a single directory.  The Parquet format offers features that make it the ideal choice for storing \"big data\" on distributed file systems. \n\nFor more information, see <a href=\"https://parquet.apache.org/\" target=\"_blank\">Apache Parquet</a>.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"4b89741d-61bf-4888-83b5-e50eead736cc"},{"version":"CommandV1","origId":1745794912677594,"guid":"5b245a90-6951-40e0-990c-097f04b6153b","subtype":"command","commandType":"auto","position":9.0,"command":"ipGeocodeDF = spark.read.parquet(\"/mnt/training/ip-geocode.parquet\")","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"4ecafaf3-a43e-4f09-96b3-ce54abbeb031"},{"version":"CommandV1","origId":1745794912677595,"guid":"efa99320-41a9-4ef1-b9a9-5fc4d002f9de","subtype":"command","commandType":"auto","position":10.0,"command":"%md\nNow the DataFrame has been created, see its schema by invoking the `printSchema` method.\n\nNote the data types are known ahead of time (this is a property of the parquet file format) and \nthat `nullable` is set to `true`.\n\nThis treats all missing values as `NULLs`.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"e86446ef-23ad-42e9-933c-48a946f929f3"},{"version":"CommandV1","origId":1745794912677596,"guid":"1ec7342f-7888-4d98-8d62-3eded67bba76","subtype":"command","commandType":"auto","position":11.0,"command":"ipGeocodeDF.printSchema()","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"0e5bac83-6178-473f-b73e-1daf62c380e0"},{"version":"CommandV1","origId":1745794912677597,"guid":"4d204179-1d08-4e90-831c-2ed2da58ffc3","subtype":"command","commandType":"auto","position":12.0,"command":"%md\n### File Formats Other than Parquet\n\nYou can create DataFrames from file other formats. \n\nOne common format is comma-separated-values (CSV), for which you specify:\n* The file's delimiter; the default is \"**,**\".\n* Whether the file has a header or not; the default is **false**.\n* Whether or not to infer the schema; the default is **false**.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"8e401aed-4275-4cf1-8f03-2fa07baae540"},{"version":"CommandV1","origId":1745794912677598,"guid":"33f706e1-3fa2-4922-a3d7-9204aa4e6968","subtype":"command","commandType":"auto","position":14.0,"command":"%md\nIn order to know which options to use, look at the first couple of lines of the file.\n\nTake a look at the head of the file **/mnt/training/bikeSharing/data-001/day.csv.**","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"2a17e821-cd1a-4815-aff6-a83390c2ba43"},{"version":"CommandV1","origId":1745794912677599,"guid":"fde7f090-2be4-4654-bfcc-8f22af1af9f6","subtype":"command","commandType":"auto","position":15.0,"command":"%fs head /mnt/training/bikeSharing/data-001/day.csv --maxBytes=492","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"a69ed292-31d2-424b-91ea-221e9f5f7682"},{"version":"CommandV1","origId":1745794912677600,"guid":"bd55a8d4-f952-4480-99f7-c39fc550beee","subtype":"command","commandType":"auto","position":16.0,"command":"%md\nLet's create a DataFrame from the CSV file described above.\n\nAs you can see above:\n* There is a header.\n* The file is comma separated (the default).\n* Let Spark infer the schema.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"cc1ca646-e750-4c87-9888-f6fbf2837459"},{"version":"CommandV1","origId":1745794912677601,"guid":"2f9511d8-ee1e-4a28-a720-1f510960f37b","subtype":"command","commandType":"auto","position":17.0,"command":"bikeSharingDayDF = (spark\n  .read                                                # Call the read method returning a DataFrame\n  .option(\"inferSchema\",\"true\")                        # Option to tell Spark to infer the schema\n  .option(\"header\",\"true\")                             # Option telling Spark that the file has a header\n  .csv(\"/mnt/training/bikeSharing/data-001/day.csv\"))  # Option telling Spark where the file is","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"a68a17f3-0ee9-4216-8545-ef4795d94014"},{"version":"CommandV1","origId":1745794912677602,"guid":"37ca47a2-aad4-4f56-841e-14471fcf37a8","subtype":"command","commandType":"auto","position":18.0,"command":"%md\nNow the DataFrame is created, view its contents by invoking the `show` method.\n\nBy default, `show()` (without any parameters) prints the first 20 rows. \n\nPrint the top `n` rows by invoking `show(n)`","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"2b5e4968-22ac-45a6-a06d-00859825ba49"},{"version":"CommandV1","origId":1745794912677603,"guid":"88d5bdd2-afe6-4345-906d-44c08a2f0c36","subtype":"command","commandType":"auto","position":19.0,"command":"bikeSharingDayDF.show(10)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"7e9c56f9-4514-4a4d-b69a-b86804a1bace"},{"version":"CommandV1","origId":1745794912677604,"guid":"e358d1fb-6ce9-449c-85f2-01ffb2d5849a","subtype":"command","commandType":"auto","position":20.0,"command":"%md\nAlternatively, invoke the `display` function to show the same table in html format.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"bd62cad4-6bfb-4efc-9326-eb6c457ef91c"},{"version":"CommandV1","origId":1745794912677605,"guid":"2a7c9962-f06f-4be1-b1d0-216eac52beed","subtype":"command","commandType":"auto","position":21.0,"command":"display(bikeSharingDayDF)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"c2e2026e-0d12-444a-9b80-d115bd303f7d"},{"version":"CommandV1","origId":1745794912677606,"guid":"4e06db6a-be76-4934-af9e-9d7cd438a5f9","subtype":"command","commandType":"auto","position":22.0,"command":"%md\n### Upload a Local File as a Table\n\nThe last two examples use files already loaded on the \"server.\"\n\nYou can also create DataFrames by uploading files. The files are nominally stored as tables, from which you create DataFrames.\n\nDownload the following file to your local machine: <a href=\"https://s3-us-west-2.amazonaws.com/databricks-corp-training/common/dataframes/state-income.csv\">state-income.csv</a>","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"41d8169e-595b-46c3-9fbe-06120e24fa43"},{"version":"CommandV1","origId":1745794912677607,"guid":"8c63d436-91f2-466e-86fa-9f74f490f71c","subtype":"command","commandType":"auto","position":24.0,"command":"%md-sandbox\n\n1. Select **Data** from the sidebar, and click the **databricks** database\n2. Select the **+** icon to create a new table\n\n<img src=\"https://files.training.databricks.com/images/eLearning/DataFrames-MSFT/create-table-1-databricks-db.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; width: auto; height: auto; max-height: 383px\"/>\n\n<br>\n1. Select **Upload File**\n2. click on Browse and select the **state-income.csv** file from your machine, or drag-and-drop the file to initiate the upload\n\n<img src=\"https://files.training.databricks.com/images/eLearning/DataFrames-MSFT/create-table-2.png\" style=\"border: 1px solid #aaa; border-radius: 5px 5px 5px 5px; width: auto; height: auto; max-height: 300px  \"/>","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"d0b11d78-2b19-47a7-a5a6-3b79bd91c535"},{"version":"CommandV1","origId":1745794912677608,"guid":"8e940993-0852-4e39-a83e-83a46ca53893","subtype":"command","commandType":"auto","position":25.0,"command":"%md-sandbox\nOnce Databricks finishes processing the file, you'll see another table preview.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Databricks tries to choose a table name that doesn't clash with tables created by other users. However, a name clash is still possible. If the table already exists, you'll see an error like the following:\n\n<img src=\"https://files.training.databricks.com/images/eLearning/create-table-7.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; margin-top: 20px; padding: 10px\"/>\n\nIf that happens, type in a different table name, and try again.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"30211923-a48c-4e0b-af6f-e1d29dbd13c8"},{"version":"CommandV1","origId":1745794912677609,"guid":"c81649a7-f339-43c9-80c2-274571f3f2fb","subtype":"command","commandType":"auto","position":26.0,"command":"%md\nAccess the file via the path `/FileStore/tables/state_income-9f7c5.csv`","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"81ed0d22-857e-4216-8b1c-edbcd76bf1d9"},{"version":"CommandV1","origId":1745794912677610,"guid":"ceda11dc-835a-4758-9fa2-8a386a012565","subtype":"command","commandType":"auto","position":27.0,"command":"stateIncomeDF = (spark\n  .read                                                # Call the read method returning a DataFrame\n  .option(\"inferSchema\",\"true\")                        # Option to tell Spark to infer the schema\n  .option(\"header\",\"true\")                             # Option telling Spark that the file has a header\n  .csv(\"/FileStore/tables/state_income-9f7c5.csv\"))    # Option telling Spark where the file is","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"789f0bfb-e743-47e6-821f-d948ce4584a6"},{"version":"CommandV1","origId":1745794912677611,"guid":"86050d5b-aff7-4ff2-90ef-19be91b319de","subtype":"command","commandType":"auto","position":28.0,"command":"%md\nView the first 10 lines of its contents.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"88d53e5d-0238-4380-8245-a07ead539dc3"},{"version":"CommandV1","origId":1745794912677612,"guid":"5f5eb796-bd51-479a-b9b1-e0442bee36cb","subtype":"command","commandType":"auto","position":29.0,"command":"stateIncomeDF.show(10)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"cd3783a7-fd95-4b83-bc6f-87a94d7b5a41"},{"version":"CommandV1","origId":1745794912677613,"guid":"12d9ee8e-5910-4c96-a193-3857d2ad4769","subtype":"command","commandType":"auto","position":30.0,"command":"%md-sandbox\n### How to Mount an Azure Blob to DBFS\n\nMicrosoft Azure provides cloud file storage in the form of the Blob Store.  Files are stored in \"blobs.\"\nIf you have an Azure account, create a blob, store data files in that blob, and mount the blob as a DBFS directory. \n\nOnce the blob is mounted as a DBFS directory, access it without exposing your Azure Blob Store keys.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"edcc91ef-b424-411c-9674-55a0eeaaf662"},{"version":"CommandV1","origId":1745794912677614,"guid":"61876aeb-5be5-457b-9434-8a04e798e15b","subtype":"command","commandType":"auto","position":32.0,"command":"%md\nTake a look at the blobs already mounted to your DBFS:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"2c8a9895-a825-49fc-9e7c-9d9aa8088a6c"},{"version":"CommandV1","origId":1745794912677615,"guid":"68404ac9-f7b3-4329-aaca-9a1480963a34","subtype":"command","commandType":"auto","position":33.0,"command":"%fs mounts","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"f5900ae2-5a7c-4699-ac4e-9bd0159af195"},{"version":"CommandV1","origId":1745794912677616,"guid":"0f0342c7-7d72-4ca5-8bd7-d2157b154335","subtype":"command","commandType":"auto","position":34.0,"command":"%md-sandbox\nMount a Databricks Azure blob (using read-only access and secret key pair), access one of the files in the blob as a DBFS path, then unmount the blob.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> The mount point **must** start with `/mnt/`.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"bf9748b8-a2f6-4a07-aced-8db18c909360"},{"version":"CommandV1","origId":1745794912677617,"guid":"2652354b-9bd4-4684-bf35-f5ecf6989739","subtype":"command","commandType":"auto","position":35.0,"command":"%md-sandbox\n\n### Creating a Shared Access Signature (SAS) URL\nAzure provides you with a secure way to create and share access keys for your Azure Blob Store without compromising your account keys.\n\nMore details are provided <a href=\"http://docs.microsoft.com/en-us/azure/storage/common/storage-dotnet-shared-access-signature-part-1\" target=\"_blank\"> in this document</a>.\n\nThis allows access to your Azure Blob Store data directly from Databricks distributed file system (DBFS).\n\nAs shown in the screen shot, in the Azure Portal, go to the storage account containing the blob to be mounted. Then:\n\n1. Select Shared access signature from the menu.\n2. Click the Generate SAS button.\n3. Copy the entire Blog service SAS URL to the clipboard.\n4. Use the URL in the mount operation, as shown below.\n\n<img src=\"https://files.training.databricks.com/images/eLearning/DataFrames-MSFT/create-sas-keys.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; margin-top: 20px; padding: 10px\"/>","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"d306020b-5918-4559-b0a8-50bbe374ed1d"},{"version":"CommandV1","origId":1745794912677618,"guid":"74905132-dbc3-4674-848d-3e79d7f2d358","subtype":"command","commandType":"auto","position":36.0,"command":"%md-sandbox\nCreate the mount point with `dbutils.fs.mount(source = .., mountPoint = .., extraConfigs = ..)`.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> If the directory is already mounted, you receive the following error:\n\n> Directory already mounted: /mnt/temp-training\n\nIn this case, use a different mount point such as `temp-training-2`, and ensure you update all three references below.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"7ddf013c-8d4f-4942-9f4f-54b3dfdeefb4"},{"version":"CommandV1","origId":1745794912677619,"guid":"88f226e9-6615-459a-87f5-4a320adf1186","subtype":"command","commandType":"auto","position":37.0,"command":"SasURL = \"https://dbtraineastus2.blob.core.windows.net/?sv=2017-07-29&ss=b&srt=sco&sp=rl&se=2023-04-19T06:32:30Z&st=2018-04-18T22:32:30Z&spr=https&sig=BB%2FQzc0XHAH%2FarDQhKcpu49feb7llv3ZjnfViuI9IWo%3D\"\nindQuestionMark = SasURL.index('?')\nSasKey = SasURL[indQuestionMark:len(SasURL)]\nStorageAccount = \"dbtraineastus2\"\nContainerName = \"training\"\nMountPoint = \"/mnt/temp-training\"\n\ndbutils.fs.mount(\n  source = \"wasbs://%s@%s.blob.core.windows.net/\" % (ContainerName, StorageAccount),\n  mount_point = MountPoint,\n  extra_configs = {\"fs.azure.sas.%s.%s.blob.core.windows.net\" % (ContainerName, StorageAccount) : \"%s\" % SasKey}\n)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"8ef0694f-d22a-4724-9a5d-4e02fa22bcea"},{"version":"CommandV1","origId":1745794912677620,"guid":"2581bfb4-d871-4659-978f-f063b4863b33","subtype":"command","commandType":"auto","position":38.0,"command":"%fs mounts","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"9ad6e17c-f0d5-465e-9d7c-cdcf9a29312f"},{"version":"CommandV1","origId":1745794912677621,"guid":"c3d344f5-e1b1-4213-ab7f-ea33168ad851","subtype":"command","commandType":"auto","position":39.0,"command":"%md\nList the contents of a subdirectory in directory you just mounted:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"87b56903-14fe-475e-a1da-333418c2829c"},{"version":"CommandV1","origId":1745794912677622,"guid":"ab4594c8-a294-4d64-9642-fe4c553855cd","subtype":"command","commandType":"auto","position":40.0,"command":"%fs ls /mnt/temp-training","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"91cca391-1474-44d3-a7fe-3bdf3b6fa86f"},{"version":"CommandV1","origId":1745794912677623,"guid":"22635915-8768-4fa7-869c-0cdbd4d520cc","subtype":"command","commandType":"auto","position":41.0,"command":"%md\nTake a peek at the head of the file `auto-mpg.csv`:","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"49f75219-39bb-4f79-b85f-39a49adb3145"},{"version":"CommandV1","origId":1745794912677624,"guid":"fc6e4d7f-3053-4c46-9b71-52dfb3ba441d","subtype":"command","commandType":"auto","position":42.0,"command":"%fs head /mnt/temp-training/auto-mpg.csv","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"645cdcb4-e980-4e73-907f-9447c1a112cf"},{"version":"CommandV1","origId":1745794912677625,"guid":"71849114-11be-41d9-99de-4605ff676a28","subtype":"command","commandType":"auto","position":43.0,"command":"%md\nNow you are done, unmount the directory.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"d3242d5a-f69b-47f4-8828-15dc15159814"},{"version":"CommandV1","origId":1745794912677626,"guid":"f5188f34-ba20-4604-a7ef-f7cb91a0cd88","subtype":"command","commandType":"auto","position":44.0,"command":"%fs unmount /mnt/temp-training","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"1c24a5e7-9635-4033-b7d9-cb66f03da8ad"},{"version":"CommandV1","origId":1745794912677627,"guid":"ce63f505-d559-4a76-90bf-45bf7b174c4f","subtype":"command","commandType":"auto","position":45.0,"command":"%md\n## Summary\n\nDatabricks allows you to:\n  * Create DataFrames from existing data\n  * Create DataFrames from uploaded files\n  * Mount your own Azure blobs","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"24ccb95e-1973-4d49-a461-594b53cda00f"},{"version":"CommandV1","origId":1745794912677628,"guid":"8b457886-207d-429d-826d-eb1894437cdd","subtype":"command","commandType":"auto","position":46.0,"command":"%md\n## Review Questions\n**Q:** What is Azure Blob Store?  \n**A:** Blob Storage stores from hundreds to billions of objects such as unstructured dataâ€”images, videos, audio, documents easily and cost-effectively.\n\n**Q:** What is DBFS?  \n**A:** DBFS stands for Databricks File System.  DBFS provides for the cloud what the Hadoop File System (HDFS) provides for local spark deployments.  DBFS uses Azure Blob Store and makes it easy to access files by name.\n\n**Q:** Which is more efficient to query, a parquet file or a CSV file?  \n**A:** Parquet files are highly optimized binary formats for storing tables.  The overhead is less than required to parse a CSV file.  Parquet is the big data analogue to CSV as it is optimized, distributed, and more fault tolerant than CSV files.\n\n**Q:** What is the syntax for defining a DataFrame in Spark from an existing parquet file in DBFS?  \n**A:** Scala: \n\n`val IPGeocodeDF = spark.read.parquet(\"dbfs:/mnt/training/ip-geocode.parquet\")`\n\nPython: \n\n`IPGeocodeDF = spark.read.parquet(\"dbfs:/mnt/training/ip-geocode.parquet\")`\n\n**Q:** What is the syntax for defining a DataFrame in Spark from an existing CSV file in DBFS using the first row in the CSV as the schema? \n**A:** Scala: \n\n`val myDF = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"dbfs:/mnt/training/myfile.csv\")`\n\nPython: \n\n`myDF = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"dbfs:/mnt/training/myfile.csv\")`","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"cb77aaee-91ba-4d73-b1a4-5eaa1773b82c"},{"version":"CommandV1","origId":1745794912677629,"guid":"784379cf-006c-4cae-b468-274aa817e1ab","subtype":"command","commandType":"auto","position":47.0,"command":"%md\n## Next Steps\n\nStart the next lesson, [Querying JSON & Hierarchical Data with DataFrames]($./05-Querying-JSON).","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"97bd2f31-a3da-41ad-9e19-37c020510bd1"},{"version":"CommandV1","origId":1745794912677630,"guid":"2c874453-f1d9-4221-b12e-38c98929860a","subtype":"command","commandType":"auto","position":48.0,"command":"%md\n## Additional Topics & Resources\n\n* <a href=\"https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html\" target=\"_blank\">The Databricks DBFS File System</a>","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":0,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","latestUserId":"4258953226069350","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"datasetPreviewNameToCmdIdMap":{},"nuid":"c3689d0e-0101-42e7-8c1f-a1254699f632"}],"dashboards":[],"guid":"207b5991-2d8a-4a45-934f-6a862ab4bdff","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}}