{"cells":[{"cell_type":"markdown","source":["# Database Writes\n\nApache Spark&trade; and Azure Databricks&reg; allow you to write to a number of target databases in parallel, storing the transformed data from from your ETL job."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Database Writes in Spark\n\nWriting to a database in Spark differs from other tools largely due to its distributed nature. There are a number of variables that can be tweaked to optimize performance, largely relating to how data is organized on the cluster. Partitions are the first step in understanding performant database connections.\n\n**A partition is a portion of your total data set,** which is divided into many of these portions so Spark can distribute your work across a cluster. \n\nThe other concept needed to understand Spark's computation is a slot (also known as a core). **A slot/core is a resource available for the execution of computation in parallel.** In brief, a partition refers to the distribution of data while a slot refers to the distribution of computation.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-2/partitions-and-cores.png\" style=\"height: 400px; margin: 20px\"/></div>\n\nAs a general rule of thumb, the number of partitions should be a multiple of the number of cores. For instance, with 5 partitions and 8 slots, 3 of the slots will be underutilized. With 9 partitions and 8 slots, a job will take twice as long as it waits for the extra partition to finish."],"metadata":{}},{"cell_type":"markdown","source":["### Managing Partitions\n\nIn the context of JDBC database writes, **the number of partitions determine the number of connections used to push data through the JDBC API.** There are two ways to controll this parallelism:  \n\n| Function | Transformation Type | Use | Evenly distributes data across partitions? |\n| :----------------|:----------------|:----------------|:----------------| \n| `.coalesce(n)`   | narrow (does not shuffle data) | reduce the number of partitions | no |\n| `.repartition(n)`| wide (includes a shuffle operation) | increase the number of partitions | yes |"],"metadata":{}},{"cell_type":"markdown","source":["Run the cell below to mount the data."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Start by importing a DataFrame of Wikipedia pageviews."],"metadata":{}},{"cell_type":"code","source":["wikiDF = (spark.read\n  .parquet(\"/mnt/training/wikipedia/pageviews/pageviews_by_second.parquet\")\n)\ndisplay(wikiDF)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["View the number of partitions by changing the DataFrame into an RDD and using the `.getNumPartitions()` method.  \nSince the Parquet file was saved with 5 partitions, those paritions are retained when you import the data."],"metadata":{}},{"cell_type":"code","source":["partitions = wikiDF.rdd.getNumPartitions()\nprint(\"Partitions: {0:,}\".format( partitions ))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["To increase the number of partitions to 16, use `.repartition()`."],"metadata":{}},{"cell_type":"code","source":["repartitionedWikiDF = wikiDF.repartition(16)\nprint(\"Partitions: {0:,}\".format( repartitionedWikiDF.rdd.getNumPartitions() ))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["To reduce the number of partitions, use `.coalesce()`."],"metadata":{}},{"cell_type":"code","source":["coalescedWikiDF = repartitionedWikiDF.coalesce(2)\nprint(\"Partitions: {0:,}\".format( coalescedWikiDF.rdd.getNumPartitions() ))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["-sandbox\n### Configure Default Partitions\n\nSpark uses a default value of 200 partitions, which comes from real-world experience by Databricks engineers. This is an adjustable configuration setting. Run the following cell to see this value.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Get and set any number of different configuration settings in this manner. <a href=\"https://spark.apache.org/docs/latest/configuration.html\" target=\"_blank\">See the Spark documents</a> for details."],"metadata":{}},{"cell_type":"code","source":["spark.conf.get(\"spark.sql.shuffle.partitions\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Adjust the number of partitions with the following cell.  **This changes the number of partitions after a shuffle operation.**"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Now check to see how this changes an operation involving a data shuffle, such as an `.orderBy()`.  Recall that coalesced `coalescedWikiDF` to 2 partitions."],"metadata":{}},{"cell_type":"code","source":["orderByPartitions = coalescedWikiDF.orderBy(\"requests\").rdd.getNumPartitions()\nprint(\"Partitions: {0:,}\".format( orderByPartitions ))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["The `.orderBy()` triggered the repartition of the DataFrame into 8 partitions.  Now reset the default value."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### Parallel Database Writes\n\nDatabase writes are the inverse of what was covered in Lesson 4 of ETL Part 1.  In that lesson you defined the number of partitions in the call to the database.  \n\n**By contrast and when writing to a database, the number of active connections to the database is determined by the number of partitions of the DataFrame.**"],"metadata":{}},{"cell_type":"markdown","source":["Examine this by writing `wikiDF` to the `/tmp` directory.  Recall that `wikiDF` has 5 partitions."],"metadata":{}},{"cell_type":"code","source":["wikiDF.write.mode(\"OVERWRITE\").parquet(\"/tmp/wiki.parquet\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Examine the number of paritions in `/tmp/wiki.parquet`."],"metadata":{}},{"cell_type":"code","source":["%fs ls /tmp/wiki.parquet"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["This file has 5 parts, meaning Spark wrote the data through 5 different connections to this directory in the file system."],"metadata":{}},{"cell_type":"markdown","source":["### Examining in the Spark UI\n\nClick the arrow next to `Spark Jobs` under the following code cell in order to see a breakdown of the job you triggered. Click the next arrow to see a breakdown of the stages."],"metadata":{}},{"cell_type":"code","source":["wikiDF.repartition(12).write.mode(\"OVERWRITE\").parquet(\"/tmp/wiki.parquet\")"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["5 stages were initially triggered, one for each partition of our data.  When you repartitioned the DataFrame to 12 paritions, 12 stages were needed, one to write each partition of the data.  Run the following and observe how the repartitioning changes the number of stages."],"metadata":{}},{"cell_type":"code","source":["wikiDF.repartition(10).write.mode(\"OVERWRITE\").parquet(\"/tmp/wiki.parquet\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["For more details, click `View` to examine the more details about the operation in the Spark UI."],"metadata":{}},{"cell_type":"markdown","source":["### A Note on Upserts\n\nUpserts insert a record into a database if it doesn't already exist, and updates the existing record if it does.  **Upserts are not supported in core Spark.** You can only append or overwrite.  Databricks offers a data management system called Databricks Delta that does allow for upserts and other transactional functionality. [See the Databricks Delta docs for more information.](https://docs.azuredatabricks.net/delta/index.html#delta-guide)"],"metadata":{}},{"cell_type":"markdown","source":["## Exercise 1: Changing Paritions\n\nChange the number of partitions to prepare the optimal database write."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Import Helper Functions and Data\n\nA function is defined for you to print out the number of records in each DataFrame.  Run the following cell to define that function."],"metadata":{}},{"cell_type":"code","source":["def printRecordsPerPartition(df):\n  '''\n  Utility method to count & print the number of records in each partition\n  '''\n  print(\"Per-Partition Counts:\")\n  \n  def countInPartition(iterator): \n    yield __builtin__.sum(1 for _ in iterator)\n    \n  results = (df.rdd                   # Convert to an RDD\n    .mapPartitions(countInPartition)  # For each partition, count\n    .collect()                        # Return the counts to the driver\n  )\n\n  for result in results: \n    print(\"* \" + str(result))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["Import the data to sitting in `/mnt/training/wikipedia/pageviews/pageviews_by_second.parquet` to `wikiDF`."],"metadata":{}},{"cell_type":"code","source":["# TODO\nwikiDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ndbTest(\"ET2-P-06-01-01\", 7200000, wikiDF.count())\ndbTest(\"ET2-P-06-01-02\", ['timestamp', 'site', 'requests'], wikiDF.columns)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["Print the count of records by partition using `printRecordsPerPartition()`."],"metadata":{}},{"cell_type":"code","source":["printRecordsPerPartition(wikiDF)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["### Step 2: Repartition the Data\n\nDefine three new DataFrames:\n\n* `wikiDF1Partition`: `wikiDF` with 1 partition\n* `wikiDF16Partition`: `wikiDF` with 16 partitions\n* `wikiDF128Partition`: `wikiDF` with 128 partitions"],"metadata":{}},{"cell_type":"code","source":["# TODO\nwikiDF1Partition = # FILL_IN\nwikiDF16Partition = # FILL_IN\nwikiDF128Partition = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\n\ndbTest(\"ET2-P-06-02-01\", 1, wikiDF1Partition.rdd.getNumPartitions())\ndbTest(\"ET2-P-06-02-02\", 16, wikiDF16Partition.rdd.getNumPartitions())\ndbTest(\"ET2-P-06-02-03\", 128, wikiDF128Partition.rdd.getNumPartitions())\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["### Step 3: Examine the Distribution of Records\n\nUse `printRecordsPerPartition()` to examine the distribution of records across the partitions."],"metadata":{}},{"cell_type":"code","source":["# TODO\nprintRecordsPerPartition( <FILL_IN> )"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["### Step 4: Coalesce `wikiDF16Partition` and Examine the Results\n\nCoalesce `wikiDF16Partition` to `10` partitions, saving the result to `wikiDF16PartitionCoalesced`."],"metadata":{}},{"cell_type":"code","source":["# TODO\nwikiDF16PartitionCoalesced = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\n\ndbTest(\"ET2-P-06-03-01\", 10, wikiDF16PartitionCoalesced.rdd.getNumPartitions())\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["Examine the new distribution of data using `printRecordsPerPartition`.  Is the distribution uniform?"],"metadata":{}},{"cell_type":"code","source":["# TODO"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["## Review\n**Question:** How do you determine the number of connections to the database you write to?  \n**Answer:** Spark makes one connection for each partition in your data. Increasing the number of partitions increases the database connections.\n\n**Question:** How do you increase and decrease the number of partitions in your data?  \n**Answer:** `.repartitions(n)` increases the number of partitions in your data. It can also decrease the number of partitions, but since this is a wide operation it should be used sparingly.  `.coalesce(n)` decreases the number of partitions in your data. If you use `.coalesce(n)` with a number greater than the current paritions, this DataFrame method will have no effect.\n\n**Question:** How can you change the default number of partitions?  \n**Answer:** Changing the configuration parameter `spark.sql.shuffle.partitions` will alter the default number of partitions."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Table Management]($./07-Table-Management )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I find more information on reading the Spark UI?  \n**A:** Check out the Databricks blog on <a href=\"https://databricks.com/blog/2015/06/22/understanding-your-spark-application-through-visualization.html\" target=\"_blank\">Understanding your Spark Application through Visualization</a>"],"metadata":{}}],"metadata":{"name":"06-Database-Writes","notebookId":291050440996541},"nbformat":4,"nbformat_minor":0}
