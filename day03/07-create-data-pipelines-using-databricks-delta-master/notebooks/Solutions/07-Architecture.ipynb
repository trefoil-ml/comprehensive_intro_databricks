{"cells":[{"cell_type":"markdown","source":["# Databricks Delta Architecture\nDatabricks&reg; Delta simplifies data pipelines and eliminates the need for the traditional Lambda architecture.\n\n## Datasets Used\n* Read Wikipedia edits in real time, with a multitude of different languages. \n* Aggregate the anonymous edits by country, over a window, to see who's editing the English Wikipedia over time.\n\n## CAUTION\n* Do not use <b>RunAll</b> mode (next to <b>Permissions</b>)."],"metadata":{}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["-sandbox\n## Lambda Architecture\n\nThe Lambda architecture is a big data processing architecture that combines both batch- and real-time processing methods.\nIt features an append-only immutable data source that serves as system of record. Timestamped events are appended to \nexisting events (nothing is overwritten). Data is implicitly ordered by time of arrival. \n\nNotice how there are really two pipelines here, one batch and one streaming, hence the name <i>lambda</i> architecture.\n\nIt is very difficult to combine processing of batch and real-time data as is evidenced by the diagram below.\n\n\n<div><img src=\"https://s3-us-west-2.amazonaws.com/curriculum-release/images/eLearning/Delta/lambda.png\" style=\"height: 400px\"/></div><br/>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n## Databricks Delta Architecture\n\nThe Databricks Delta Architecture is a vast improvmemt upon the traditional Lambda architecture.\n\nText files, RDBMS data and streaming data is all collected into a <b>raw</b> table (also known as \"bronze\" tables at Databricks).\n\nA Raw table is then parsed into <b>query</b> tables (also known as \"silver\" tables at Databricks). They may be joined with dimension tables.\n\n<b>Summary</b> tables (also known as \"gold\" tables at Databricks) are business level aggregates often used for reporting and dashboarding. \nThis would include aggregations such as daily active website users.\n\nThe end outputs are actionable insights, dashboards and reports of business metrics.\n\n<div><img src=\"https://s3-us-west-2.amazonaws.com/curriculum-release/images/eLearning/Delta/delta.png\" style=\"height: 350px\"/></div><br/>"],"metadata":{}},{"cell_type":"markdown","source":["Set up relevant paths."],"metadata":{}},{"cell_type":"code","source":["basePath       = userhome + \"/wikipedia-streaming\"\nbronzePath     = basePath + \"/wikipediaEditsRaw.delta\"\nsilverPath     = basePath + \"/wikipediaEdits.delta\"\ngoldPath       = basePath + \"/wikipediaEditsSummary.delta\"\ncheckpointPath = basePath + \"/checkpoints\""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Save to RAW table (aka \"bronze table\")\n\n<b>Raw data</b> is unaltered data that is collected into a data lake, either via bulk upload or through streaming sources.\n\nThe following function reads the Wikipedia IRC channels that has been dumped into our Kafka server.\n\nThe Kafka server acts as a sort of \"firehose\" and dumps raw data into our data lake.\n\nSince raw data coming in from a stream is transient, we'd like to save it to a more permanent data structure.\n\nBelow, the first step is to set up schema. The fields we use further down in the notebook are commented."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType\nfrom pyspark.sql.functions import from_json, unix_timestamp\n\nschema = StructType([\n  StructField(\"channel\", StringType(), True),\n  StructField(\"comment\", StringType(), True),\n  StructField(\"delta\", IntegerType(), True),\n  StructField(\"flag\", StringType(), True),\n  StructField(\"geocoding\", StructType([                 # (OBJECT): Added by the server, field contains IP address geocoding information for anonymous edit.\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"countryCode2\", StringType(), True),\n    StructField(\"countryCode3\", StringType(), True),\n    StructField(\"stateProvince\", StringType(), True),\n    StructField(\"latitude\", DoubleType(), True),\n    StructField(\"longitude\", DoubleType(), True),\n  ]), True),\n  StructField(\"isAnonymous\", BooleanType(), True),      # (BOOLEAN): Whether or not the change was made by an anonymous user\n  StructField(\"isNewPage\", BooleanType(), True),\n  StructField(\"isRobot\", BooleanType(), True),\n  StructField(\"isUnpatrolled\", BooleanType(), True),\n  StructField(\"namespace\", StringType(), True),         # (STRING): Page's namespace. See https://en.wikipedia.org/wiki/Wikipedia:Namespace \n  StructField(\"page\", StringType(), True),              # (STRING): Printable name of the page that was edited\n  StructField(\"pageURL\", StringType(), True),           # (STRING): URL of the page that was edited\n  StructField(\"timestamp\", StringType(), True),         # (STRING): Time the edit occurred, in ISO-8601 format\n  StructField(\"url\", StringType(), True),\n  StructField(\"user\", StringType(), True),              # (STRING): User who made the edit or the IP address associated with the anonymous editor\n  StructField(\"userURL\", StringType(), True),\n  StructField(\"wikipediaURL\", StringType(), True),\n  StructField(\"wikipedia\", StringType(), True),         # (STRING): Short name of the Wikipedia that was edited (e.g., \"en\" for the English)\n])"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["-sandbox\nNext, stream into bronze Databricks Delta directory.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Notice how we are invoking the `.start(path)` method. \n\nThis is so that the data is streamed into the path we want (and not a default directory)."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import from_json, col\n(spark.readStream\n  .format(\"kafka\")  \n  .option(\"kafka.bootstrap.servers\", \"server1.databricks.training:9092\")\n  .option(\"subscribe\", \"en\")\n  .load()\n  .withColumn(\"json\", from_json(col(\"value\").cast(\"string\"), schema))\n  .select(col(\"timestamp\").alias(\"kafka_timestamp\"), col(\"json.*\"))\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", checkpointPath + \"/bronze\")\n  .outputMode(\"append\")\n  .start(bronzePath)\n)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Wait until stream is done initializing before running this cell."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS WikipediaEditsRaw\")\n\nspark.sql(\"\"\"\n  CREATE TABLE WikipediaEditsRaw\n  USING Delta\n  LOCATION '{}'\n\"\"\".format(bronzePath))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Take a look at the raw table."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM WikipediaEditsRaw LIMIT 5"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["## Create QUERY tables (aka \"silver tables\")\n\nNotice how `WikipediaEditsRaw` has JSON encoding. For example `{\"city\":null,\"country\":null,\"countryCode2\":null,\"c..`\n\nIn order to be able parse the data in human-readable form, create query tables out of the raw data using columns<br>\n`wikipedia`, `isAnonymous`, `namespace`, `page`, `pageURL`, `geocoding`, `timestamp` and `user`.\n\nStream into a Databricks Delta query directory."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import unix_timestamp\n\n(spark.readStream\n  .format(\"delta\")\n  .load(str(bronzePath))\n  .select(col(\"wikipedia\"),\n          col(\"isAnonymous\"),\n          col(\"namespace\"),\n          col(\"page\"),\n          col(\"pageURL\"),\n          col(\"geocoding\"),\n          unix_timestamp(col(\"timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSX\").cast(\"timestamp\").alias(\"timestamp\"),\n          col(\"user\"))\n  .writeStream\n  .format(\"delta\")\n  .option(\"checkpointLocation\", checkpointPath + \"/silver\")\n  .outputMode(\"append\")\n  .start(silverPath)\n)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Wait till stream is done initializing before running this cell."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"DROP TABLE IF EXISTS WikipediaEdits\")\n\nspark.sql(\"\"\"\n  CREATE TABLE WikipediaEdits\n  USING Delta\n  LOCATION '{}'\n\"\"\".format(silverPath))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Take a peek at the streaming query view."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM WikipediaEdits"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["## Create SUMMARY (aka \"gold\") level data \n\nSummary queries can take a long time.\n\nInstead of running the below query off `WikipediaEdits`, let's create a summary query.\n\nWe are interested in a breakdown of what countries anonymous edits are coming from."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, desc, count\n\ngoldDF = (spark.readStream\n  .format(\"delta\")\n  .load(str(silverPath))\n  .withColumn(\"countryCode\", col(\"geocoding.countryCode3\"))\n  .filter(col(\"namespace\") == \"article\")\n  .filter(col(\"countryCode\") != \"null\")\n  .filter(col(\"isAnonymous\") == True)\n  .groupBy(col(\"countryCode\"))\n  .count() \n  .withColumnRenamed('count', 'total')\n  .orderBy(col(\"total\").desc())\n)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["-sandbox\n\n## Creating Visualizations (aka \"platinum\" level) \n\n#### Mapping Anonymous Editors' Locations\n\nUse that geocoding information to figure out the countries associated with the editors.\n\nWhen you run the query, the default is a (live) html table.\n\nIn order to create a slick world map visualization of the data, you'll need to click on the item below.\n\n<div><img src=\"https://s3-us-west-2.amazonaws.com/curriculum-release/images/eLearning/Delta/plot-options-1.png\" style=\"height: 200px\"/></div><br/>\n\nThen go to <b>Plot Options...</b> and drag `countryCode` into the <b>Keys:</b> box and `total` into the <b>Values:</b> box and click <b>Apply</b>.\n\n<div><img src=\"https://s3-us-west-2.amazonaws.com/curriculum-release/images/eLearning/Delta/plot-options-2.png\" style=\"height: 200px\"/></div><br/> \n\nBy invoking a `display` action on a DataFrame created from a `readStream` transformation, we can generate a LIVE visualization!\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Keep an eye on the plot for a minute or two and watch the colors change."],"metadata":{}},{"cell_type":"code","source":["display(goldDF)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Make sure all streams are stopped."],"metadata":{}},{"cell_type":"code","source":["for s in spark.streams.active:\n    s.stop()\n    \ndbutils.fs.rm(userhome, True)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["## Summary\n\nUse the Databricks Delta architecture to craft raw, query and summary tables to produce beautiful visualizations of key business metrics.\n\nUse these concepts to implement a Delta architecture in the Capstone project."],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n**Q:** What is the difference between Lambda and Databricks Delta architecture?<br>\n**A:** The principal difference is that with Databricks Delta architecture, output queries can be performed on streaming and historical data at the same time.\n\nIn Lambda architecture, streaming and historical data are treated as two separate branches feeding output queries.\n\n**Q:** What is role of raw (bronze) tables?<br>\n**A:** Raw tables capture streaming and historical data into a permanent record (streaming data tends to disappear after a short while). Though, it's generally hard to query.\n\n**Q:** What is role of query (silver) tables?<br>\n**A:** Query tables consist of normalized raw data that is easier to query.\n\n**Q:** What is role of summary (gold) tables?<br>\n**A:** Summary tables contain aggregated key business metrics that are queried frequently, but the silver queries themselves would take too long."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"http://lambda-architecture.net/#\" target=\"_blank\">Lambda Architecture</a>\n* <a href=\"https://bennyaustin.wordpress.com/2010/05/02/kimball-and-inmon-dw-models/#\" target=\"_blank\">Data Warehouse Models</a>\n* <a href=\"https://people.apache.org//~pwendell/spark-nightly/spark-branch-2.1-docs/latest/structured-streaming-kafka-integration.html#\" target=\"_blank\">Reading structured streams from Kafka</a>\n* <a href=\"http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#creating-a-kafka-source-stream#\" target=\"_blank\">Create a Kafka Source Stream</a>\n\n**Extra Practice:** Apply what you learned in this module by completing the optional [Gain Actionable Insights from a Data Lake, Satisfy GDPR]($./Optional/Data-Lake-Insights) exercise."],"metadata":{}}],"metadata":{"name":"07-Architecture","notebookId":291050440994480},"nbformat":4,"nbformat_minor":0}
