{"cells":[{"cell_type":"markdown","source":["# Event Hubs and Spark Structured Streaming\n\nIn this lesson, you will see how you can perform near-realtime processing of messages using Spark Structured Streaming by sending a batch of messages to Event Hubs. Then, you will write a Structured Streaming query that lets you view the data as it comes in, and perform analytics against the streaming data using Spark SQL.\n\nIf you are running in an Azure Databricks environment that is already pre-configured with the libraries you need, you can skip to the next cell. To use this notebook in your own Databricks environment, you will need to create libraries, using the [Create Library](https://docs.azuredatabricks.net/user-guide/libraries.html) interface in Azure Databricks. Follow the steps below to attach the `azure-eventhubs-spark` library to your cluster:\n\n1. In the left-hand navigation menu of your Databricks workspace, select **Workspace**, select the down chevron next to **Shared**, and then select **Create** and **Library**.\n\n  ![Create Databricks Library](https://databricksdemostore.blob.core.windows.net/images/08/03/databricks-create-library.png 'Create Databricks Library')\n\n2. On the New Library screen, do the following:\n\n  - **Source**: Select Maven Coordinate.\n  - **Coordinate**: Enter \"azure-eventhubs-spark\", and then select **com.microsoft.azure:azure-eventhubs-spark_2.11:2.3.5**.\n  - Select **Create Library**.\n  \n  ![Databricks new Maven library](https://databricksdemostore.blob.core.windows.net/images/08/03/databricks-new-maven-library.png 'Databricks new Maven library')\n\n3. On the library page that is displayed, check the **Attach** checkbox next to the name of your cluster to run the library on that cluster.\n\n  ![Databricks attach library](https://databricksdemostore.blob.core.windows.net/images/08/03/databricks-attach-library.png 'Databricks attach library')\n\nOnce complete, return to this notebook to continue with the lesson."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Getting Started\n\nRun the following cell to configure your module.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Remember to attach your notebook to a cluster before running any cells in your notebook. In the notebook's toolbar, select the drop down arrow next to Detached, then select your cluster under Attach to.\n\n![Attached to cluster](https://databricksdemostore.blob.core.windows.net/images/03/03/databricks-cluster-attach.png)"],"metadata":{}},{"cell_type":"code","source":["%run \"./includes/Module-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["In order to reach Event Hubs, you will need to insert the connection string-primary key you acquired at the end of the Getting Started notebook in this module. You acquired this from the Azure Portal, and copied it into Notepad.exe or another text editor.\n\n> Read this article to learn [how to acquire the connection string for an Event Hub](https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-create) in your own Azure Subscription."],"metadata":{}},{"cell_type":"code","source":["event_hub_connection_string = #{your-event-hubs-connection-string-primary-key}"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Sending Events to Event Hubs\n\nFirst we need to import some support modules that will help us in creating a DataFrame that has the schema expected by Event Hubs."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructField, StructType, StringType, Row\nimport json"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["-sandbox\nIn the following, you create schema definition that represents the structure expected by Event Hubs. Then it adds five rows to that DataFrame and saves the DataFrame to the configured Event Hubs instance. This is in effect sending messages to the Event Hubs instance.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> You can run this cell as many times as you like to add more messages:"],"metadata":{}},{"cell_type":"code","source":["# Set up the Event Hub config dictionary with default settings\nwriteConnectionString = event_hub_connection_string\ncheckpointLocation = \"///checkpoint.txt\"\n\nehWriteConf = {\n  'eventhubs.connectionString' : writeConnectionString\n}\n\nevent_hubs_schema = StructType([\n  StructField(\"body\",StringType(), False),\n  StructField(\"partitionId\",StringType(), True),\n  StructField(\"partitionKey\",StringType(), True),\n])\n\nnewRows = [\n  Row(\"This is new message 1!\", None, None),\n  Row(\"This is new message 2!\", None, None),\n  Row(\"This is new message 3!\", None, None),\n  Row(\"This is new message 4!\", None, None),\n  Row(\"This is new message 5!\", None, None)\n]\nparallelizeRows = spark.sparkContext.parallelize(newRows)\nnew_messages = spark.createDataFrame(parallelizeRows, event_hubs_schema)\n\n# Write body data from a DataFrame to EventHubs. Events are distributed across partitions using round-robin model.\nds = new_messages \\\n  .select(\"body\") \\\n  .write \\\n  .format(\"eventhubs\") \\\n  .options(**ehWriteConf) \\\n  .option(\"checkpointLocation\", checkpointLocation) \\\n  .save()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Reading Events from Event Hubs\n\nNext, let's examine how you can read the messages you have sent. In the cell below you will setup a Structured Streaming query, which will be represented by a DataFrame. This cell effectively starts a process to listen for new messages, but when it first runs it will read from the beginning of the Event Hubs events (this is enabled by setting the offset attribute of the startingPosition configuration to -1). \n\nSee this [document](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/PySpark/structured-streaming-pyspark.md) for more details on the configuration options."],"metadata":{}},{"cell_type":"code","source":["# Source with default settings\nconnectionString = event_hub_connection_string\nehConf = {\n  'eventhubs.connectionString' : connectionString,\n  'eventhubs.startingPosition' : json.dumps({\"offset\":\"-1\", \"seqNo\":-1,\"enqueuedTime\": None,\"isInclusive\": True})\n}\n\nstreaming_df = spark \\\n  .readStream \\\n  .format(\"eventhubs\") \\\n  .options(**ehConf) \\\n  .load()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Even though the streaming query is running, we don't see anything in the output. \n\nTo see the streaming output, you simply pass the streaming DataFrame to the display method as the following cell shows. This cell will continue to run until you hit cancel."],"metadata":{}},{"cell_type":"code","source":["display(streaming_df.withColumn(\"body\", streaming_df[\"body\"].cast(\"string\")))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["You can register the streaming DataFrame as a temporary view, which will enable you to query the streaming using SQL! \n\nRun the following two cells to view the output. Feel free to scroll back up to the top of this notebook to send more messages and to see the count of messages in the SQL output increase (be patient, it can take a few seconds to update)."],"metadata":{}},{"cell_type":"code","source":["streaming_df.withColumn(\"body\", streaming_df[\"body\"].cast(\"string\")).createOrReplaceTempView(\"eventhubEvents\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%sql\nSELECT Count(body) FROM eventhubEvents"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## Challenge\nNow that you can query using SQL, try authoring an new query that looks only at the enqueuedTime and by using the built-in notebook visualizations produce a bar chart with the count of events that were enqueued at the same time (so the horizontal axis is enqueuedTime and the vertical axis is some form of count)."],"metadata":{}},{"cell_type":"markdown","source":["### Answers to Challenge"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT 1, enqueuedTime FROM eventhubEvents"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Databricks Delta Streaming]($./04-Streaming-with-Databricks-Delta)"],"metadata":{}}],"metadata":{"name":"03-Event-Hubs","notebookId":291050441001819},"nbformat":4,"nbformat_minor":0}
