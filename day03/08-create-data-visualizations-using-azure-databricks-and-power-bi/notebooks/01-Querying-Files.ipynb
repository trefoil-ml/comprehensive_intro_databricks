{"cells":[{"cell_type":"markdown","source":["-sandbox\n# Querying Files with Dataframes\n\nApache Spark&trade; and Azure Databricks&reg; allow you to use DataFrames to query large data files.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> For this course all notebooks are provided, but knowing how to create notebooks is essential to your future work."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Getting Started\n\nRun the following cell to configure our \"classroom.\"\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Remember to attach your notebook to a cluster; click  <b>Detached</b> in the upper left hand corner and then select your preferred cluster.\n\n<img src=\"https://files.training.databricks.com/images/eLearning/attach-to-cluster.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["### Introducing DataFrames\n\nUnder the covers, DataFrames are derived from data structures known as Resilient Distributed Datasets (RDDs). RDDs and DataFrames are immutable distributed collections of data. Let's take a closer look at what some of these terms mean before we understand how they relate to DataFrames:\n\n* **Resilient**: They are fault tolerant, so if part of your operation fails, Spark  quickly recovers the lost computation.\n* **Distributed**: RDDs are distributed across networked machines known as a cluster.\n* **DataFrame**: A data structure where data is organized into named columns, like a table in a relational database, but with richer optimizations under the hood. \n\nWithout the named columns and declared types provided by a schema, Spark wouldn't know how to optimize the executation of any computation. Since DataFrames have a schema, they use the Catalyst Optimizer to determine the optimal way to execute your code.\n\nDataFrames were invented because the business community uses tables in a relational database, Pandas or R DataFrames, or Excel worksheets. A Spark DataFrame is conceptually equivalent to these, with richer optimizations under the hood and the benefit of being distributed across a cluster."],"metadata":{}},{"cell_type":"markdown","source":["#### Interacting with DataFrames\n\nOnce created (instantiated), a DataFrame object has methods attached to it. Methods are operations one can perform on DataFrames such as filtering,\ncounting, aggregating and many others.\n\n> <b>Example</b>: To create (instantiate) a DataFrame, use this syntax: `df = ...`\n\nTo display the contents of the DataFrame, apply a `show` operation (method) on it using the syntax `df.show()`. \n\nThe `.` indicates you are *applying a method on the object*.\n\nIn working with DataFrames, it is common to chain operations together, such as: `df.select().filter().orderBy()`.  \n\nBy chaining operations together, you don't need to save intermediate DataFrames into local variables (thereby avoiding the creation of extra objects).\n\nAlso note that you do not have to worry about how to order operations because the optimizier determines the optimal order of execution of the operations for you.\n\n`df.select(...).orderBy(...).filter(...)`\n\nversus\n\n`df.filter(...).select(...).orderBy(...)`"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n#### DataFrames and SQL\n\nDataFrame syntax is more flexible than SQL syntax. Here we illustrate general usage patterns of SQL and DataFrames.\n\nSuppose we have a data set we loaded as a table called `myTable` and an equivalent DataFrame, called `df`.\nWe have three fields/columns called `col_1` (numeric type), `col_2` (string type) and `col_3` (timestamp type)\nHere are basic SQL operations and their DataFrame equivalents. \n\nNotice that columns in DataFrames are referenced by `col(\"<columnName>\")`.\n\n| SQL                                         | DataFrame (Python)                    |\n| ------------------------------------------- | ------------------------------------- | \n| `SELECT col_1 FROM myTable`                 | `df.select(col(\"col_1\"))`             | \n| `DESCRIBE myTable`                          | `df.printSchema()`                    | \n| `SELECT * FROM myTable WHERE col_1 > 0`     | `df.filter(col(\"col_1\") > 0)`         | \n| `..GROUP BY col_2`                          | `..groupBy(col(\"col_2\"))`             | \n| `..ORDER BY col_2`                          | `..orderBy(col(\"col_2\"))`             | \n| `..WHERE year(col_3) > 1990`                | `..filter(year(col(\"col_3\")) > 1990)` | \n| `SELECT * FROM myTable LIMIT 10`            | `df.limit(10)`                        |\n| `display(myTable)` (text format)            | `df.show()`                           | \n| `display(myTable)` (html format)            | `display(df)`                         |\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** You can also run SQL queries with the special syntax `spark.sql(\"SELECT * FROM myTable\")`\n\nIn this course you see many other usages of DataFrames. It is left up to you to figure out the SQL equivalents \n(left as exercises in some cases)."],"metadata":{}},{"cell_type":"markdown","source":["### Querying Data \nThis lesson uses the `people-10m` data set, which is in Parquet format.\n\nThe data is fictitious; in particular, the Social Security numbers are fake."],"metadata":{}},{"cell_type":"markdown","source":["Run the command below to see the contents of the `people-10m.parquet` file."],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/training/dataframes/people-10m.parquet"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["peopleDF = spark.read.parquet(\"/mnt/training/dataframes/people-10m.parquet\")\ndisplay(peopleDF)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Take a look at the schema with the `printSchema` method. This tells you the field name, field type, and whether the column is nullable or not (default is true)."],"metadata":{}},{"cell_type":"code","source":["peopleDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Answer the following question:\n> According to our data, which women were born after 1990?\n\nUse the DataFrame `select` and `filter` methods."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import year\ndisplay(peopleDF \n  .select(\"firstName\",\"middleName\",\"lastName\",\"birthDate\",\"gender\") \n  .filter(\"gender = 'F'\") \n  .filter(year(\"birthDate\") > \"1990\")\n)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Built-In Functions\n\nSpark provides a number of <a href=\"https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\" target=\"_blank\">built-in functions</a>, many of which can be used directly with DataFrames.  Use these functions in the `filter` expressions to filter data and in `select` expressions to create derived columns.\n\nThe following DataFrame statement finds women born after 1990; it uses the `year` function, and it creates a `birthYear` column on the fly."],"metadata":{}},{"cell_type":"code","source":["display(peopleDF\n  .select(\"firstName\",\"middleName\",\"lastName\",year(\"birthDate\").alias(\"birthYear\"),\"salary\")\n  .filter(year(\"birthDate\") > \"1990\") \n  .filter(\"gender = 'F' \"))"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["-sandbox\n### Visualization\n\nDatabricks provides easy-to-use, built-in visualizations for your data. \n\nDisplay the data by invoking the Spark `display` function.\n\nVisualize the query below by selecting the bar graph icon once the table is displayed:\n\n<img src=\"https://files.training.databricks.com/images/eLearning/visualization-1.png\" style=\"border: 1px solid #aaa; padding: 10px; border-radius: 10px 10px 10px 10px\"/>"],"metadata":{}},{"cell_type":"markdown","source":["How many women were named Mary in each year?"],"metadata":{}},{"cell_type":"code","source":["marysDF = (peopleDF.select(year(\"birthDate\").alias(\"birthYear\")) \n  .filter(\"firstName = 'Mary' \") \n  .filter(\"gender = 'F' \") \n  .orderBy(\"birthYear\") \n  .groupBy(\"birthYear\") \n  .count()\n)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["To start the visualization process, first apply the `display` function to the DataFrame. \n\nNext, click the graph button in the bottom left corner (second from left) to display data in different ways.\n\nThe data initially shows up in html format as an `n X 2` column where one column is the `birthYear` and another column is `count`."],"metadata":{}},{"cell_type":"code","source":["display(marysDF)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Compare popularity of two names from 1990."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndordonDF = (peopleDF \n  .select(year(\"birthDate\").alias(\"birthYear\"), \"firstName\") \n  .filter((col(\"firstName\") == 'Donna') | (col(\"firstName\") == 'Dorothy')) \n  .filter(\"gender == 'F' \") \n  .filter(year(\"birthDate\") > 1990) \n  .orderBy(\"birthYear\") \n  .groupBy(\"birthYear\", \"firstName\") \n  .count()\n)\ndisplay(dordonDF)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Temporary Views\n\nIn DataFrames, <b>temporary views</b> are used to make the DataFrame available to SQL, and work with SQL syntax seamlessly.\n\nA temporary view gives you a name to query from SQL, but unlike a table it exists only for the duration of your Spark Session. As a result, the temporary view will not carry over when you restart the cluster or switch to a new notebook. It also won't show up in the Data button on the menu on the left side of a Databricks notebook which provides easy access to databases and tables.\n\nThe statement in the following cells create a temporary view containing the same data."],"metadata":{}},{"cell_type":"code","source":["peopleDF.createOrReplaceTempView(\"People10M\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["To view the contents of temporary view, use select notation."],"metadata":{}},{"cell_type":"code","source":["display(spark.sql(\"SELECT * FROM  People10M where firstName = 'Donna' \"))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Create a DataFrame with a more specific query."],"metadata":{}},{"cell_type":"code","source":["womenBornAfter1990DF = (peopleDF \n  .select(\"firstName\", \"middleName\", \"lastName\",year(\"birthDate\").alias(\"birthYear\"), \"salary\") \n  .filter(year(\"birthDate\") > 1990) \n  .filter(\"gender = 'F' \") \n)\ndisplay(womenBornAfter1990DF)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Create Temporary Views from the `womenBornAfter1990DF` DataFrame"],"metadata":{}},{"cell_type":"code","source":["womenBornAfter1990DF.createOrReplaceTempView(\"womenBornAfter1990\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Once a temporary view has been created, it can be queried as if it were a table. \n\nFind out how many Marys are in the WomenBornAfter1990 DataFrame."],"metadata":{}},{"cell_type":"code","source":["display(spark.sql(\"SELECT count(*) FROM womenBornAfter1990 where firstName = 'Mary' \"))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["-sandbox\n## Exercise 1\n\nCreate a DataFrame called top10FemaleFirstNamesDF that contains the 10 most common female first names out of the people data set.\n\n* `firstName` - the first name\n* `total` - the total number of rows with that first name\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** \n* You may need to break ties by using firstName because some of the totals are identical.\n* To restrict the number of names to 10, you need to use the `limit(10)` method.\n* You also need to use the `agg()` method to do a count of `firstName` and give it an alias.\n* The `agg()` method is applied after the `groupBy` since it requires column values to be collected in some fashion.\n* You will need to import the `count` and `desc` methods in Scala or Python, as appropriate.\n\nDisplay the results."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Step 1\nCreate a DataFrame called `top10FemaleFirstNamesDF` and display the results."],"metadata":{}},{"cell_type":"code","source":["# TODO\nfrom pyspark.sql.functions import count, desc\ntop10FemaleFirstNamesDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["top10FemaleNamesDF = top10FemaleFirstNamesDF.orderBy(\"firstName\")\n\ndisplay(top10FemaleNamesDF)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["from pyspark.sql import Row\nresults = top10FemaleNamesDF.collect()\n\ndbTest(\"DF-L2-names-0\", Row(firstName=u\"Alesha\",    total=1368), results[0])  \ndbTest(\"DF-L2-names-1\", Row(firstName=u\"Alice\",     total=1384), results[1])\ndbTest(\"DF-L2-names-2\", Row(firstName=u\"Bridgette\", total=1373), results[2])\ndbTest(\"DF-L2-names-3\", Row(firstName=u\"Cristen\",   total=1375), results[3])\ndbTest(\"DF-L2-names-4\", Row(firstName=u\"Jacquelyn\", total=1381), results[4])\ndbTest(\"DF-L2-names-5\", Row(firstName=u\"Katherin\",  total=1373), results[5])\ndbTest(\"DF-L2-names-6\", Row(firstName=u\"Lashell\",   total=1387), results[6])\ndbTest(\"DF-L2-names-7\", Row(firstName=u\"Louie\",     total=1382), results[7])\ndbTest(\"DF-L2-names-8\", Row(firstName=u\"Lucille\",   total=1384), results[8])\ndbTest(\"DF-L2-names-9\", Row(firstName=u\"Sharyn\",    total=1394), results[9]) \n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["### Step 2\n\nConvert the DataFrame to a temporary view and display the contents of the temporary view."],"metadata":{}},{"cell_type":"code","source":["# TODO\ntop10FemaleFirstNamesDF.createOrReplaceTempView(\"Top10FemaleFirstNames\")\nresultsDF = # FILL_IN\ndisplay(resultsDF)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["## Summary\n* Spark DataFrames can be used to query Data Sets.\n* Visualize the results of your queries with built-in Databricks graphs."],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n\n**Q:** How do you *create* a DataFrame object?  \n**A:** An object is created by introducing a variable name and equating it to something like `myDataFrameDF =`. \n\n**Q:** What methods (operations) can you perform on a DataFrame object?  \n**A:** The full list is here: <a href=\"http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html\" target=\"_blank\"> pyspark.sql module</a>\n\n**Q:** Why do you chain methods (operations) `myDataFrameDF.select().filter().groupBy()`?  \n**A:** To avoid the creation of temporary DataFrames as local variables. \n\nFor example, you could have written the above as: `tempDF1 = myDataFrameDF.select()`,  `tempDF2 = tempDF1.filter()` and\nthen `tempDF2.groupBy()`. \n\nThis is syntatically equivalent, but, notice how you now have extra local variables.\n\n**Q:** What is the DataFrame syntax to create a temporary view?    \n**A:** ```myDF.createOrReplaceTempView(\"MyTempView\")```"],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Exploratory Data Analysis]($./02-Exploratory-Data-Analysis)."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html\" target=\"_blank\">Spark SQL, DataFrames and Datasets Guide</a>"],"metadata":{}}],"metadata":{"name":"01-Querying-Files","notebookId":291050440995954},"nbformat":4,"nbformat_minor":0}
