{"cells":[{"cell_type":"markdown","source":["# ETL Process Overview\n\nApache Spark&trade; and Azure Databricks&reg; allow you to create an end-to-end _extract, transform, load (ETL)_ pipeline."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### The Spark Approach\n\nSpark offers a compute engine and connectors to virtually any data source. By leveraging easily scaled infrastructure and accessing data where it lives, Spark addresses the core needs of a big data application.\n\nThese principles comprise the Spark approach to ETL, providing a unified and scalable approach to big data pipelines: <br><br>\n\n1. Databricks and Spark offer a **unified platform** \n - Spark on Databricks combines ETL, stream processing, machine learning, and collaborative notebooks.\n - Data scientists, analysts, and engineers can write Spark code in Python, Scala, SQL, and R.\n2. Spark's unified platform is **scalable to petabytes of data and clusters of thousands of nodes**.  \n - The same code written on smaller data sets scales to large workloads, often with only small changes.\n2. Spark on Databricks decouples data storage from the compute and query engine.  \n - Spark's query engine **connects to any number of data sources** such as S3, Azure Blob Storage, Redshift, and Kafka.  \n - This **minimizes costs**; a dedicated cluster does not need to be maintained and the compute cluster is **easily updated to the latest version** of Spark.\n \n<img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/Workload_Tools_2-01.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"],"metadata":{}},{"cell_type":"markdown","source":["### A Basic ETL Job\n\nIn this lesson you use web log files from the <a href=\"https://www.sec.gov/dera/data/edgar-log-file-data-set.html\" target=\"_blank\">US Securities and Exchange Commision website</a> to do a basic ETL for a day of server activity. You will extract the fields of interest and load them into persistent storage."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Getting Started\n\nRun the following cell to configure our \"classroom.\"\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Remember to attach your notebook to a cluster. Click <b>Detached</b> in the upper left hand corner and then select your preferred cluster.\n\n<img src=\"https://files.training.databricks.com/images/eLearning/attach-to-cluster.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"],"metadata":{}},{"cell_type":"markdown","source":["Run the cell below to mount the data. Details on how this works are covered in the next lesson."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["The Databricks File System (DBFS) is an HDFS-like interface to bulk data storages like Azure's Blob storage service.\n\nPass the path `/mnt/training/EDGAR-Log-20170329/EDGAR-Log-20170329.csv` into `spark.read.csv`to access data stored in DBFS. Use the header option to specify that the first line of the file is the header."],"metadata":{}},{"cell_type":"code","source":["path = \"/mnt/training/EDGAR-Log-20170329/EDGAR-Log-20170329.csv\"\n\nlogDF = (spark\n  .read\n  .option(\"header\", True)\n  .csv(path)\n  .sample(withReplacement=False, fraction=0.3, seed=3) # using a sample to reduce data size\n)\n\ndisplay(logDF)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Next, review the server-side errors, which have error codes in the 500s."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nserverErrorDF = (logDF\n  .filter((col(\"code\") >= 500) & (col(\"code\") < 600))\n  .select(\"date\", \"time\", \"extention\", \"code\")\n)\n\ndisplay(serverErrorDF)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Data Validation\n\nOne aspect of ETL jobs is to validate that the data is what you expect.  This includes:<br><br>\n* Approximately the expected number of records\n* The expected fields are present\n* No unexpected missing values"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\nTake a look at the server-side errors by hour to confirm the data meets your expectations. Visualize it by selecting the bar graph icon once the table is displayed. <br><br>\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/visualization.png\" style=\"height: 400px\" style=\"margin-bottom: 20px; height: 150px; border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/></div>"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import from_utc_timestamp, hour, col\n\ncountsDF = (serverErrorDF\n  .select(hour(from_utc_timestamp(col(\"time\"), \"GMT\")).alias(\"hour\"))\n  .groupBy(\"hour\")\n  .count()\n  .orderBy(\"hour\")\n)\n\ndisplay(countsDF)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["The distribution of errors by hour meets the expections.  There is an uptick in errors around midnight, possibly due to server maintenance at this time."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Saving Back to DBFS\n\nA common and highly effective design pattern in the Databricks and Spark ecosytem involves loading structured data back to DBFS as a parquet file. Learn more about [the scalable and optimized data storage format parquet here](http://parquet.apache.org/).\n\nSave the parsed DataFrame back to DBFS as parquet using the `.write` method.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> All clusters have storage availiable to them in the `/tmp/` directory.  In the case of Community Edition clusters, this is a small, but helpful, amount of storage.  \n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> If you run out of storage, use the command `dbutils.fs.rm(\"/tmp/<my directory>\", True)` to recursively remove all items from a directory.  Note that this is a permanent action."],"metadata":{}},{"cell_type":"code","source":["(serverErrorDF\n  .write\n  .mode(\"overwrite\") # overwrites a file if it already exists\n  .parquet(\"/tmp/log20170329/serverErrorDF.parquet\")\n)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["-sandbox\n### Our ETL Pipeline\n\nHere's what the ETL pipeline you just built looks like.  In the rest of this course you will work with more complex versions of this general pattern.\n\n| Code | Stage |\n|------|-------|\n| `logDF = (spark`                                                                          | Extract |\n| &nbsp;&nbsp;&nbsp;&nbsp;`.read`                                                           | Extract |\n| &nbsp;&nbsp;&nbsp;&nbsp;`.option(\"header\", True)`                                         | Extract |\n| &nbsp;&nbsp;&nbsp;&nbsp;`.csv(<source>)`                                                  | Extract |\n| `)`                                                                                       | Extract |\n| `serverErrorDF = (logDF`                                                                  | Transform |\n| &nbsp;&nbsp;&nbsp;&nbsp;`.filter((col(\"code\") >= 500) & (col(\"code\") < 600))`             | Transform |\n| &nbsp;&nbsp;&nbsp;&nbsp;`.select(\"date\", \"time\", \"extention\", \"code\")`                    | Transform |\n| `)`                                                                                       | Transform |\n| `(serverErrorDF.write`                                                                 | Load |\n| &nbsp;&nbsp;&nbsp;&nbsp;`.parquet(<destination>))`                                      | Load |\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> This is a distributed job, so it can easily scale to fit the demands of your data set."],"metadata":{}},{"cell_type":"markdown","source":["## Exercise 1: Perform an ETL Job\n\nWrite a basic ETL script that captures the 20 most active website users and load the results to DBFS."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Create a DataFrame of Aggregate Statistics\n\nCreate a DataFrame `ipCountDF` that uses `logDF` to create a count of each time a given IP address appears in the logs, with the counts sorted in descending order.  The result should have two columns: `ip` and `count`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import desc\n\nipCountDF = (logDF\n  .select(\"ip\")\n  .groupBy(\"ip\")\n  .count()\n  .orderBy(desc(\"count\"))\n)\n\ndisplay(ipCountDF)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nip1, count1 = ipCountDF.first()\ncols = set(ipCountDF.columns)\n\ndbTest(\"ET1-P-02-01-01\", \"213.152.28.bhe\", ip1)\ndbTest(\"ET1-P-02-01-02\", True, count1 > 500000 and count1 < 550000)\ndbTest(\"ET1-P-02-01-03\", {'count', 'ip'}, cols)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["-sandbox\n### Step 2: Save the Results\n\nUse your tempory folder to save the results back to DBFS as `ipCount.parquet`\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** If you run out of space, use `%fs rm -r /tmp/<my directory>` to recursively (and permanently) remove all items from a directory."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n(ipCountDF\n  .write\n  .mode(\"overwrite\")\n  .parquet(\"/tmp/ipCount.parquet\")\n)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nfrom pyspark.sql.functions import desc\n\nipCountDF2 = (spark\n  .read\n  .parquet(\"/tmp/ipCount.parquet\")\n  .orderBy(desc(\"count\"))\n)\nip1, count1 = ipCountDF2.first()\ncols = set(ipCountDF2.columns)\n\ndbTest(\"ET1-P-02-02-01\", \"213.152.28.bhe\", ip1)\ndbTest(\"ET1-P-02-02-02\", True, count1 > 500000 and count1 < 550000)\ndbTest(\"ET1-P-02-02-03\", {'count', 'ip'}, cols)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Check the load worked by using `%fs ls <path>`.  Parquet divides your data into a number of files.  If successful, you see a `_SUCCESS` file as well as the data split across a number of parts."],"metadata":{}},{"cell_type":"code","source":["%fs ls /tmp/ipCount.parquet"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["## Review\n**Question:** What does ETL stand for and what are the stages of the process?  \n**Answer:** ETL stands for `extract-transform-load`\n0. *Extract* refers to ingesting data.  Spark easily connects to data in a number of different sources.\n0. *Transform* refers to applying structure, parsing fields, cleaning data, and/or computing statistics.\n0. *Load* refers to loading data to its final destination, usually a database or data warehouse.\n\n**Question:** How does the Spark approach to ETL deal with devops issues such as updating a software version?  \n**Answer:** By decoupling storage and compute, updating your Spark version is as easy as spinning up a new cluster.  Your old code will easily connect to Azure Blob, or other storage.  This also avoids the challenge of keeping a cluster always running, such as with Hadoop clusters.\n\n**Question:** How does the Spark approach to data applications differ from other solutions?  \n**Answer:** Spark offers a unified solution to use cases that would otherwise need individual tools. For instance, Spark combines machine learning, ETL, stream processing, and a number of other solutions all with one technology."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Connecting to Azure Blob Storage]($./03-Connecting-to-Azure-Blob-Storage )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I get more information on building ETL pipelines?  \n**A:** Check out the Spark Summit talk on <a href=\"https://databricks.com/session/building-robust-etl-pipelines-with-apache-spark\" target=\"_blank\">Building Robust ETL Pipelines with Apache Spark</a>\n\n**Q:** Where can I find out more information on moving from traditional ETL pipelines towards Spark?  \n**A:** Check out the Spark Summit talk <a href=\"https://databricks.com/session/get-rid-of-traditional-etl-move-to-spark\" target=\"_blank\">Get Rid of Traditional ETL, Move to Spark!</a>\n\n**Q:** What are the visualization options in Databricks?  \n**A:** Databricks provides a wide variety of <a href=\"https://docs.azuredatabricks.net/user-guide/visualizations/index.html#id1\" target=\"_blank\">built-in visualizations</a>.  Databricks also supports a variety of 3rd party visualization libraries, including <a href=\"https://d3js.org/\" target=\"_blank\">d3.js</a>, <a href=\"https://matplotlib.org/\" target=\"_blank\">matplotlib</a>, <a href=\"http://ggplot.yhathq.com/\" target=\"_blank\">ggplot</a>, and <a href=\"https://plot.ly/\" target=\"_blank\">plotly<a/>."],"metadata":{}}],"metadata":{"name":"02-ETL-Process-Overview","notebookId":291050440997758},"nbformat":4,"nbformat_minor":0}
