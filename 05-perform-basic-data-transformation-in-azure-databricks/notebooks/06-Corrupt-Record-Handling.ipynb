{"cells":[{"cell_type":"markdown","source":["# Corrupt Record Handling\n\nApache Spark&trade; and Azure Databricks&reg; provide ways to handle corrupt records."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n## Working with Corrupt Data\n\nETL pipelines need robust solutions to handle corrupt data. This is because data corruption scales as the size of data and complexity of the data application grow. Corrupt data includes:  \n<br>\n* Missing information\n* Incomplete information\n* Schema mismatch\n* Differing formats or data types\n* User errors when writing data producers\n\nSince ETL pipelines are built to be automated, production-oriented solutions must ensure pipelines behave as expected. This means that **data engineers must both expect and systematically handle corrupt records.**\n\nIn the roadmap for ETL, this is the **Handle Corrupt Records** step:\n<img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/ETL-Process-3.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell to mount the data:"],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["-sandbox\nRun the following cell, which contains a corrupt record, `{\"a\": 1, \"b, \"c\":10}`:\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> This is not the preferred way to make a DataFrame.  This code allows us to mimic a corrupt record you might see in production."],"metadata":{}},{"cell_type":"code","source":["data = \"\"\"{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b, \"c\":10}\"\"\".split('|')\n\ncorruptDF = (spark.read\n  .option(\"mode\", \"PERMISSIVE\")\n  .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n  .json(sc.parallelize(data))\n)\ndisplay(corruptDF)  "],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["In the previous results, Spark parsed the corrupt record into its own column and processed the other records as expected. This is the default behavior for corrupt records, so you didn't technically need to use the two options `mode` and `columnNameOfCorruptRecord`.\n\nThere are three different options for handling corrupt records [set through the `ParseMode` option](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/ParseMode.scala#L34):\n\n| `ParseMode` | Behavior |\n|-------------|----------|\n| `PERMISSIVE` | Includes corrupt records in a \"_corrupt_record\" column (by default) |\n| `DROPMALFORMED` | Ignores all corrupted records |\n| `FAILFAST` | Throws an exception when it meets corrupted records |\n\nThe following cell acts on the same data but drops corrupt records:"],"metadata":{}},{"cell_type":"code","source":["data = \"\"\"{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b, \"c\":10}\"\"\".split('|')\n\ncorruptDF = (spark.read\n  .option(\"mode\", \"DROPMALFORMED\")\n  .json(sc.parallelize(data))\n)\ndisplay(corruptDF)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["The following cell throws an error once a corrupt record is found, rather than ignoring or saving the corrupt records:"],"metadata":{}},{"cell_type":"code","source":["data = \"\"\"{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b, \"c\":10}\"\"\".split('|')\n\ncorruptDF = (spark.read\n  .option(\"mode\", \"FAILFAST\")\n  .json(sc.parallelize(data))\n)\ndisplay(corruptDF)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Recommended Pattern: `badRecordsPath`\n\nDatabricks Runtime has [a built-in feature](https://docs.azuredatabricks.net/spark/latest/spark-sql/handling-bad-records.html#handling-bad-records-and-files) that saves corrupt records to a given end point. To use this, set the `badRecordsPath`.\n\nThis is a preferred design pattern since it persists the corrupt records for later analysis even after the cluster shuts down."],"metadata":{}},{"cell_type":"code","source":["data = \"\"\"{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b\":2, \"c\":3}|{\"a\": 1, \"b, \"c\":10}\"\"\".split('|')\n\ncorruptDF = (spark.read\n  .option(\"badRecordsPath\", \"/tmp/badRecordsPath\")\n  .json(sc.parallelize(data))\n)\ndisplay(corruptDF)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["-sandbox\nSee the results in `/tmp/badRecordsPath`.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Recall that `/tmp` is a directory backed by S3 or the Azure Blob available to all clusters."],"metadata":{}},{"cell_type":"code","source":["display(spark.read.json(\"/tmp/badRecordsPath/*/*/*\"))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Exercise 1: Working with Corrupt Records"],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Diagnose the Problem\n\nImport the data used in the last lesson, which is located at `/mnt/training/UbiqLog4UCI/14_F/log*`.  Import the corrupt records in a new column `SMSCorrupt`.  <br>\n\nSave only the columns `SMS` and `SMSCorrupt` to the new DataFrame `SMSCorruptDF`."],"metadata":{}},{"cell_type":"code","source":["# TODO\nSMSCorruptDF = # FILL_IN"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\ncols = set(SMSCorruptDF.columns)\nSMSCount = SMSCorruptDF.cache().count()\n\ndbTest(\"ET1-P-06-01-01\", {'SMS', 'SMSCorrupt'}, cols)\ndbTest(\"ET1-P-06-01-02\", 8, SMSCount)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["-sandbox\nExamine the corrupt records to determine what the problem is with the bad records.\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** Take a look at the name in metadata."],"metadata":{}},{"cell_type":"markdown","source":["The entry `{\"name\": \"mr Khojasteh\"flash\"\"}` should have single quotes around `flash` since the double quotes are interpreted as the end of the value.  It should read `{\"name\": \"mr Khojasteh'flash'\"}` instead.\n\nThe optimal solution is to fix the initial producer of the data to correct the problem at its source.  In the meantime, you could write ad hoc logic to turn this into a readable field."],"metadata":{}},{"cell_type":"markdown","source":["### Step 2: Use `badRecordsPath`\n\nUse the `badRecordsPath` option to save corrupt records to the directory `/tmp/corruptSMS`."],"metadata":{}},{"cell_type":"code","source":["# TODO"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# TEST - Run this cell to test your solution\nSMSCorruptDF2.count()\ncorruptCount = spark.read.json(\"/tmp/corruptSMS/*/*/*\").count()\n\ndbTest(\"ET1-P-06-02-01\", True, corruptCount >= 8)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## Review\n**Question:** By default, how are corrupt records dealt with using `spark.read.json()`?  \n**Answer:** They appear in a collumn called `_corrupt_record`.\n\n**Question:** How can a query persist corrupt records in separate destination?  \n**Answer:** The Databricks feature `badRecordsPath` allows a query to save corrupt records to a given end point for the pipeline engineer to investigate corruption issues."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Loading Data and Productionalizing]($./07-Loading-Data-and-Productionalizing)."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I get more information on dealing with corrupt records?  \n**A:** Check out the Spark Summit talk on <a href=\"https://databricks.com/session/exceptions-are-the-norm-dealing-with-bad-actors-in-etl\" target=\"_blank\">Exceptions are the Norm: Dealing with Bad Actors in ETL</a>"],"metadata":{}}],"metadata":{"name":"06-Corrupt-Record-Handling","notebookId":291050440997621},"nbformat":4,"nbformat_minor":0}
