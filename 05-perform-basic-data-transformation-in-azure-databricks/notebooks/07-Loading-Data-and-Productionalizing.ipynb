{"cells":[{"cell_type":"markdown","source":["# Loading Data and Productionalizing\n\nApache Spark&trade; and Azure Databricks&reg; allow you to productionalize code by scheduling notebooks for regular execution."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n## Introductory Productionalizing\n\nIncorporating notebooks into production workflows will be covered in detail in later courses. This lesson focuses on two aspects of productionalizing: Parquet as a best practice for loading data from ETL jobs and scheduling jobs.\n\nIn the roadmap for ETL, this is the **Load and Automate** step:\n\n<img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/ETL-Process-4.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n## Writing Parquet\n\nBLOB stores like Azure Blob Storage are the data storage option of choice on Databricks, and Parquet is the storage format of choice.  [Apache Parquet](https://parquet.apache.org/documentation/latest/) is a highly efficient, column-oriented data format that shows massive performance increases over other options such as CSV. For instance, Parquet compresses data repeated in a given column and preserves the schema from a write.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> When writing data to DBFS, the best practice is to use Parquet."],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell to mount the data:"],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Import Chicago crime data."],"metadata":{}},{"cell_type":"code","source":["crimeDF = (spark.read\n  .option(\"delimiter\", \"\\t\")\n  .option(\"header\", True)\n  .option(\"timestampFormat\", \"mm/dd/yyyy hh:mm:ss a\")\n  .option(\"inferSchema\", True)\n  .csv(\"/mnt/training/Chicago-Crimes-2018.csv\")\n)\ndisplay(crimeDF)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Rename the columns in `CrimeDF` so there are no spaces or invalid characters. This is required by Spark and is a best practice.  Use camel case."],"metadata":{}},{"cell_type":"code","source":["cols = crimeDF.columns\ntitleCols = [''.join(j for j in i.title() if not j.isspace()) for i in cols]\ncamelCols = [column[0].lower()+column[1:] for column in titleCols]\n\ncrimeRenamedColsDF = crimeDF.toDF(*camelCols)\ndisplay(crimeRenamedColsDF)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["-sandbox\nWrite to Parquet by calling the following method on a DataFrame: `.write.parquet(\"mnt/<destination>.parquet\")`.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Specify the write mode (for example, `overwrite` or `append`) using `.mode()`.  \n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Write to `/tmp/`, a directory backed by the Azure Blob or S3 available to all Datatabricks clusters. If your `/tmp/` directory is full, clear contents using `%fs rm -r /tmp/`.\n\n[See the documentation for additional specifications.](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=parquet#pyspark.sql.DataFrameWriter.parquet)"],"metadata":{}},{"cell_type":"code","source":["crimeRenamedColsDF.write.mode(\"overwrite\").parquet(\"/tmp/crime.parquet\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["-sandbox\nReview how this command writes the Parquet file. An advantage of Parquet is that, unlike a CSV file which is normally a single file, Parquet is distributed so each partition of data in the cluster writes to its own \"part\". Notice the different log data included in this directory.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Write other file formats in this same way (for example, `.write.csv(\"mnt/<destination>.csv\")`)"],"metadata":{}},{"cell_type":"code","source":["%fs ls /tmp/crime.parquet"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["-sandbox\nUse the `repartition` DataFrame method to repartition the data to limit the number of separate parts.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> What appears to the user as a single DataFrame is actually data distribted across a cluster.  Each cluster holds _partitions_, or parts, of the data.  By repartitioning, we define how many different parts of our data to have."],"metadata":{}},{"cell_type":"code","source":["crimeRenamedColsDF.repartition(1).write.mode(\"overwrite\").parquet(\"/tmp/crimeRepartitioned.parquet\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Now look at how many parts are in the new folder. You have one part for each partition. Since you repartitioned the DataFrame with a value of `1`, now all the data is in `part-00000`."],"metadata":{}},{"cell_type":"code","source":["%fs ls /tmp/crimeRepartitioned.parquet"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["-sandbox\n### Automate by Scheduling a Job\n\nScheduling a job allows you to perform a batch process at a regular interval. Schedule email updates for successful completion and error logs.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Since jobs are not available in the Community Edition version of Databricks, you are unable to follow along in Community Edition."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n\n1. Click **Jobs** in the lefthand panel of the screen.\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/Jobs.png\" style=\"height: 200px\" style=\"margin-bottom: 20px; height: 150px; border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa; margin: 20px\"/></div>\n2. Click **Create Job**.\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/Jobs2.png\" style=\"height: 200px\" style=\"margin-bottom: 20px; height: 150px; border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa; margin: 20px\"/></div>\n3. Perform the following:\n - Name the job\n - Choose the notebook the job will execute\n - Specify the cluster\n - Choose a daily job\n - Send yourself an email alert\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/Jobs3.png\" style=\"height: 200px\" style=\"margin-bottom: 20px; height: 150px; border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa; margin: 20px\"/></div>\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Remember to turn off the job so it does not execute indefinitely."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n## Exercise 1 (Optional): Productionalizing a Job\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Community Edition users are not able to complete this exercise."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### Step 1: Run All\n\nClick **Run All** to confirm the notebook runs.  If there are any errors, fix them.\n\n<div><img src=\"https://files.training.databricks.com/images/eLearning/ETL-Part-1/Jobs4.png\" style=\"height: 200px\" style=\"margin-bottom: 20px; height: 150px; border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/></div>"],"metadata":{}},{"cell_type":"markdown","source":["### Step 2: Schedule a Job\n\nSchedule this notebook to run using the steps above."],"metadata":{}},{"cell_type":"markdown","source":["## Review\n\n**Question:** What is the recommended storage format to use with Spark?  \n**Answer:** Apache Parquet is a highly optimized solution for data storage and is the recommended option for storage where possible.  In addition to offering benefits like compression, it's distributed, so a given partition of data writes to its own file, enabling parallel reads and writes. Formats like CSV are prone to corruption since a single missing comma could corrupt the data. Also, the data cannot be parallelized.\n\n**Question:** How do you schedule a regularly occuring task in Databricks?  \n**Answer:** The Jobs tab of a Databricks notebook or the new [Jobs API](https://docs.azuredatabricks.net/api/latest/jobs.html#job-api) allows for job automation."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n**Q:** Where can I get more information on scheduling jobs on Databricks?  \n**A:** Check out the Databricks documentation on <a href=\"https://docs.azuredatabricks.net/user-guide/jobs.html\" target=\"_blank\">Scheduling Jobs on Databricks</a>\n\n**Q:** How can I schedule complex jobs, such as those involving dependencies between jobs?  \n**A:** There are two options for complex jobs.  The easiest solution is <a href=\"https://docs.azuredatabricks.net/user-guide/notebooks/notebook-workflows.html\" target=\"_blank\">Notebook Workflows</a>, which involes using one notebook that triggers the execution of other notebooks. For more complexity, <a href=\"https://databricks.com/blog/2017/07/19/integrating-apache-airflow-with-databricks.html\" target=\"_blank\">Databricks integrates with the open source workflow scheduler Apache Airflow.</a>\n\n**Q:** How do I perform spark-submit jobs?  \n**A:** Spark-submit is the process for running Spark jobs in the open source implementation of Spark.  [Jobs](https://docs.azuredatabricks.net/user-guide/jobs.html) and [the jobs API](https://docs.azuredatabricks.net/api/latest/jobs.html#job-api) are a robust option offered in the Databricks environment.  You can also launch spark-submit jobs through the jobs UI as well\n\n**Extra Practice:** Apply what you learned in this module by completing the optional [Parsing Nested Data]($./Optional/Parsing-Nested-Data) exercise."],"metadata":{}}],"metadata":{"name":"07-Loading-Data-and-Productionalizing","notebookId":291050440998009},"nbformat":4,"nbformat_minor":0}
