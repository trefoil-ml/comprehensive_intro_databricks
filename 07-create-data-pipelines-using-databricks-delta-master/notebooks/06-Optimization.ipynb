{"cells":[{"cell_type":"markdown","source":["# Databricks Delta Optimizations and Best Practices\n\nDatabricks&reg; Delta has nifty optimizations to speed up your queries.\n\n## Datasets Used\n* Online retail datasets from\n`/mnt/training/online_retail`"],"metadata":{}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Set up relevant paths."],"metadata":{}},{"cell_type":"code","source":["deltaIotPath = userhome + \"/delta/iot-pipeline/\"\ndeltaDataPath = userhome + \"/delta/customer-data/\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## SMALL FILE PROBLEM\n\nHistorical and new data is often written in very small files and directories. \n\nThis data may be spread across a data center or even across the world (that is, not co-located).\n\nThe result is that a query on this data may be very slow due to\n* network latency \n* volume of file metatadata \n\nThe solution is to compact many small files into one larger file.\nDatabricks Delta has a mechanism for compacting small files."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n\n\nUse Azure Data Explorer to see many small files.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Data Explorer is available ONLY on Azure (not in Databricks)\n\n<img src=\"https://s3-us-west-2.amazonaws.com/curriculum-release/images/eLearning/Delta/azure-small-file.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px\"/></div>"],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n### OPTIMIZE\nDatabricks Delta supports the `OPTIMIZE` operation, which performs file compaction.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Small files are compacted together into new larger files up to 1GB.\nThus, at this point the number of files increases!\n\nThe 1GB size was determined by the Databricks optimization team as a trade-off between query speed and run-time performance.\n\n`OPTIMIZE` is not run automatically because you must collect many small files first.\n\n* Run `OPTIMIZE` more often if you want better end-user query performance \n* Since `OPTIMIZE` is a time consuming step, run it less often if you want to optimize cost of compute hours\n* To start with, run `OPTIMIZE` on a daily basis (preferably at night when spot prices are low), and determine the right frequency for your particular business case\n* In the end, the frequency at which you run `OPTIMIZE` is a business decision\n\nThe easiest way to see what `OPTIMIZE` does is to perform a simple `count(*)` query before and after and compare the timing!"],"metadata":{}},{"cell_type":"markdown","source":["Take a look at the `deltaIotPath + \"/date=2018-06-01/\" ` directory.\n\nNotice, in particular files like `../delta/iot-pipeline/date=2018-06-01/part-xxxx.snappy.parquet`. There are hundreds of small files!\n\n**Make sure you run exercises 2, 3 and 4 from lesson 03-Append before running the next command**"],"metadata":{}},{"cell_type":"code","source":["display(dbutils.fs.ls(deltaIotPath + \"/date=2018-06-01/\"))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["CAUTION: Run this query. Notice it is very slow, due to the number of small files."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM demo_iot_data_delta where deviceId=379"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Data Skipping and ZORDER\n\nDatabricks Delta uses two mechanisms to speed up queries.\n\n<b>Data Skipping</b> is a performance optimization that aims at speeding up queries that contain filters (WHERE clauses). \n\nFor example, we have a data set that is partitioned by `date`. \n\nA query using `WHERE date > 2018-06-01` would not access data that resides in partitions that correspond to dates prior to `2018-06-01`.\n\n<b>ZOrdering</b> is a technique to colocate related information in the same set of files. \n\nZOrdering maps multidimensional data to one dimension while preserving locality of the data points."],"metadata":{}},{"cell_type":"markdown","source":["-sandbox\n\n\n#### ZORDER example\nIn the image below, table `Students` has 4 columns: \n* `gender` with 2 distinct values\n* `Pass-Fail` with 2 distinct values\n* `Class` with 4 distinct values  \n* `Student` with many distinct values \n\nSuppose you wish to perform the following query:\n\n```SELECT Name FROM Students WHERE gender = 'M' AND Pass_Fail = 'P' AND Class = 'Junior'```\n\n```ORDER BY Gender, Pass_Fail```\n\nThe most effective way of performing that search is to order the data starting with the largest set, which is `Gender` in this case. \n\nIf you're searching for `gender = 'M'`, then you don't even have to look at students with `gender = 'F'`. \n\nNote that this technique only works if all `gender = 'M'` values are co-located.\n\n\n<div><img src=\"https://s3-us-west-2.amazonaws.com/curriculum-release/images/eLearning/Delta/zorder.png\" style=\"height: 300px\"/></div><br/>"],"metadata":{}},{"cell_type":"markdown","source":["#### ZORDER usage\n\nWith Databricks Delta the notation is:\n\n> `OPTIMIZE Students`<br>\n`ZORDER BY Gender, Pass_Fail`\n\nThis will ensure all the data backing `Gender = 'M' ` is colocated, then data associated with `Pass_Fail = 'P' ` is colocated.\n\nSee References below for more details on the algorithms behind ZORDER.\n\nUsing ZORDER, you can order by multiple columns as a comma separated list; however, the effectiveness of locality drops.\n\nIn streaming, where incoming events are inherently ordered (more or less) by event time, use `ZORDER` to reorder by a more meaningful index."],"metadata":{}},{"cell_type":"code","source":["%sql\nOPTIMIZE demo_iot_data_delta\nZORDER by (deviceId)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%sql\nSELECT * FROM demo_iot_data_delta WHERE deviceId=379"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["-sandbox\n## VACUUM\n\nTo save on storage costs you should occasionally clean up invalid files using the `VACUUM` command. \n\nInvalid files are small files compacted into a larger file with the `OPTIMIZE` command.\n\nThe  syntax of the `VACUUM` command is \n>`VACUUM name-of-table RETAIN number-of HOURS;`\n\nThe `number-of` parameter is the <b>retention interval</b>, specified in hours.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Databricks does not recommend you set a retention interval shorter than seven days because old snapshots and uncommitted files can still be in use by concurrent readers or writers to the table.\n\nThe scenario here is:\n0. User A starts a query off uncompacted files, then\n0. User B invokes a `VACUUM` command, which deletes the uncompacted files\n0. User A's query fails because the underlying files have disappeared\n\nInvalid files can also result from updates/upserts/deletions.\n\nMore details are provided here: <a href=\"https://docs.azuredatabricks.net/delta/optimizations.html#garbage-collection\" target=\"_blank\"> Garbage Collection</a>."],"metadata":{}},{"cell_type":"code","source":["len(dbutils.fs.ls(deltaIotPath + \"/date=2018-06-01\"))"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["-sandbox\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> In the example below we set off an immediate `VACUUM` operation with an override of the retention check so that all files are cleaned up immediately.\n\nDo not do this in production!"],"metadata":{}},{"cell_type":"code","source":["%sql\n\nVACUUM demo_iot_data_delta RETAIN 0 HOURS;"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["-sandbox\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Notice how the directory looks vastly cleaned up!"],"metadata":{}},{"cell_type":"code","source":["len(dbutils.fs.ls(deltaIotPath + \"/date=2018-06-01\"))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## Exercise 1: OPTIMIZE and ZORDER\n\nLet's apply some of these optimizations to `../delta/customer-data/`.\n\nOur data is partitioned by `Country`.\n\nWe want query the data for `StockCode` equal to `22301`. \n\nWe expect this query to be slow because we have to examine ALL OF `../delta/customer-data/` to find the desired `StockCode` and not just in one or two partitions.\n\nFirst, let's time the above query: you will need to form a DataFrame to pass to `preZorderQuery`."],"metadata":{}},{"cell_type":"code","source":["# TODO\n%timeit preZorderQuery = spark.sql(\"FILL_IN\").collect()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Compact the files and re-index by `StockCode`."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\nOPTIMIZE FILL_IN\nZORDER by (FILL_IN)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Let's time the above query again: you will need to form a DataFrame to pass to `postZorderQuery`."],"metadata":{}},{"cell_type":"code","source":["# TODO\n%timeit postZorderQuery = spark.sql(\"FILL_IN\").collect()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["## Exercise 2: VACUUM\n\nCount number of files before `VACUUM` for `Country=Sweden`."],"metadata":{}},{"cell_type":"code","source":["# TODO\npreNumFiles = len(dbutils.fs.ls(FILL_IN))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ndbTest(\"Delta-08-numFilesSweden-pre\", True, preNumFiles > 1)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Now, watch the number of files shrink as you perform `VACUUM`."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\nVACUUM FILL_IN"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Count how many files there are for `Country=Sweden`."],"metadata":{}},{"cell_type":"code","source":["# TODO\npostNumFiles = len(dbutils.fs.ls(FILL_IN))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ndbTest(\"Delta-08-numFilesSweden-post\", 1, postNumFiles)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["## Summary\nDatabricks Delta offers key features that allow for query optimization and garbage collection, resulting in improved performance."],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n\n**Q:** Why are many small files problematic when doing queries on data backed by these?<br>\n**A:** If there are many files, some of whom may not be co-located the principal sources of slowdown are\n* network latency \n* (volume of) file metatadata \n\n**Q:** What do `OPTIMIZE` and `VACUUM` do?<br>\n**A:** `OPTIMIZE` creates the larger file from a collection of smaller files and `VACUUM` deletes the invalid small files that were used in compaction.\n\n**Q:** What size files does `OPTIMIZE` compact to and why that value?<br>\n**A:** Small files are compacted to around 1GB; this value was determined by the Spark optimization team as a good compromise between speed and performace.\n\n**Q:** What should one be careful of when using `VACUUM`?<br>\n**A:** Don't set a retention interval shorter than seven days because old snapshots and uncommitted files can still be in use by concurrent readers or writers to the table.\n\n**Q:** What does `ZORDER` do?<br>\n**A:** It is a technique to colocate related information in the same set of files."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Architecture]($./07-Architecture )."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"https://docs.azuredatabricks.net/delta/optimizations.html\" target=\"_blank\">Optimizing Performance and Cost</a>\n* <a href=\"http://parquet.apache.org/documentation/latest/\" target=\"_blank\">Parquet Metadata</a>\n* <a href=\"https://en.wikipedia.org/wiki/Z-order_curve\" target=\"_blank\">Z-Order Curve</a>"],"metadata":{}}],"metadata":{"name":"06-Optimization","notebookId":291050440994887},"nbformat":4,"nbformat_minor":0}
