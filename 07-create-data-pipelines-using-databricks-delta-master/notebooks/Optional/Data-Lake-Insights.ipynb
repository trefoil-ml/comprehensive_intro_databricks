{"cells":[{"cell_type":"markdown","source":["# Gain Actionable Insights from a Data Lake, Satisfy GDPR\n\nIn this optional project, use Databricks Delta to manage a data lake consisting of a lot of historical data plus incoming streaming data.\n\nA video gaming company stores historical data in a data lake, which is growing exponentially. \n\nThe data isn't sorted in any particular way (actually, it's quite a mess).\n\nIt is proving to be _very_ difficult to query and manage this data because there is so much of it.\n\nTo further complicate issues, a regulatory agency has decreed you be able to identify and delete all data associated with a specific user (i.e. GDPR). \n\nIn other words, you must delete data associated with a specific `deviceId`.\n\n## Instructions\n0. Read in streaming data into Databricks Delta raw tables\n0. Create Databricks Delta query table\n0. Compute aggregate statistics about data i.e. create summary table\n0. Identify events associated with specific `deviceId` \n0. Do data cleanup using Databricks Delta advanced features\n\n## CAUTION\n* Do not use <b>RunAll</b> mode (next to <b>Permissions</b>)."],"metadata":{}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Set up relevant paths."],"metadata":{}},{"cell_type":"code","source":["inputPath = \"/mnt/training/gaming_data/mobile_streaming_events_b\"\noutputPathBronze = userhome + \"/gaming/bronze\"\noutputPathSilver = userhome + \"/gaming/silver\"\noutputPathGold = userhome + \"/gaming/gold\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Step 1: Prepare Schema and Read Streaming Data from input source\n\nThe input source is a folder containing files of around 100,000 bytes each and is set up to stream slowly.\n\nRun this code to read streaming data in."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StringType, IntegerType, TimestampType, DoubleType\n\neventSchema = ( StructType()\n  .add('eventName', StringType()) \n  .add('eventParams', StructType() \n    .add('game_keyword', StringType()) \n    .add('app_name', StringType()) \n    .add('scoreAdjustment', IntegerType()) \n    .add('platform', StringType()) \n    .add('app_version', StringType()) \n    .add('device_id', StringType()) \n    .add('client_event_time', TimestampType()) \n    .add('amount', DoubleType()) \n  )     \n)\n\ngamingEventDF = (spark\n  .readStream\n  .schema(eventSchema) \n  .option('streamName','mobilestreaming_demo') \n  .option(\"maxFilesPerTrigger\", 1)                # treat each file as Trigger event\n  .json(inputPath) \n)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Step 2: Write Stream\n\nThe instructions here are to:\n\n* Write the stream from `gamingEventDF` to the Databricks Delta data lake in path defined by `outputPathBronze`.\n* Convert `client_event_time` to a date format and rename to `eventDate`\n* Filter out null `eventDate`"],"metadata":{}},{"cell_type":"code","source":["# TODO\nfrom pyspark.sql.functions import to_date, col\n\neventsStream = (gamingEventDF\n  .withColumn(FILL_IN)\n  .filter(FILL_IN)\n\n  FILL_IN\n\n  .option('checkpointLocation', outputPathBronze + '/_checkpoint') \n  .start(outputPathBronze)\n)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Wait until the stream is initialized, then create table `mobile_events_delta_raw`."],"metadata":{}},{"cell_type":"code","source":["# TODO\nspark.sql(\"\"\"\n   DROP TABLE IF EXISTS mobile_events_delta_raw\n \"\"\")\nspark.sql(\"\"\"\n   CREATE TABLE mobile_events_delta_raw\n   FILL_IN"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ntry:\n  rawTableExists = (spark.table(\"mobile_events_delta_raw\") is not None)\nexcept:\n  rawTableExists = False\n  \nfirstRow = spark.sql(\"SELECT * FROM mobile_events_delta_raw\").take(1)\n\ndbTest(\"Delta-08-rawTableExists\", True, rawTableExists)  \ndbTest(\"Delta-08-rowsExist\", False, not firstRow) \n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Step 3a: Create a Databricks Delta table\n\nCreate `device_id_type_table` from data in `/mnt/training/gaming_data/dimensionData`.\n\nThis table associates `deviceId` with `deviceType` = `{android, ios}`."],"metadata":{}},{"cell_type":"code","source":["# TODO\ntablePath = \nspark.sql(\"\"\"\n   DROP TABLE IF EXISTS device_id_type_table\n \"\"\")\nspark.sql(\"\"\"\n   CREATE TABLE device_id_type_table \n   FILL_IN"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ntry:\n  tableExists = (spark.table(\"device_id_type_table\") is not None)\nexcept:\n  tableExists = False\n  \ndbTest(\"Delta-08-tableExists\", True, tableExists)  \n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Step 3b: Create a query table\n\nCreate table `mobile_events_delta_query` by joining `device_id_type_table` with `mobile_events_delta_raw` on `deviceId`.\n* Your fields should be `eventName`, `deviceId`, `eventTime`, `eventDate` and `deviceType`.\n* Make sure to `PARTITION BY (eventDate)`\n* Write to `outputPathSilver`"],"metadata":{}},{"cell_type":"code","source":["# TODO\nspark.sql(\"\"\"\n   DROP TABLE IF EXISTS mobile_events_delta_query\n\"\"\")\n\nspark.sql(\"\"\"\n    CREATE TABLE mobile_events_delta_query\n    FILL_IN"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nfrom pyspark.sql.types import StructField, StructType, StringType, TimestampType, DateType\nschema = spark.table(\"mobile_events_delta_query\").schema\n\nexpectedSchema = StructType([\n   StructField(\"eventName\", StringType(), True),\n   StructField(\"deviceId\", StringType(), True),\n   StructField(\"eventTime\", TimestampType(), True),\n   StructField(\"eventDate\", DateType(), True),\n   StructField(\"deviceType\", StringType(), True),\n])\n\nfirstRowQuery = spark.sql(\"SELECT * FROM mobile_events_delta_query limit 1\").collect()[0][0]\n\ndbTest(\"Delta-08-querySchema\", set(expectedSchema), set(schema))\ndbTest(\"Delta-08-queryRowsExist\", False, not firstRowQuery) \n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Step 4a: Create a Delta summary table out of query table\n\nThe company executives want to look at the number of active users by week.\n\nCount number of events in the by week."],"metadata":{}},{"cell_type":"code","source":["# TODO\nspark.sql(\"\"\"\n   DROP TABLE IF EXISTS mobile_events_delta_summary\n\"\"\")\n\nspark.sql(\"\"\"\n   CREATE TABLE mobile_events_delta_summary \n   FILL_IN"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nfrom pyspark.sql.types import StructType, StringType, LongType\n\nWAUactualSchema = spark.table(\"mobile_events_delta_summary\").schema\n\nWAUexpectedSchema = StructType([\n   StructField(\"week\",IntegerType(), True),\n   StructField(\"WAU\",LongType(), True),\n])\n\ndbTest(\"Delta-L8-WAUquerySchema\", set(WAUexpectedSchema), set(WAUactualSchema))\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### Step 4b: Visualization\n\nThe company executives are visual people: they like pretty charts.\n\nCreate a bar chart out of `mobile_events_delta_summary` where the horizontal axis is month and the vertical axis is WAU."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\nFILL_IN"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Step 5: Isolate a specific `deviceId`\n\nIdentify all the events associated with a specific user, rougly proxied by the first `deviceId` we encounter in our query. \n\nUse the `mobile_events_delta_query` table.\n\nThe `deviceId` you come up with should be a string."],"metadata":{}},{"cell_type":"code","source":["# TODO \ndeviceId = str(spark.sql(\"FILL_IN\").collect()[0][0])"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ndeviceIdexists = len(deviceId) > 0\n\ndbTest(\"Delta-L8-lenDeviceId\", True, deviceIdexists)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["-sandbox\n### Step 6: ZORDER \n\nSince the events are implicitly ordered by `eventTime`, implicitly re-order by `deviceId`. \n\nThe data pertaining to this `deviceId` is spread out all over the data lake. (It's definitely _not_ co-located!).\n\nPass in the `deviceId` variable you defined above.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> `ZORDER` may take a few minutes."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\nOPTIMIZE FILL_IN\nZORDER BY FILL_IN"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["### Step 7: Delete Specific `deviceId`\n\n0. Delete rows with that particular `deviceId` from `mobile_events_delta_query`.\n0. Make sure that `deviceId` is no longer in the table!"],"metadata":{}},{"cell_type":"code","source":["# TODO\nspark.sql(\"FILL_IN\".format(deviceId))\nnoDeviceId = spark.sql((\"FILL_IN\").format(deviceId)).collect()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ndbTest(\"Delta-08-noDeviceId\", True , not noDeviceId)\n\nprint(\"Tests passed!\")\n\ndbutils.fs.rm(userhome, True)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### Step 8: Stop the streaming process"],"metadata":{}},{"cell_type":"code","source":["# TODO\nfor streamingQuery in spark.streams.active:\n  FILL_IN"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nnumActiveStreams = len(spark.streams.active)\ndbTest(\"Delta-08-numActiveStreams\", 0, numActiveStreams)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["-sandbox\n### Step 9: Clean Up\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Do not use a retention of 0 hours in production, as this may affect queries that are currently in flight. \nBy default this value is 7 days. \n\nWe use 0 hours here for purposes of demonstration only.\n\nRecall, we use `VACUUM` to reduce the number of files in each partition directory to 1."],"metadata":{}},{"cell_type":"code","source":["%sql\n-- TODO\nVACUUM FILL_IN"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["If the `../eventDate=2018-05-20` directory does not exist, try a different `eventDate` directory."],"metadata":{}},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nnumFilesOne = len(dbutils.fs.ls((\"{}/eventDate=2018-05-20\").format(outputPathSilver)))\n\ndbTest(\"Delta-08-numFilesOne\", 1, numFilesOne)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["Congratulations: ALL DONE!!"],"metadata":{}}],"metadata":{"name":"Data-Lake-Insights","notebookId":291050440994969},"nbformat":4,"nbformat_minor":0}
