{"cells":[{"cell_type":"markdown","source":["# Databricks Delta Batch Operations - Append\n\nDatabricks&reg; Delta allows you to read, write and query data in data lakes in an efficient manner.\n\n## Datasets Used\nWe will use online retail datasets from\n* `/mnt/training/online_retail` in the demo part and\n* `/mnt/training/structured-streaming/events/` in the exercises"],"metadata":{}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Set up relevant paths."],"metadata":{}},{"cell_type":"code","source":["miniDataInputPath = \"/mnt/training/online_retail/outdoor-products/outdoor-products-mini.csv\"\ngenericDataPath = userhome + \"/generic/customer-data/\"\ndeltaDataPath = userhome + \"/delta/customer-data/\"\ndeltaIotPath = userhome + \"/delta/iot-pipeline/\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["Here, we add new data to the consumer product data.\n\nBefore we load data into non-Databricks Delta and Databricks Delta tables, do a simple pre-processing step:\n\n* The column `StockCode` should be of type `String`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nnewDataDF = (spark       \n  .read                                              # Read a DataFrame from storage\n  .option(\"inferSchema\",\"true\")                      # Infer schema\n  .option(\"header\",\"true\")                           # File has a header\n  .csv(miniDataInputPath)                                    # Path to file\n  .withColumn(\"StockCode\", col('StockCode').cast(\"String\")) \n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["Do a simple count of number of new items to be added to production data."],"metadata":{}},{"cell_type":"code","source":["newDataDF.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">23</span><span class=\"ansired\">]: </span>36\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["## APPEND Using Non-Databricks Delta pipeline\nAppend to the production table.\n\nIn the next cell, load the new data in `parquet` format and save to `../generic/customer-data/`."],"metadata":{}},{"cell_type":"code","source":["(newDataDF\n  .write\n  .format(\"parquet\")\n  .partitionBy(\"Country\")\n  .mode(\"append\")\n  .save(genericDataPath)\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["We expect to see `65499 + 36 = 65535` rows, but we do not.\n\nWe may even see an error message."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT count(*) FROM customer_data"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>65571</td></tr></tbody></table></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["-sandbox\n\nStrange: we got a count we were not expecting!\n\nThis is the <b>schema on read</b> problem. It means that as soon as you put data into a data lake, \nthe schema is unknown <i>until</i> you perform a read operation.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Repair the table again and count the number of records."],"metadata":{}},{"cell_type":"code","source":["%sql\nMSCK REPAIR TABLE customer_data;\n\nSELECT count(*) FROM customer_data"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>65571</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["## APPEND Using Databricks Delta Pipeline\n\nNext, repeat the process by writing to Databricks Delta format. \n\nIn the next cell, load the new data in Databricks Delta format and save to `../delta/customer-data/`."],"metadata":{}},{"cell_type":"code","source":["# Just in case it exists already.\ndbutils.fs.rm(deltaDataPath, True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">25</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["(newDataDF\n  .write\n  .format(\"delta\")\n  .partitionBy(\"Country\")\n  .mode(\"append\")\n  .save(deltaDataPath)\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["Perform a simple `count` query to verify the number of records and notice it is correct.\n\nShould be `65535`."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT count(*) FROM customer_data_delta"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>36</td></tr></tbody></table></div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["## Exercise 1\n\n0. Read the JSON data under `streamingEventPath` into a DataFrame\n0. Add a `date` column using `from_unixtime(col(\"time\").cast('String'),'MM/dd/yyyy').cast(\"date\"))`\n0. Add a `deviceId` column consisting of random numbers from 0 to 99 using this expression `expr(\"cast(rand(5) * 100 as int)`\n0. Use the `repartition` method to split the data into 200 partitions\n\nRefer to  <a href=\"http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#\" target=\"_blank\">Pyspark function documentation</a>."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import expr, col, from_unixtime, to_date\nstreamingEventPath = \"/mnt/training/structured-streaming/events/\"\nrawDataDF = (spark\n  .read \n  .option(\"inferSchema\", \"true\") \n  .json(streamingEventPath) \n  .withColumn(\"date\", to_date(from_unixtime(col('time').cast('Long'),'yyyy-MM-dd')))\n  .withColumn(\"deviceId\", expr(\"cast(rand(5) * 100 as int)\"))\n  .repartition(200)\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType, DateType, IntegerType\n\nexpectedSchema = StructType([\n   StructField(\"action\",StringType(), True),\n   StructField(\"time\",LongType(), True),\n   StructField(\"date\",DateType(), True),\n   StructField(\"deviceId\",IntegerType(), True),\n])\n\ndbTest(\"Delta-03-schemas\", set(expectedSchema), set(rawDataDF.schema))\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["## Exercise 2\n\nWrite out the raw data in Databricks Delta format to `/delta/iot-pipeline/` and create a Databricks Delta table called `demo_iot_data_delta`.\n\nRemember to\n* partition by `date`\n* save to `deltaIotPath`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n(rawDataDF\n  .write\n  .mode(\"overwrite\")\n  .format(\"delta\")\n  .partitionBy(\"date\")\n  .save(deltaIotPath)\n)\n\nspark.sql(\"\"\"\n    DROP TABLE IF EXISTS demo_iot_data_delta\n  \"\"\")\nspark.sql(\"\"\"\n    CREATE TABLE demo_iot_data_delta\n    USING DELTA \n    LOCATION '{}' \n  \"\"\".format(deltaIotPath))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">29</span><span class=\"ansired\">]: </span>DataFrame[]\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ntry:\n  tableExists = (spark.table(\"demo_iot_data_delta\") is not None)\nexcept:\n  tableExists = False\n  \ndbTest(\"Delta-03-tableExists\", True, tableExists)  \n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["## Exercise 3\n\nCreate a new DataFrame with columns `action`, `time`, `date` and `deviceId`. The columns contain the following data:\n\n* `action` contains the value `Open`\n* `time` contains the Unix time cast into a long integer `cast(1529091520 as bigint)`\n* `date` contains `cast('2018-06-01' as date)`\n* `deviceId` contains a random number from 0 to 499 given by `expr(\"cast(rand(5) * 500 as int)\")`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.functions import expr, from_unixtime\nfrom pyspark.sql.types import LongType\n\nnewDataDF = (spark.range(10000) \n  .repartition(200)\n  .selectExpr(\"'Open' as action\", \"cast(1529091520 as bigint) as time\",  \"cast('2018-06-01' as date) as date\") \n  .withColumn(\"deviceId\", expr(\"cast(rand(5) * 500 as int)\"))\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ntotal = newDataDF.count()\n\ndbTest(\"Delta-03-newDataDF-count\", 10000, total)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":29},{"cell_type":"markdown","source":["## Exercise 4\n\nAppend new data to `demo_iot_data_delta`.\n\n* Use `append` mode\n* Save to `deltaIotPath`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n(newDataDF\n  .write\n  .format(\"delta\")\n  .partitionBy(\"date\")\n  .mode(\"append\")\n  .save(deltaIotPath)\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nfrom pyspark.sql.types import Row\nnumFiles = spark.sql(\"SELECT count(*) as total FROM demo_iot_data_delta\").collect()[0][0]\n\ndbTest(\"Delta-03-numFiles\", 110000 , numFiles)\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Tests passed!\n</div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["## Summary\nWith Databricks Delta, you can easily append new data without schema-on-read issues."],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n**Q:** What parameter do you need to add to an existing dataset in a Delta table?<br>\n**A:** \n`df.write...mode(\"append\").save(\"..\")`\n\n**Q:** What's the difference between `.mode(\"append\")` and `.mode(\"overwrite\")` ?<br>\n**A:** `append` atomically adds new data to an existing Databricks Delta table and `overwrite` atomically replaces all of the data in a table.\n\n**Q:** I've just repaired `myTable` using `MSCK REPAIR TABLE myTable;`\nHow do I verify that the repair worked ?<br>\n**A:** `SELECT count(*) FROM myTable` and make sure the count is what I expected\n\n\n  \n**Q:** In exercise 2, why did we use `.withColumn(.. cast(rand(5) ..)` i.e. pass a seed to the `rand()` function ?<br>\n**A:** In order to ensure we get the SAME set of pseudo-random numbers every time, on every cluster."],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Upsert]($./04-Upsert)."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"https://docs.azuredatabricks.net/delta/delta-batch.html\" target=\"_blank\">Table Batch Read and Writes</a>"],"metadata":{}}],"metadata":{"name":"03-Append","notebookId":291050440994511},"nbformat":4,"nbformat_minor":0}
