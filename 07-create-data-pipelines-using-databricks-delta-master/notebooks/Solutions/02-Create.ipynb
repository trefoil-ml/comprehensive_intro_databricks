{"cells":[{"cell_type":"markdown","source":["# Databricks Delta Batch Operations - Create Table\n\nDatabricks&reg; Delta allows you to read, write and query data in data lakes in an efficient manner.\n\n## Datasets Used\nWe will use online retail datasets from `/mnt/training/online_retail`"],"metadata":{}},{"cell_type":"markdown","source":["### Getting Started\n\nYou will notice that throughout this course, there is a lot of context switching between PySpark/Scala and SQL.\n\nThis is because:\n* `read` and `write` operations are performed on DataFrames using PySpark or Scala\n* table creates and queries are performed directly off Databricks Delta tables using SQL\n\nRun the following cell to configure our \"classroom.\""],"metadata":{}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Set up relevant paths."],"metadata":{}},{"cell_type":"code","source":["inputPath = \"/mnt/training/online_retail/data-001/data.csv\"\ngenericDataPath = userhome + \"/generic/customer-data/\"\ndeltaDataPath = userhome + \"/delta/customer-data/\"\nbackfillDataPath = userhome + \"/delta/backfill-data/\""],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["###  READ CSV data then WRITE to Parquet / Databricks Delta\n\nRead the data into a DataFrame. Since this is a CSV file, let Spark infer the schema from the first row by setting\n* `inferSchema` to `true`\n* `header` to `true`\n\nUse overwrite mode so that it is not a problem to re-write data in case you end up running the cell again.\n\nPartition on `Country` because there are only a few unique countries. \n\nMore information on the how and why of partitioning is contained in the links at the bottom of this notebook.\n\nThen write the data to Parquet and Databricks Delta."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\nfrom pyspark.sql.functions import col\n\nrawDataDF = (spark.read \n  .option(\"inferSchema\", \"true\") \n  .option(\"header\", \"true\")\n  .csv(inputPath) \n  .withColumn(\"InvoiceNo\", col(\"InvoiceNo\").cast(IntegerType()))\n)\n\n# write to generic dataset\nrawDataDF.write.mode(\"overwrite\").format(\"parquet\").partitionBy(\"Country\").save(genericDataPath)\n\n# write to delta dataset\nrawDataDF.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Country\").save(deltaDataPath)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["-sandbox\n### CREATE Using Non-Databricks Delta Pipeline\n\nCreate a table called `customer_data` using `parquet` out of the above data.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Notice how you MUST specify a schema and partitioning info!"],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"\"\"\n    DROP TABLE IF EXISTS customer_data\n  \"\"\")\nspark.sql(\"\"\"\n    CREATE TABLE customer_data (\n      InvoiceNo INTEGER,\n      StockCode STRING,\n      Description STRING,\n      Quantity INTEGER,\n      InvoiceDate STRING,\n      UnitPrice DOUBLE,\n      CustomerID INTEGER,\n      Country STRING)\n    USING parquet \n    OPTIONS (path = '{}' )\n    PARTITIONED BY (Country)\n  \"\"\".format(genericDataPath))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Perform a simple `count` query to verify the number of records."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT count(*) FROM customer_data"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["-sandbox\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Wait, no results? \n\nWhat is going on here is a problem that stems from its Apache Hive origins.\n\nIt's the concept of\n<b>schema on read</b> where data is applied to a plan or schema as it is pulled out of a stored location, rather than as it goes into a stored location.\n\nThis means that as soon as you put data into a data lake, the schema is unknown <i>until</i> you perform a read operation.\n\nTo remedy, you repair the table using `MSCK REPAIR TABLE`.\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Only after table repair is our count of customer data correct.\n\nSchema on read is explained in more detail <a href=\"https://stackoverflow.com/a/11764519/53495#\" target=\"_blank\">in this article</a>."],"metadata":{}},{"cell_type":"code","source":["%sql\nMSCK REPAIR TABLE customer_data;\n\nSELECT count(*) FROM customer_data"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["-sandbox\n### CREATE Using Databricks Delta Pipeline\n\nCreate a table called `customer_data_delta` using `DELTA` out of the above data.\n\nThe notation is:\n> `CREATE TABLE <table-name>` <br>\n  `USING DELTA` <br>\n  `LOCATION <path-do-data> ` <br>\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Notice how we do not have to specify partition columns."],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"\"\"\n  DROP TABLE IF EXISTS customer_data_delta\n\"\"\")\nspark.sql(\"\"\"\n  CREATE TABLE customer_data_delta \n  USING DELTA \n  LOCATION '{}' \n\"\"\".format(deltaDataPath))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["-sandbox\nPerform a simple `count` query to verify the number of records.\n\n<img alt=\"Caution\" title=\"Caution\" style=\"vertical-align: text-bottom; position: relative; height:1.3em; top:0.0em\" src=\"https://files.training.databricks.com/static/images/icon-warning.svg\"/> Notice how the count is right off the bat; no need to worry about table repairs."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT count(*) FROM customer_data_delta"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["#### Metadata\n\nSince we already have data backing `customer_data_delta` in place, \nthe table in the Hive metastore automatically inherits the schema, partitioning, \nand table properties of the existing data. \n\nNote that we only store table name, path, database info in the Hive metastore,\nthe actual schema is stored in `_delta_logs`.\n\nMetadata is displayed through `DESCRIBE DETAIL <tableName>`.\n\nAs long as we have some data in place already for a Databricks Delta table, we can infer schema."],"metadata":{}},{"cell_type":"code","source":["%sql\nDESCRIBE DETAIL customer_data_delta"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["-sandbox\n## Exercise 1\n\nRead data in `outdoorSmallPath` with options:\n* first row is the header\n* infer schema from the header\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** Since `StockCode` looks numeric, you will need to convert `StockCode` explicitly to String. \n\n* Use this notation `withColumn(\"StockCode\", col(\"StockCode\").cast(StringType()))`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import col\n\noutdoorSmallPath = \"/mnt/training/online_retail/outdoor-products/outdoor-products-small.csv\"\nbackfillDF = (spark       \n  .read                                               \n  .option(\"inferSchema\",\"true\")                       \n  .option(\"header\",\"true\")                            \n  .csv(outdoorSmallPath)   \n  .withColumn(\"StockCode\", col(\"StockCode\").cast(StringType()))\n)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\nfrom pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType, DoubleType\n\nexpectedSchema = StructType([\n   StructField(\"InvoiceNo\", IntegerType(), True),\n   StructField(\"StockCode\", StringType(), True),\n   StructField(\"Description\", StringType(), True),\n   StructField(\"Quantity\", IntegerType(), True),\n   StructField(\"InvoiceDate\", StringType(), True),\n   StructField(\"UnitPrice\", StringType(), True),\n   StructField(\"CustomerID\", IntegerType(), True),\n   StructField(\"Country\", StringType(), True),\n])\n\ndbTest(\"Delta-02-schemas\", set(expectedSchema), set(backfillDF.schema))\n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["-sandbox\n## Exercise 2\n\nCreate a Databricks Delta table `backfill_data_delta` backed by `backfillDataPath`.\n\n<img alt=\"Hint\" title=\"Hint\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-light-bulb.svg\"/>&nbsp;**Hint:** \n* Don't forget to use overwrite mode just in case\n* Partititon by `Country`"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\n(backfillDF\n  .write\n  .mode(\"overwrite\")\n  .format(\"delta\")\n  .partitionBy(\"Country\")\n  .save(backfillDataPath)\n)\n\nspark.sql(\"\"\"\n    DROP TABLE IF EXISTS backfill_data_delta\n  \"\"\")\nspark.sql(\"\"\"\n    CREATE TABLE backfill_data_delta \n    USING DELTA \n    LOCATION '{}' \n  \"\"\".format(backfillDataPath))\nNone"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ntry:\n  tableExists = (spark.table(\"backfill_data_delta\") is not None)\nexcept:\n  tableExists = False\n  \ndbTest(\"Delta-02-backfillTableExists\", True, tableExists)  \n\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["## Exercise 3\n\nCount number of records from `backfill_data_delta` where the `Country` is `Sweden`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\ncount = spark.sql(\"SELECT count(*) as total FROM backfill_data_delta WHERE Country='Sweden'\").collect()[0][0]"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# TEST - Run this cell to test your solution.\ndbTest(\"Delta-L2-backfillDataDelta-count\", 2925, count)\nprint(\"Tests passed!\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["## Summary\nUsing Databricks Delta to create tables is quite straightforward and you do not need to specify schemas."],"metadata":{}},{"cell_type":"markdown","source":["## Review Questions\n\n**Q:** What is the Databricks Delta command to display metadata?<br>\n**A:** Metadata is displayed through `DESCRIBE DETAIL tableName`.\n\n**Q:** Where does the schema for a Databricks Delta data set reside?<br>\n**A:** The table name, path, database info are stored in Hive metastore, the actual schema is stored in the `_delta_logs` directory.\n\n**Q:** What is the general rule about partitioning and the cardinality of a set?<br>\n**A:** We should partition on sets that are of small cardinality to avoid penalties incurred with managing large quantities of partition info meta-data.\n\n**Q:** What is schema-on-read?<br>\n**A:** It stems from Hive and roughly means: the schema for a data set is unknown until you perform a read operation.\n\n**Q:** How does this problem manifest in Databricks assuming a `parquet` based data lake?<br>\n**A:** It shows up as missing data upon load into a table in Databricks.\n\n**Q:** How do you remedy this problem in Databricks above?<br>\n**A:** To remedy, you repair the table using `MSCK REPAIR TABLE` or switch to Databricks Delta!"],"metadata":{}},{"cell_type":"markdown","source":["## Next Steps\n\nStart the next lesson, [Append]($./03-Append)."],"metadata":{}},{"cell_type":"markdown","source":["## Additional Topics & Resources\n\n* <a href=\"https://docs.azuredatabricks.net/delta/delta-batch.html\" target=\"_blank\">Table Batch Read and Writes</a>\n* <a href=\"https://en.wikipedia.org/wiki/Partition_(database)#\" target=\"_blank\">Database Partitioning</a>"],"metadata":{}}],"metadata":{"name":"02-Create","notebookId":291050440994729},"nbformat":4,"nbformat_minor":0}
